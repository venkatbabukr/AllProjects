# -*- coding: utf-8 -*-
"""Pandas-2 notes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wtFuCs_E3WT1qC9ykFUPB7EMqOirE0kV

# Pandas 2

---

## Content

- Working with both rows & columns
- Handling duplicate records
- Pandas built-in operations
  - Aggregate functions
  - Sorting values
- Concatenating DataFrames
- Merging DataFrames

---

### **Working with rows & columns together**

#### Loading the dataset

Dataset: https://drive.google.com/file/d/1E3bwvYGf1ig32RmcYiWc0IXPN-mD_bI_/view?usp=sharing
"""

!wget "https://drive.google.com/uc?export=download&id=1E3bwvYGf1ig32RmcYiWc0IXPN-mD_bI_" -O mckinsey.csv

import pandas as pd
import numpy as np

df = pd.read_csv('mckinsey.csv')
df

"""**How can we add a row to our dataframe?**

There are multiple ways to do this.
- `concat()`
- `loc/iloc`

**How can we do add a row using the `concat()` method?**
"""

new_row = {'country': 'India', 'year': 2000,'population':13500000, 'continent': "Asia", 'life_exp':37.08, 'gdp_cap':900.23}

df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
df

"""**Why are we using `ignore_index=True`?**

- This parameter tells Pandas to ignore the existing index and create a new one based on the length of the resulting DataFrame.

Perfect! Our row is now added at the bottom of the dataframe.

**Note:**
- `concat()` doesn't mutate the the dataframe.
- It does not change the DataFrame, but returns a new DataFrame with the appended row.

Another method would be by **using loc**.

We will need to provide the position at which we want to add the new row.

**What do you think this positional value would be?**

- `len(df.index)` since we will add the new row at the end.

For this method we only need to insert the values of columns in the respective manner.
"""

new_row = {'country': 'India', 'year': 2000,'population':13500000, 'continent': "Asia", 'life_exp':37.08, 'gdp_cap':900.23}
new_row_val = list(new_row.values())
new_row_val

df.loc[len(df.index)] = new_row_val
df

"""The new row was added but the data has been duplicated.

**What you can infer from last two duplicate rows?**

- DataFrame allow us to feed duplicate rows in the data.

**Now, can we also use `iloc`?**

Adding a row at a specific index position will replace the existing row at that position.
"""

df.iloc[len(df.index)-1] = ['Japan', 1000, 1350000, 'Asia', 37.08, 100.23]
df

"""**What if we try to add the row with a new index?**"""

df.iloc[len(df.index)] = ['India', 2000, 13500000, 'Asia', 37.08, 900.23]

"""**Why are we getting an error?**

- For using `iloc` to add a row, the dataframe must already have a row in that position.
- If a row is not available, you’ll see this `IndexError`.


**Note:** When using the `loc[]` attribute, it’s not mandatory that a row already exists with a specific label.

**What if we want to delete a row?**

- use `df.drop()`

If you remember we specified axis=1 for columns.

We can modify this - `axis=0` for rows.

**Does `drop()` method uses positional indices or labels?**

- We had to specify column title.
- So **`drop()` uses labels**, NOT positional indices.

\
Let's drop the row with label 3.
"""

df

df = df.drop(3, axis=0)
df

"""We can see that the **row with label 3 is deleted**.

We now have **rows with labels 0, 1, 2, 4, 5, ...**

`df.loc[4]` and `df.iloc[4]` will give different results.
"""

df.loc[4] # The 4th row is printed

df.iloc[4] # The 5th row is printed

"""**Why did this happen?**

It is because the `loc` function selects rows using row labels (0,1,2,4,..) whereas the `iloc` function selects rows using their integer positions (staring from 0 and +1 for each row).

So for `iloc`, the 5th row starting from 0 index was printed.

**How can we drop multiple rows?**
"""

df.drop([1, 2, 4], axis=0) # drops rows with labels 1, 2, 4

"""Let's reset our indices now."""

df.reset_index(drop=True,inplace=True) # since we removed a row earlier, we reset our indices
df

"""---

### **Handling duplicate records**

If you remember, the last two rows were duplicates.

**How can we deal with these duplicate rows?**

Let's create some more duplicate rows to understand this.
"""

df.loc[len(df.index)] = ['India', 2000, 13500000, 'Asia', 37.08, 900.23]
df.loc[len(df.index)] = ['Sri Lanka',2022 ,130000000, 'Asia', 80.00,500.00]
df.loc[len(df.index)] = ['Sri Lanka',2022 ,130000000, 'Asia', 80.00,500.00]
df.loc[len(df.index)] = ['India',2000 ,13500000, 'Asia', 80.00,900.23]
df

"""**How to check for duplicate rows?**

-  We use `duplicated()` method on the DataFrame.
"""

df.duplicated()

"""It gives True if an entire row is identical to the previous row.

However, it is not practical to see a list of True and False.

We can the `loc` data selector to extract those duplicate rows.
"""

df.loc[df.duplicated()]

"""**How do we get rid of these duplicate rows?**

- We can use the `drop_duplicates()` function.
"""

df.drop_duplicates()

"""**But how do we decide among all duplicate rows which ones to keep?**

Here we can use the `keep` argument.

It has only three distinct values -
- `first`
- `last`
- `False`

The default is 'first'.

If `first`, this considers first value as unique and rest of the identical values as duplicate.
"""

df.drop_duplicates(keep='first')

"""If `last`, this considers last value as unique and rest of the identical values as duplicate."""

df.drop_duplicates(keep='last')

"""If `False`, this considers all the identical values as duplicates."""

df.drop_duplicates(keep=False)

"""**What if you want to look for duplicacy only for a few columns?**

We can use the `subset` argument to mention the list of columns which we want to use.
"""

df.drop_duplicates(subset=['country'],keep='first')

"""---

### **Slicing the DataFrame**

**How can we slice the dataframe into, say first 4 rows and first 3 columns?**

- We can use `iloc`
"""

df.iloc[0:4, 0:3]

"""Pass in 2 different ranges for slicing - **one for row** and **one for column**, just like Numpy.

Recall, `iloc` doesn't include the end index while slicing.

**Can we do the same thing with `loc`?**
"""

df.loc[1:5, 1:4]

"""**Why does slicing using indices doesn't work with `loc`?**

Recall, we need to work with explicit labels while using `loc`.
"""

df.loc[1:5, ['country','life_exp']]

"""In `loc`, we can mention ranges using column labels as well."""

df.loc[1:5, 'year':'life_exp']

"""**How can we get specific rows and columns?**"""

df.iloc[[0,10,100], [0,2,3]]

"""We pass in those **specific indices packed in `[]`**,

**Can we do step slicing?** Yes!
"""

df.iloc[1:10:2]

"""**Does step slicing work for `loc` too?** Yes!"""

df.loc[1:10:2]

"""---

## **Pandas built-in operations**

### **Aggregate functions**

Let's select the feature `'life_exp'` -
"""

le = df['life_exp']
le

"""**How can we find the mean of the column `life_exp`?**"""

le.mean()

"""What other operations can we do?

- `sum()`
- `count()`
- `min()`
- `max()`

... and so on

**Note:** We can see more methods by **pressing "tab" after `le.`**

"""

le.sum()

le.count()

"""What will happen we get if we divide `sum()` by `count()`?"""

le.sum() / le.count()

"""It gives us the **mean/average** of life expectancy.

---

### **Sorting Values**

If you notice, the `life_exp` column is not sorted.

**How can we perform sorting in Pandas?**
"""

df.sort_values(['life_exp'])

"""Rows get sorted **based on values in `life_exp` column**.

**By default**, values are sorted in **ascending order**.

**How can we sort the rows in descending order?**
"""

df.sort_values(['life_exp'], ascending=False)

"""**Can we perform sorting on multiple columns?** Yes!"""

df.sort_values(['year', 'life_exp'])

"""**What exactly happened here?**

- Rows were **first sorted** based on **`'year'`**
- Then, **rows with same values of `'year'`** were sorted based on **`'lifeExp'`**

<img src="https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/065/707/original/download.png?1708408923">

This way, we can do multi-level sorting of our data.

**How can we have different sorting orders for different columns in multi-level sorting?**
"""

df.sort_values(['year', 'life_exp'], ascending=[False, True])

"""**Just pack `True` and `False` for respective columns in a list `[]`**

---

## **Concatenating DataFrames**

Often times our data is separated into multiple tables, and we would require to work with them.

Let's see a mini use-case of `users` and `messages`.

`users` --> **Stores the user details** - **IDs** and **Names of users**
"""

users = pd.DataFrame({"userid":[1, 2, 3], "name":["sharadh", "shahid", "khusalli"]})
users

"""`msgs` --> **Stores the messages** users have sent - **User IDs** and **Messages**"""

msgs = pd.DataFrame({"userid":[1, 1, 2, 4], "msg":['hmm', "acha", "theek hai", "nice"]})
msgs

"""**Can we combine these 2 DataFrames to form a single DataFrame?**"""

pd.concat([users, msgs])

"""**How exactly did `concat()` work?**

- **By default**, `axis=0` (row-wise) for concatenation.
- **`userid`**, being same in both DataFrames, was **combined into a single column**.
  - First values of `users` dataframe were placed, with values of column `msg` as NaN
  - Then values of `msgs` dataframe were placed, with values of column `msg` as NaN
- The original indices of the rows were preserved.

**How can we make the indices unique for each row?**
"""

pd.concat([users, msgs], ignore_index = True)

"""**How can we concatenate them horizontally?**"""

pd.concat([users, msgs], axis=1)

"""As you can see here,

- Both the dataframes are combined horizontally (column-wise).
- It gives 2 columns with **different positional (implicit) index**, but **same label**.

---

### **Merging DataFrames**

So far we have only concatenated but not merged data.

**But what is the difference between `concat` and `merge`?**

`concat`
- simply stacks multiple dataframes together along an axis.

`merge`
- combines dataframes in a **smart** way based on values in shared column(s).

<img src="https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/065/708/original/d1.png?1708409121" height = 200/>

<img src="https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/065/709/original/d2.png?1708409138" height = 200/>

**How can we know the name of the person who sent a particular message?**

We need information from **both the dataframes**.

So can we use `pd.concat()` for combining the dataframes? No.
"""

pd.concat([users, msgs], axis=1)

"""**What are the problems with here?**

- `concat` simply **combined/stacked** the dataframe **horizontally**.
- If you notice, `userid 3` for **user** dataframe is stacked against `userid 2` for **msg** dataframe.
- This way of stacking doesn't help us gain any insights.

We need to **merge** the data.

**How can we join the dataframes?**
"""

users.merge(msgs, on="userid")

"""Notice that `users` has a userid=3 but `msgs` does not.

- When we **merge** these dataframes, the **userid=3 is not included**.
- Similarly, **userid=4 is not present** in `users`, and thus **not included**.
- Only the userid **common in both dataframes** is shown.

**What type of join is this?** Inner Join

Remember joins from SQL?

<img src="https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/065/710/original/joins.webp?1708409218">

The `on` parameter specifies the `key`, similar to `primary key` in SQL.

\
**What join we want to use to get info of all the users and all the messages?**
"""

users.merge(msgs, on="userid", how="outer")

"""**Note:** All missing values are replaced with `NaN`.

**What if we want the info of all the users in the dataframe?**
"""

users.merge(msgs, on="userid", how="left")

"""**Similarly, what if we want all the messages and info only for the users who sent a message?**"""

users.merge(msgs, on="userid", how="right")

"""`NaN` in **name** can be thought of as an anonymous message.

But sometimes, the column names might be different even if they contain the same data.

Let's rename our users column `userid` to `id`.
"""

users.rename(columns = {"userid": "id"}, inplace=True)
users

"""**Now, how can we merge the 2 dataframes when the `key` has a different value?**"""

users.merge(msgs, left_on="id", right_on="userid")

"""Here,
- `left_on`: Specifies the **key of the 1st dataframe** (users).
- `right_on`: Specifies the **key of the 2nd dataframe** (msgs).

---
"""