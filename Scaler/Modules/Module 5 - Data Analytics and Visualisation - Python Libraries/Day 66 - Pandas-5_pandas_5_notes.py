# -*- coding: utf-8 -*-
"""Pandas-5 notes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BV-i0G9YnaolmVYBL7_pcP0C3Jfbfykm

# Pandas 5

---

## Content

- Null/Missing values
  - `None` vs `NaN` values
  - `isna()` & `isnull()`
- Removing null values
  - `dropna()`
- Data Imputation
  - `fillna()`
- String methods
- Datetime values
- Writing to a file

---

### Data Preparation
"""

!pip install --upgrade gdown

!gdown 173A59xh2mnpmljCCB9bhC4C5eP2IS6qZ

import pandas as pd
import numpy as np

data = pd.read_csv('Pfizer_1.csv')

data_melt = pd.melt(data,id_vars = ['Date', 'Drug_Name', 'Parameter'],
            var_name = "time",
            value_name = 'reading')

data_tidy = data_melt.pivot(index=['Date','time', 'Drug_Name'],
                                        columns = 'Parameter',
                                        values='reading')
data_tidy = data_tidy.reset_index()
data_tidy.columns.name = None

data.head()

data_melt.head()

data_tidy.head()

"""---

### `None` vs `NaN`

If you notice, there are many `NaN` values in our data.

**What are these `NaN` values?**

- They are basically **missing/null values**.
- A null value signifies an **empty cell/no data**.

There can be 2 kinds of missing values:
1. `None`
2. `NaN` (Not a Number)

**Whats the difference between the `None` and `NaN`?**

Both `None` and `NaN` can be used for missing values, but their representation and behaviour may differ based on the **column's data type**.
"""

type(None)

type(np.nan)

"""1. **None in Non-numeric** columns: None can be used directly, and it will appear as None.
2. **None in Numeric** columns: Pandas automatically converts None to NaN.
3. **NaN in Numeric** columns: NaN is used to represent missing values and appears as NaN.
4. **NaN in Non-numeric** Columns: NaN can be used, and it appears as NaN.
"""

pd.Series([1, np.nan, 2, None])

"""For **numerical** type, Pandas changes `None` to `NaN`.

"""

pd.Series(["1", "np.nan", "2", None])

pd.Series(["1", "np.nan", "2", np.nan])

"""For **object** type, the `None` is preserved and not changed to `NaN`.

---

### `isna()` & `isnull()`

**How to get the count of missing values for each row/column?**

- `df.isna()`
- `df.isnull()`
"""

data.isna().head()

data.isnull().head()

"""Notice that both `isna()` and `isnull()` give the same results.

**But why do we have two methods, `isna()` and `isnull()` for the same operation?**

- `isnull()` is just an alias for `isna()`
"""

pd.isnull

pd.isna

"""As we can see, the function signature is same for both.

- `isna()` returns a **boolean dataframe**, with each cell as a boolean value.
- This value corresponds to **whether the cell has a missing value**.
- On top of this, we can use `.sum()` to find the count of the missing values.
"""

data.isna().sum()

"""This gives us the total number of missing values in each column.

**How can we get the number of missing values in each row?**
"""

data.isna().sum(axis=1)

"""**Note:** By default, the value is `axis=0` for `sum()`.

**We now have identified the null count, but how do we deal with them?**

We have two options:
- Delete the rows/columns containing the null values.
- Fill the missing values with some data/estimate.

Let's first look at deleting the rows.

---

### Removing null values

**How can we drop rows containing null values?**
"""

data.dropna()

"""Notice that rows with even a single missing value have been deleted.

**What if we want to delete the columns having missing value?**
"""

data.dropna(axis=1)

"""Notice that every column which had even a single missing value has been deleted.

**But what are the problems with deleting rows/columns?**
- loss of valuable data

So instead of dropping, it would be better to **fill the missing values with some data**.

---

### Data Imputation

**How can we fill the missing values with some data?**
"""

data.fillna(0).head()

"""**What is `fillna(0)` doing?**

- It fills all the missing values with 0.

We can do the same on a particular column too.
"""

data['2:30:00'].fillna(0)

"""**Note:**

Handling missing value completely depends on the business problem.

However, in general practice (assuming you have a large dataset) -

- if the missing values are minimal (\<5% of rows), dropping them is acceptable.
- for substantial missing values (\>10% of rows), use a suitable imputation technique.
- if a column has over 50% of null values, drop that column (unless it's very crucial for the analysis).

**What other values can we use to fill the missing values?**

We can use some kind of estimator too.
- mean (average value)
- median
- mode (most frequently occuring value)

**How would you calculate the mean of the column `2:30:00`?**
"""

data['2:30:00'].mean()

"""Now let's fill the `NaN` values with the mean value of the column."""

data['2:30:00'].fillna(data['2:30:00'].mean())

"""But this doesn't feel right. What could be wrong with this?

**Can we use the mean of all compounds as average for our estimator?**

- Different drugs have different characteristics.
- We can't simply do an average and fill the null values.

**Then what could be the solution here?**

We could fill the null values of respective compounds with their respective means.

**How can we form a column with mean temperature of respective compounds?**

- We can use `apply()`

Let's first create a function to calculate the mean.
"""

def temp_mean(x):
  x['Temperature_avg'] = x['Temperature'].mean()
  return x

"""Now we can form a new column based on the average values of temperature for each drug."""

data_tidy = data_tidy.groupby(["Drug_Name"], group_keys=False).apply(temp_mean)
data_tidy

"""Now we fill the null values in `Temperature` using this new column."""

data_tidy['Temperature'].fillna(data_tidy["Temperature_avg"], inplace=True)
data_tidy

data_tidy.isna().sum()

"""Great!

We have removed the null values from our `Temperature` column.

Let's do the same for `Pressure`.
"""

def pr_mean(x):
  x['Pressure_avg'] = x['Pressure'].mean()
  return x
data_tidy=data_tidy.groupby(["Drug_Name"]).apply(pr_mean)
data_tidy['Pressure'].fillna(data_tidy["Pressure_avg"], inplace=True)
data_tidy

data_tidy.isna().sum()

"""**How to decide if we should impute the missing values with `mean`, `median` or `mode`?**

1. `Mean`: Use when dealing with numerical data that is normally distributed and not heavily skewed by outliers.

2. `Median`: Preferable when data is skewed or contains outliers. It's suitable for ordinal or interval data.

3. `Mode`: Suitable for categorical or nominal data where there are distinct categories.

---

#### **Question**

Based on the given DataFrame, which of the following statements regarding data imputation is mostly accurate?

```
|   CustomerID   |  TransactionAmount  |     Gender     |  Age  |  ProductCategory  |
|----------------|---------------------|----------------|-------|-------------------|
|       101      |         20          |     Male       |   35  |       Apparel     |
|       102      |        NaN          |     Female     |   28  |       NaN         |
|       103      |         15          |     Female     |  NaN  |       Electronics |
|       104      |         30          |     NaN        |   42  |       Electronics |
|       105      |        150          |     Male       |   30  |       Apparel     |

```

```
A) Imputing missing values in the "TransactionAmount" column using the mean of the available values may not be suitable due to potential skewness caused by outliers.
B) Imputing missing values in the "TransactionAmount" column using the median of the available values may be suitable to handle skewness due to outliers.
C) The presence of missing values in the "Gender" column can be effectively handled by imputing the most frequent category (mode).
D) All of the above
```

**Answer:** All of the above

**Explanation:**

* Option A is correct because imputing missing values in the "TransactionAmount" column with the mean may not be appropriate if the data contains outliers. Outliers can significantly skew the mean, leading to inaccurate imputations.
* Option B is correct because as the data is skewed, the median that is roubst to outliers can better impute the missing data

* Option C is correct because for the "Gender" categorical column, the most frequently occuring category can be used to impute as gender is unlikely to exhibit significant variation in a dataset of customer transactions.

---

### String methods

**What kind of questions can we use string methods for?**

- Find rows which contains a particular string.

Say,

**How you can you filter rows containing "hydrochloride" in their drug name?**
"""

data_tidy.loc[data_tidy['Drug_Name'].str.contains('hydrochloride')].head()

"""- So in general, we will be using the following format: `Series.str.function()`

- `Series.str` can be used to access the values of the series as strings and apply several methods to it.

Now suppose we want to form a new column based on the year of the experiments?

**What can we do form a column containing the year?**
"""

data_tidy['Date'].str.split('-')

"""To extract the year, we need to select the last element of each list."""

data_tidy['Date'].str.split('-').apply(lambda x:x[2])

"""But there are certain problems with this approach.

- The **dtype of the output is still an object**, we would prefer a number type.
- The date format will always **not be in day-month-year**, it can vary.

Thus, to work with such date-time type of data, we can use a special method from Pandas.

---

### Datetime

**How can we handle datetime data types?**

- We can use the `to_datetime()` function of Pandas
- It takes as input:
  - Array/Scalars with values having proper date/time format
  - `dayfirst`: Indicating if the day comes first in the date format used
  - `yearfirst`: Indicates if year comes first in the date format used

Let's first merge our `Date` and `Time` columns into a new `timestamp` column.
"""

data_tidy['timestamp'] = data_tidy['Date'] + " " + data_tidy['time']

data_tidy.drop(['Date', 'time'], axis=1, inplace=True)

data_tidy.head()

"""Now let's convert our `timestamp` column into **datetime**."""

data_tidy['timestamp'] = pd.to_datetime(data_tidy['timestamp'])
data_tidy

data_tidy.info()

"""The type of `timestamp` column has been changed from `object` to `datetime`.

Now, let's look at a single timestamp using Pandas.

**How can we extract information from a single timestamp using Pandas?**
"""

ts = data_tidy['timestamp'][0]
ts

ts.year, ts.month, ts.day, ts.month_name()

ts.hour, ts.minute, ts.second

"""This data parsing from `string` to `datetime` makes it easier to work with such data.

We can use this data from the columns as a whole using `.dt` object.
"""

data_tidy['timestamp'].dt

"""- `dt` gives properties of values in a column.
- From this `DatetimeProperties` of column `'end'`, we can extract `year`.
"""

data_tidy['timestamp'].dt.year

"""We can use `strfttime` (**short for stringformat time**), to modify our datetime format.

Let's learn this with the help of few examples.
"""

data_tidy['timestamp'][0]

print(data_tidy['timestamp'][0].strftime('%Y')) # formatter for year

"""Similarly we can combine the format types to modify the datetime format as per our convinience.

A comprehensive list of other formats can be found here: https://pandas.pydata.org/docs/reference/api/pandas.Period.strftime.html
"""

data_tidy['timestamp'][0].strftime('%m-%d')

"""---

### Writing to a file

**How can we write our dataframe to a CSV file?**

- We have to provide the `path` and `file_name` in which we want to store the data.
"""

data_tidy.to_csv('pfizer_tidy.csv', sep=",", index=False)

"""Setting `index=False` will not inlcude the index column while writing.

---

### Extra practice material

- [Coding Exercise (Pandas)](https://colab.research.google.com/drive/1yn1OGCBJJQJp1sIljkmJwdf0uySIJUhO?usp=sharing)
"""