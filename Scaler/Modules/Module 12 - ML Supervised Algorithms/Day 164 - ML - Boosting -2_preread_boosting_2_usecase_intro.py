# -*- coding: utf-8 -*-
"""PreRead-Boosting-2-UseCase-Intro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jRl6U-gGxTvT1FHGzMW-5BDAhQ6ON6om

## Use case Intro: EMG signal classification

#### Imagine You are a Decision Scientist at Boston Dynamics( a robotics company)

- Your team is making a **robotics arm that can be controlled by brain signals**.
- These brain signals are recorded through **EMG**.

#### Problem Statement:
- Your task is to classify these EMG signals into 20 different physical actions
- This will then be used for controlling the robotics arm.

#### What is EMG (ElectroMioGraphy) ?
  - Technique to study electrical signals produced by muscular movement.

#### Dataset
- You have a dataset of EMG signals from 4 subjects/people.

#### How was the data collected ?
  - Subject was asked to perform specific physical actions
  - Signals produced due to that movement were recorded over time.
  - 8 channels were used to record the signals
  - Channels here correspond to muscles\
    For eg: Right-hand bicep
  - Frequency : 10 $ms^{-1}$

Now, lets import some libs at first.

Source: https://archive.ics.uci.edu/ml/datasets/EMG+Physical+Action+Data+Set
"""

import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import sklearn

"""#### Extracting data"""

!gdown 1h86M8si2YT-aI4Zec1MeMP_mPYsLPy5F

# x is extract

!unrar x "./emg.rar" "./"

"""#### Visualizing file structure"""

!sudo apt install tree

!tree "./EMG Physical Action Data Set/sub1"

"""Here if you see for subject 1 , we have  sub folders
- aggressive and
- normal

These folders mention the aggresive and normal activities respectively with corresponding log and txt files

We will use txt files


Let's see one of the folder from above
"""

!ls -lrt ./EMG\ Physical\ Action\ Data\ Set/sub1/Aggressive/txt/

"""#### Reading data

Now, let's see what is the data in slapping.txt
"""

!cat ./EMG\ Physical\ Action\ Data\ Set/sub1/Aggressive/txt/Slapping.txt

"""**Key observations**

* We got eight columns of the data which corresponds to eight electrodes
* We are collecting data 10 times per millisecond,
- each row gives the data for every 0.1 millisecond

# **Loading data**

While importing the data we are also going to chunk it.

#### What does chunking of data mean ?
  - Pick continuous intervals of a fixed size from data.
  - Replace those intervals with their mean/median/max etc.

#### What size of interval to choose ?
  - Depends on dataset and application
  - For this case, interval size = 10

#### But doesn't this result in a loss of data ?
  - Yes
  - #### Then why should we chunk a dataset ?
    - It depends on
      1. Data acquisition techniques
      2. Application

#### Why are we chunking our data ?
  - EMG signals suffer from problem of duplication
  - #### What does duplication mean in EMG?
    - Consecutive samples are similar to one another.
  - #### Why can this be a problem ?
    - Unnecessary data leads to :
      1. Longer training time
      2. More memory

Now lets import the data and chunk it
"""

actions = {}

data_dirs = ["./EMG Physical Action Data Set/sub1/Aggressive/txt",
             "./EMG Physical Action Data Set/sub1/Normal/txt"]

ind = 0
data = pd.DataFrame()

for dirs in data_dirs :

  for files in os.listdir(dirs):

    with open(os.path.join(dirs, files), "r") as f:

      temp = pd.read_csv(f.name,
                        sep = "\t",
                        header = None,
                        names = ["ch" + str(i) for i in range(1, 9)] # 8 input channels
                        )

      # chunking using Max of every 10 sequential values.
      temp_chunked = pd.DataFrame()

      for i in range(0, len(temp), 10):
        temp_chunked = temp_chunked.append(temp.iloc[i:i+10].max(), ignore_index = True)

      labels = [files[:-4] for i in range(len(temp_chunked))] # remove the last 4 characters=".txt" from the filename
      actions[files[:-4]] = ind

      temp_chunked["Action"] = labels

      data = pd.concat([data, temp_chunked])

      ind+=1

print(actions)

data.head()

data.info()

"""#### What can all we see from this data ?

- The data contains :
  1. 8 features and 1 target variable
  2. No Null values
  3. Around 20,000 samples

- We will use "Action" as the target attribute
"""

Y = data["Action"]
X = data.drop(columns = ["Action"])

"""
Now, lets analyze the target variable.
"""

print(Y.unique())

"""#### What can you tell about target variable from this info ?
  - Target variable contains 20 unique values.
  - It is categorical.
  - But the values are textual.
  
#### How is this going to be a problem ?
- ML algos can only take inputs in number form.

<br>
  
#### **How should we transform target variable to numerical ?**

  - It has 20 distinct values.
    #### 1. Can we use Binary Encoding ?
      - No - Why ?
        - Works with variables having only 2 values.
    
    #### 2. Can we use One Hot Encoding ?
      - No - Why ?
        - Memory consumption will become very high.

    #### 3. Can we use Label Encoding ?
      - Yes - Why ?
        - Doesn't require extra memory.
        - Works with any number of unique vals.
      
      - But the target var does not have any order.
      - #### Why won't this be a problem ?
        - The algo doesn't directly use it as input - What does this mean ?
    

"""

Y = Y.map(actions)
Y.head()

"""
Now lets check if the dataset is balanced.
#### How can we check for data balance ?
  - Check their value counts."""

print(Y.value_counts())

"""#### What can we see from this information ?
  - Each class is equally represented in the dataset.
  - i.e. The dataset is balanced.

## **Domain specific preprocessing - Rectification**

Our EMG signals should also be Rectified

#### What does rectification of EMG signals mean ?

<img src='https://drive.google.com/uc?id=14vHbNx-gTTkkI-ey1bNABT0JxPuLmLGK' width = 400>


  - Our data contains both neg/pos values.
  
  - This means that the signal cancels out to 0.
  
  - #### How can we deal with this problem ?
  
    1. Half Wave rectification:
      - Discard neg/pos values
  
    2. Full wave rectification:
      - Take abs values of entire data

#### How should we rectify our EMG signals ?
  - Full wave rectification
  - #### Why not do half-wave rectification ?
    - To minimize loss of data.

Lets rectify our data now
"""

X = abs(X)
X.head()

"""## **Handling Noise**

There can also be a lot of noise in EMG signals.

#### Why does noise occur in EMG data ?
  - Faulty equipment
  - Sensitive techniques

So, we need to remove this noise
#### Why is it important to remove noise ?
  - Noise is unwanted data
  - Hampers performance
  - Longer training time.

#### How can we remove noise from EMG signals ?
  - Taking a moving average.

<img src='https://drive.google.com/uc?id=1mgd7NwvNcL0Shs-yLVunt6yWHyzj1zVz' width = 800>

#### Why do we take a moving average ?
1. Good smoother i.e. reduces oscillations
2. Simple to implement.

So, lets remove noise in our data


#### What about points till t = 9 ?

There are various strategies for handling that

- Leave as NaN
- Use avg of as many numbers as possible
- Use the values as is, until enough points are available.
Etc.

Depending on library we are using.


We will be using pandas [ewm](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ewm.html) function

- Which calculates exponentially weighted mean columnwise.

Also we will keep the parameter ```com``` = 10
- Which Specifies the decay in terms of center of mass
"""

# what is ewm??

X = X.ewm(10).mean()
X.info()

"""#### Splitting data

Now our dataset is ready for training.

#### But do we feed the entire dataset into our algo ?
  - No - Why ?

#### So how should we split our data ?
  - We split it into train and test set.
  
  - #### What should be the ratios for splitting ?
    - 80%:20% for train/test set is good enough.

  - #### But what about validation set?
    - We will use k fold cross-validation technique.
  
  - #### Why use cross-validation ?
      - Prevents overfitting on dev-set.
      - Gives estimate of how precise model's evaluation is.

Lets split the data now.

"""

from sklearn.model_selection import train_test_split

X = np.array(X.values)
Y = np.array(Y.values)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, shuffle = True)

print(f"Sizes of the sets created are:\nTraining set:{X_train.shape[0]}\nTest set:{X_test.shape[0]}")

"""#### Preprocessed Data"""

# import pickle

# !gdown 171Yoe_GSapyrmOnD9oBzHWNOD_OnQs0F
# !gdown 1hnIlTPW3AMeB69EbeaXCRIrpMVT1Vwmc
# !gdown 1nZtB_RtxMg_MgoRczb8UWQX-AEK_l3qE
# !gdown 1zLDUErwKdmF-RacOyHEuI_z_46LssQtP


# with open('X_train.pickle', 'rb') as handle:
#     X_train = pickle.load(handle)

# with open('X_test.pickle', 'rb') as handle:
#     X_test = pickle.load(handle)

# with open('Y_train.pickle', 'rb') as handle:
#     Y_train = pickle.load(handle)

# with open('Y_test.pickle', 'rb') as handle:
#     Y_test = pickle.load(handle)