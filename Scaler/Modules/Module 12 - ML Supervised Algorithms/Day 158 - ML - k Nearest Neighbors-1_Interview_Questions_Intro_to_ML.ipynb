{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Interview Questions"
      ],
      "metadata": {
        "id": "S3Ym-LE_Q9cL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. **Question:**\n",
        "You have a linear regression model $y = w_1x + b$ but the data suggests a clear curvature in residual plots. Which conclusion is more appropriate?\n",
        "\n",
        "- A) The linear model fits perfectly, so no further action is needed\n",
        "- B) The curvature in residuals indicates the linear form might be incomplete, suggesting a possible need for polynomial terms\n",
        "- C) The residual plot only matters if the slope is zero\n",
        "- D) Curvature in residuals automatically means heteroscedasticity is satisfied\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) The curvature in residuals indicates the linear form might be incomplete, suggesting a possible need for polynomial terms\n",
        "\n",
        "**Explanation:**  \n",
        "When a plot of residuals against $x$ shows a systematic curve, it signals the linear assumption may be incorrect; you might incorporate polynomial or other transformations to capture non-linear trends.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Question:**\n",
        "A dataset has features highly correlated with each other (e.g., $x_1$ and $x_2$ nearly the same). In **multiple linear regression**, which typical issue may arise?\n",
        "\n",
        "- A) Residuals become automatically Gaussian\n",
        "- B) Perfect correlation is necessary for a good fit\n",
        "- C) Multicollinearity can inflate coefficient variances, making them unstable\n",
        "- D) Homoscedasticity is guaranteed\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** C) Multicollinearity can inflate coefficient variances, making them unstable\n",
        "\n",
        "**Explanation:**  \n",
        "When features are strongly correlated, the design matrix becomes ill-conditioned, complicating the estimation of unique, stable coefficients. The model can still fit but each weight might become highly sensitive to small data changes.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Question:**\n",
        "You fit a **linear regression** using ordinary least squares. One assumption is that the errors have **constant variance** (homoscedasticity). What pattern in a residuals vs. predicted-values plot might suggest a **violation** of this assumption?\n",
        "\n",
        "- A) Residuals scattered randomly without structure\n",
        "- B) Residuals forming a funnel shape, narrowing for small predictions and widening for large predictions\n",
        "- C) A horizontal band of uniform residual magnitudes\n",
        "- D) All residuals lying exactly on zero\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) Residuals forming a funnel shape, narrowing for small predictions and widening for large predictions\n",
        "\n",
        "**Explanation:**  \n",
        "A funnel or “cone” shape in residual plots indicates the variance of errors changes with predicted values, suggesting heteroscedasticity—an assumption violation for standard linear regression.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Question:**\n",
        "One typical assumption of **linear regression** is that the model is “linear in parameters.” Which scenario violates that?\n",
        "\n",
        "- A) A model $\\hat{y} = w_1 x + w_2 x^2 + b$, which is linear in $w_1, w_2$\n",
        "- B) A model $\\hat{y} = w_1 \\sin(x) + b$, still linear in $w_1$\n",
        "- C) A model $\\hat{y} = w_1^2 x + b$, where the parameter squared multiplies $x$\n",
        "- D) A model $\\hat{y} = w_1 x_1 + w_2 x_2 + b$, linear in $w_1, w_2$\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** C) A model $\\hat{y} = w_1^2 x + b$, where the parameter squared multiplies $x$\n",
        "\n",
        "**Explanation:**  \n",
        "“Linear in parameters” means the function is linear with respect to the **coefficients**, even if in terms of $x$ it can be polynomial or sinusoidal. But if the parameter itself appears in a nonlinear way (like $w_1^2$), it breaks that assumption.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Question:**\n",
        "You suspect **outliers** are heavily influencing your ordinary least squares linear regression. Which scenario is more indicative that outliers might distort the slope significantly?\n",
        "\n",
        "- A) The residuals are all zero\n",
        "- B) A few points with extremely large residuals, shifting the best-fit line away from the majority\n",
        "- C) All points lie perfectly on a straight line\n",
        "- D) The model’s R-squared is 1.0\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) A few points with extremely large residuals, shifting the best-fit line away from the majority\n",
        "\n",
        "**Explanation:**  \n",
        "Outliers can disproportionately affect OLS solutions, because the squared error gives heavy weight to large residuals. If a few points have huge errors, they can drastically pull the regression line.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Question:**\n",
        "A linear regression model is fitted. You check the residuals vs. each feature separately. One feature’s residual plot shows a clear wave pattern. Which statement is most plausible regarding model assumptions?\n",
        "\n",
        "- A) There is no violation; wave patterns are normal in residuals\n",
        "- B) The linear assumption for that feature might be incomplete, or another transformation is needed\n",
        "- C) This pattern enforces homoscedasticity\n",
        "- D) Perfect linear relationships produce wave-like residuals\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) The linear assumption for that feature might be incomplete, or another transformation is needed\n",
        "\n",
        "**Explanation:**  \n",
        "If residuals vs. a particular feature show a systematic shape (like waves, curves), it suggests that the relationship is not purely linear. The model might miss a polynomial or other form for that feature.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Question:**\n",
        "You do multiple linear regression on a large dataset. The regression performs extremely well on training data, but test error is much higher. In the context of linear regression, which factor might be **most** responsible?\n",
        "\n",
        "- A) The data is perfectly linear\n",
        "- B) Overfitting due to excessive features or combinations\n",
        "- C) Residuals all vanish on test data\n",
        "- D) The intercept is negative\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) Overfitting due to excessive features or combinations\n",
        "\n",
        "**Explanation:**  \n",
        "Even linear regression can overfit if there are many features relative to sample size (or various polynomial expansions). This can produce a near-perfect fit on training but high test error. The same phenomenon is a high variance scenario.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 8. **Question:**\n",
        "In linear regression, assume residuals are **normally** distributed with mean zero. Which statement about the distribution of **features** is typically **not** required?\n",
        "\n",
        "- A) Features must be linearly independent\n",
        "- B) Features must be normally distributed\n",
        "- C) Features must be numeric\n",
        "- D) Residuals must have constant variance\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) Features must be normally distributed\n",
        "\n",
        "**Explanation:**  \n",
        "Classical linear regression doesn’t demand each feature’s distribution is normal. The key assumption is about residuals (errors), not necessarily about how the features themselves are distributed. They should be numeric or convertible to numeric, but no normality requirement for X.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 9. **Question:**\n",
        "You apply linear regression. The training error is moderate, but the model generalizes well to new data (similar moderate test error). Which phenomenon does this reflect?\n",
        "\n",
        "- A) Underfitting with high bias, but good generalization\n",
        "- B) Overfitting with huge variance\n",
        "- C) Both training and test error are zero\n",
        "- D) The residual distribution is guaranteed to be uniform\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) Underfitting with high bias, but good generalization\n",
        "\n",
        "**Explanation:**  \n",
        "If the model underfits, it can’t reduce training error to very low. However, if training and test performances are similar, it indicates stable generalization (not high variance). That scenario typically suggests a simpler model with consistent moderate error across sets—higher bias, lower variance.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 10. **Question:**\n",
        "A linear regression has a coefficient $\\beta_2$ that’s extremely large in magnitude, but the dataset is small and the design matrix is nearly singular. Which explanation best fits?\n",
        "\n",
        "- A) Because there's perfect collinearity, the model’s weight can blow up to huge values to compensate\n",
        "- B) A large coefficient always means minimal test error\n",
        "- C) If a matrix is singular, the model forcibly sets all weights to zero\n",
        "- D) That scenario is only possible if R-squared = 1\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) Because there's perfect collinearity, the model’s weight can blow up to huge values to compensate\n",
        "\n",
        "**Explanation:**  \n",
        "When the columns of X are almost linearly dependent, OLS solutions can yield huge coefficients (multicollinearity). Minor changes in data lead to large coefficient swings. A nearly singular design matrix is a prime cause of extreme coefficient magnitudes.\n",
        "\n",
        "---\n",
        "\n",
        "### 11. **Question:**\n",
        "A regression model has **low training error** but a much **higher test error**. Which statement about bias and variance is most consistent with this situation?\n",
        "\n",
        "- A) It suggests the model has *high bias* and *low variance*\n",
        "- B) It suggests the model is *overfitting* with *low bias* but *high variance*\n",
        "- C) It suggests a perfect balance of bias and variance\n",
        "- D) It implies the data is strictly linear\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) It suggests the model is *overfitting* with *low bias* but *high variance*\n",
        "\n",
        "**Explanation:**  \n",
        "When training error is significantly lower than test error, it usually indicates overfitting. That corresponds to a scenario of low bias (the model fits training data extremely well) but high variance (the model doesn’t generalize well to unseen data).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 12. **Question:**\n",
        "You add more features to a multiple linear regression. The unadjusted R² rises slightly, but the **adjusted R²** remains stagnant or even drops. Which conclusion is more justified?\n",
        "\n",
        "- A) The additional features meaningfully improve the generalization of the model\n",
        "- B) The model is definitely underfitting by ignoring new features\n",
        "- C) The new features do *not* provide enough explanatory power to offset the penalty for more parameters, indicating they may be unhelpful or leading to potential overfitting\n",
        "- D) The adjusted R² is always lower by definition\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** C) The new features do *not* provide enough explanatory power to offset the penalty for more parameters, indicating they may be unhelpful or leading to potential overfitting\n",
        "\n",
        "**Explanation:**  \n",
        "Adjusted R² penalizes for adding parameters that don’t significantly improve the model. If unadjusted R² climbs but adjusted R² stagnates or falls, those features aren’t truly beneficial, possibly indicating overfitting or minimal actual contribution.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 13. **Question:**\n",
        "You suspect a **high-bias** (underfitting) situation: training and test errors are large and about the same. Which remedy is often most effective?\n",
        "\n",
        "- A) Use a simpler model\n",
        "- B) Reduce the number of features\n",
        "- C) Increase model capacity (e.g., add polynomial features) or reduce regularization, allowing the model to learn more complex patterns\n",
        "- D) Add random noise to the data\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** C) Increase model capacity (e.g., add polynomial features) or reduce regularization, allowing the model to learn more complex patterns\n",
        "\n",
        "**Explanation:**  \n",
        "High bias means the model is too simple to capture the data’s complexity. Typically, you address it by increasing model flexibility (more parameters, polynomial expansions) so it can better fit or by weakening strong regularization that’s constraining the model.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 14. **Question:**\n",
        "You compare two regression models: Model A has R²=0.90 but an **adjusted R²**=0.60, whereas Model B has R²=0.85 and adjusted R²=0.74. If your goal is robust generalization, which model is *likely* preferable and why?\n",
        "\n",
        "- A) Model A is clearly better because 0.90 > 0.85\n",
        "- B) Model B, because a higher adjusted R² suggests it’s more genuinely explanatory relative to its number of parameters, indicating possibly less overfitting\n",
        "- C) Model A because it has the largest difference between R² and adjusted R²\n",
        "- D) Both models are worthless with those metrics\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) Model B, because a higher adjusted R² suggests it’s more genuinely explanatory relative to its number of parameters, indicating possibly less overfitting\n",
        "\n",
        "**Explanation:**  \n",
        "Model A’s big R² drop from 0.90 to 0.60 indicates many extra features may not truly help. Model B’s smaller drop from 0.85 to 0.74 implies it’s balancing complexity vs. explanatory power better, often meaning better generalization.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 15. **Question:**\n",
        "A colleague claims “If a model’s training error is high, it must have high variance.” Which subtlety might correct their perspective?\n",
        "\n",
        "- A) High training error usually indicates underfitting (high bias), not high variance\n",
        "- B) Any large training error means zero bias\n",
        "- C) Variance is always zero in linear regression\n",
        "- D) Overfitting is proven by large training error\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) High training error usually indicates underfitting (high bias), not high variance\n",
        "\n",
        "**Explanation:**  \n",
        "When training error is large, the model isn’t fitting even the training data well, which is typically a sign of high bias or underfitting, rather than high variance. High variance is typically indicated by a large discrepancy between train vs. test performance.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 16. **Question:**\n",
        "You add polynomial features to reduce bias. After doing so, you see training error drop significantly and test error rise. Which statement best reflects the outcome?\n",
        "\n",
        "- A) The model is likely overfitting: it has now *lower bias* but *higher variance*, as indicated by the increased test error\n",
        "- B) The model is underfitting, so we need even higher-degree polynomials\n",
        "- C) Additional features always decrease test error\n",
        "- D) A large gap between training and test error means reduced variance\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) The model is likely overfitting: it has now *lower bias* but *higher variance*, as indicated by the increased test error\n",
        "\n",
        "**Explanation:**  \n",
        "When you add complexity (e.g., polynomial terms) and observe a big drop in training error but a jump in test error, that’s a classical sign of overfitting: the model is capturing training details at the cost of generalization.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 17. **Question:**\n",
        "If a model has extremely high **variance**, which training vs. test error pattern is most characteristic?\n",
        "\n",
        "- A) Both training and test errors are large and very close\n",
        "- B) Training error is low, but test error is quite large\n",
        "- C) Both training and test errors are near zero\n",
        "- D) Training error is huge, test error is small\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) Training error is low, but test error is quite large\n",
        "\n",
        "**Explanation:**  \n",
        "High variance typically means the model can closely adapt to the training set (achieving low training error) but fails to generalize, producing high test error. That’s the hallmark sign of overfitting.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 18. **Question:**\n",
        "You have a regression with R²=0.85. You add two more features, and R² jumps to 0.90. However, the adjusted R² moves from 0.84 to 0.81. Which is the **best** interpretation?\n",
        "\n",
        "- A) The new features do not truly improve the model’s explanatory power enough to justify the extra complexity, possibly indicating overfitting\n",
        "- B) The new features guarantee better real-world performance\n",
        "- C) Adjusted R² is always higher than R²\n",
        "- D) The model’s slope must be negative\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) The new features do not truly improve the model’s explanatory power enough to justify the extra complexity, possibly indicating overfitting\n",
        "\n",
        "**Explanation:**  \n",
        "Although R² improved (which always can happen when adding features), the adjusted R² decreased, signifying that the improvement in fit might be trivial and not worth the penalty for more parameters. Overfitting or minimal net gain is likely.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 19. **Question:**\n",
        "You train a linear regression on 50 data points with 40 features. The model’s training error is near zero. Which phenomenon or metric might best confirm the suspicion of overfitting?\n",
        "\n",
        "- A) Adjusted R² remains extremely high as well\n",
        "- B) A smaller difference between R² and adjusted R²\n",
        "- C) A large test error or a big drop in adjusted R² relative to R²\n",
        "- D) More features always ensure less overfitting\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** C) A large test error or a big drop in adjusted R² relative to R²\n",
        "\n",
        "**Explanation:**  \n",
        "When you have nearly as many features as data points, it’s easy to fit training data almost perfectly, risking major overfitting. A large test error or a strong divergence between unadjusted R² (which could be near 1) and adjusted R² are red flags. If adjusted R² is significantly lower or test error is high, it indicates overfitting.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 20. **Question:**\n",
        "You believe your model is **underfitting**. However, your colleague notes the *adjusted R²* is relatively high. Which statement might resolve this apparent contradiction?\n",
        "\n",
        "- A) Adjusted R² can be misleading if the form of the relationship is not properly captured by the linear part (non-linear pattern missed), so you can still underfit\n",
        "- B) A high adjusted R² means zero underfitting\n",
        "- C) Underfitting never happens once you have a high R²\n",
        "- D) The model must have the correct polynomial expansions\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) Adjusted R² can be misleading if the form of the relationship is not properly captured by the linear part (non-linear pattern missed), so you can still underfit\n",
        "\n",
        "**Explanation:**  \n",
        "Even if adjusted R² is somewhat high, the model might be missing key nonlinearities, suggesting underfitting in the sense of shape mismatch. The metric doesn’t ensure the correct functional form, it just penalizes the number of parameters; it can still fail if the actual data pattern is more complex than a linear structure.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 21. **Question:**\n",
        "Your linear regression obtains training MSE=10, test MSE=12. Another more complex model obtains training MSE=3, test MSE=25. Which indicates the second model?\n",
        "\n",
        "- A) Lower variance but higher bias\n",
        "- B) Likely overfitting, with much better training fit but worse test performance\n",
        "- C) Perfectly capturing linear assumptions\n",
        "- D) Equal generalization\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) Likely overfitting, with much better training fit but worse test performance\n",
        "\n",
        "**Explanation:**  \n",
        "Model 2 drastically improves training error from 10 down to 3, but test error jumps from 12 up to 25, consistent with higher variance and overfitting.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 22. **Question:**\n",
        "You’re deciding if you should add an additional feature to your linear regression. It increases R² from 0.70 to 0.72. Meanwhile, your adjusted R² changes from 0.69 to 0.68. Which approach is typically safer?\n",
        "\n",
        "- A) Keep the feature because R² always outranks adjusted R²\n",
        "- B) Discard the feature: adjusted R² dropping implies the feature is not beneficial enough for generalization\n",
        "- C) Add more features until R² stops rising\n",
        "- D) Force the coefficient to be zero\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) Discard the feature: adjusted R² dropping implies the feature is not beneficial enough for generalization\n",
        "\n",
        "**Explanation:**  \n",
        "Even though R² rose slightly, the adjusted R² penalizes extra parameters. If it goes down, it indicates the feature doesn’t provide sufficient benefit, potentially leading to overfitting. Typically, you trust adjusted R² in such a scenario to ensure robust modeling.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 23. **Question:**\n",
        "A regression model is found to have a **low bias** but extremely **high variance**. Which training vs. test error pattern is consistent with that?\n",
        "\n",
        "- A) Training error 2, test error 5\n",
        "- B) Training error 1, test error 30\n",
        "- C) Training error 25, test error 24\n",
        "- D) Training error 15, test error 2\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) Training error 1, test error 30\n",
        "\n",
        "**Explanation:**  \n",
        "Low bias means the model can fit training data extremely well (low training error). High variance means it fails to generalize, so test error is significantly higher. The biggest gap is 1 vs. 30 among these choices, best fitting that pattern.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 24. **Question:**\n",
        "You have a dataset with some polynomial relationship. A purely linear model yields training MSE=15, test MSE=16. A 4th-degree polynomial model yields training MSE=1, test MSE=28. This difference is best explained by:\n",
        "\n",
        "- A) The linear model is probably underfitting but generalizes better than the 4th-degree which is overfitting\n",
        "- B) The 4th-degree model is definitely underfitting\n",
        "- C) The linear model must have infinite variance\n",
        "- D) The polynomial model has minimal variance\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) The linear model is probably underfitting but generalizes better than the 4th-degree which is overfitting\n",
        "\n",
        "**Explanation:**  \n",
        "One can see the linear model has moderate errors but consistent performance from train to test (a smaller gap), indicating less variance, albeit some bias. The polynomial gets a very low training error but big test error, showing overfitting. So it’s a high-variance fit.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 25. **Question:**\n",
        "After training a linear regression, you want to interpret the results. You see a fairly high R² (0.85). However, upon checking **adjusted R²**, it’s 0.50. Which is the best conclusion?\n",
        "\n",
        "- A) The model might be overfitting or adding many features that only marginally help. A large drop indicates the model’s effective explanatory power is weaker than raw R² suggests\n",
        "- B) The model’s performance is guaranteed to be perfect\n",
        "- C) Adjusted R² should always equal R²\n",
        "- D) We must remove the intercept\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) The model might be overfitting or adding many features that only marginally help. A large drop indicates the model’s effective explanatory power is weaker than raw R² suggests\n",
        "\n",
        "**Explanation:**  \n",
        "The big gap between R² and adjusted R² implies the number of parameters is large relative to how much extra variance they explain. This is a red flag for potential overfitting or trivial benefit from additional features.\n",
        "\n",
        "---\n",
        "\n",
        "### 26. **Question:**\n",
        "In an $\\ell_2$-regularized linear model ($w_1, w_2, \\dots$), which direct effect does the penalty term $\\lambda \\sum_j w_j^2$ impose?\n",
        "\n",
        "- A) It penalizes the absolute size of each weight by its sign, driving some to exactly zero\n",
        "- B) It shrinks all weights smoothly towards smaller magnitudes but rarely forces them exactly to zero\n",
        "- C) It disregards weight magnitudes and focuses only on the bias\n",
        "- D) It allows weights to grow unbounded\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) It shrinks all weights smoothly towards smaller magnitudes but rarely forces them exactly to zero\n",
        "\n",
        "**Explanation:**  \n",
        "$\\ell_2$-regularization (Ridge) adds a penalty proportional to the square of the weight magnitudes, leading to a smoother shrinkage effect. It normally does not produce exact zeros in coefficients, just smaller values.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 27. **Question:**\n",
        "You’re using **L1** regularization ($ \\lambda \\sum |w_j|$). One commonly noted property is:\n",
        "\n",
        "- A) It smooths weights but never sets any to zero\n",
        "- B) It can force some weight coefficients exactly to zero, enabling feature selection\n",
        "- C) It doesn’t affect overfitting\n",
        "- D) It penalizes squared weight magnitudes\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) It can force some weight coefficients exactly to zero, enabling feature selection\n",
        "\n",
        "**Explanation:**  \n",
        "$\\ell_1$ regularization (Lasso) is known for creating sparsity in the model by pushing certain coefficients to zero, effectively removing unimportant features from the model. This is a distinct property versus $\\ell_2$.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 28. **Question:**\n",
        "A linear model’s objective function includes $\\ell_2$ penalty. During training, if a weight $w_k$ is large in magnitude, the penalty w.r.t. that weight will:\n",
        "\n",
        "- A) Be linear in $w_k$\n",
        "- B) Increase quadratically with $|w_k|$, encouraging it to shrink more strongly the larger it becomes\n",
        "- C) Remain constant\n",
        "- D) Drive $w_k$ instantly to zero\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) Increase quadratically with $|w_k|$, encouraging it to shrink more strongly the larger it becomes\n",
        "\n",
        "**Explanation:**  \n",
        "$\\ell_2$ regularization penalizes the sum of squares of weights, so large weights are penalized heavily, pushing them to shrink. However, they typically do not become exactly zero as they do with $\\ell_1$.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 29. **Question:**\n",
        "Comparing $\\ell_1$ vs. $\\ell_2$ regularization in a linear regression:\n",
        "\n",
        "- A) $\\ell_1$ fosters many non-zero weights, while $\\ell_2$ often zeroes them\n",
        "- B) $\\ell_2$ commonly yields a sparse solution, while $\\ell_1$ keeps all weights nonzero\n",
        "- C) $\\ell_1$ can produce sparse solutions (some coefficients exactly 0), while $\\ell_2$ typically yields all small but non-zero coefficients\n",
        "- D) Both force exactly the same effect\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** C) $\\ell_1$ can produce sparse solutions (some coefficients exactly 0), while $\\ell_2$ typically yields all small but non-zero coefficients\n",
        "\n",
        "**Explanation:**  \n",
        "$\\ell_1$ or Lasso can create actual zeros in the coefficient vector, providing feature selection. $\\ell_2$ or Ridge typically shrinks weights but doesn’t force them exactly to zero, leading to smaller but mostly non-zero coefficients.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 30. **Question:**\n",
        "If a model with $\\ell_1$ regularization is tuned to have a very large regularization coefficient $\\lambda$, which outcome is likely?\n",
        "\n",
        "- A) Many coefficients collapse to exactly zero, possibly ignoring important features if $\\lambda$ is too high\n",
        "- B) We produce extremely large coefficients\n",
        "- C) Overfitting becomes guaranteed\n",
        "- D) No change occurs in the weight magnitudes\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) Many coefficients collapse to exactly zero, possibly ignoring important features if $\\lambda$ is too high\n",
        "\n",
        "**Explanation:**  \n",
        "A large $\\ell_1$ penalty strongly enforces sparsity, zeroing out many weights. If $\\lambda$ is too large, the model can oversimplify (underfit) by discarding too many features.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 31. **Question:**\n",
        "Why might a practitioner prefer **Ridge** ($\\ell_2$) regularization over **Lasso** ($\\ell_1$) in some contexts?\n",
        "\n",
        "- A) Lasso can handle collinearity better than Ridge\n",
        "- B) Ridge is differentiable at all weight values (including zero) and typically handles collinear features more smoothly, not forcing any single weight to zero\n",
        "- C) Ridge always sets at least half the weights to zero\n",
        "- D) Lasso offers smoother gradient\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) Ridge is differentiable at all weight values (including zero) and typically handles collinear features more smoothly, not forcing any single weight to zero\n",
        "\n",
        "**Explanation:**  \n",
        "Ridge’s penalty $\\|w\\|_2^2$ is differentiable everywhere, making optimization straightforward. It also distributes shrinkage among correlated features rather than picking one. Lasso’s absolute value penalty is not differentiable at zero, though optimization methods still handle it (e.g., coordinate descent), but it does produce sparsity.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 32. **Question:**\n",
        "When applying Lasso regression with a moderately sized $\\lambda$, you notice several weights become zero. From the perspective of **overfitting**:\n",
        "\n",
        "- A) Zero weights reduce model complexity, potentially mitigating overfitting\n",
        "- B) Setting coefficients to zero always means the model is overfit\n",
        "- C) $\\ell_1$ penalty always increases variance\n",
        "- D) Large $\\ell_1$ penalty never leads to zero weights\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) Zero weights reduce model complexity, potentially mitigating overfitting\n",
        "\n",
        "**Explanation:**  \n",
        "Zero weights effectively remove those features, simplifying the model. Simpler models (fewer effective parameters) often reduce variance and risk of overfitting, thus can help generalization.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 33. **Question:**\n",
        "Your linear regression has a Ridge penalty. You find that the solution yields small but nonzero weights for all features, even irrelevant ones. Why doesn’t Ridge produce many exact zeros?\n",
        "\n",
        "- A) Because $\\ell_2$ penalty encourages smooth shrinkage but doesn’t induce strict sparsity\n",
        "- B) Because the sum of squares is always negative\n",
        "- C) Because $\\ell_2$ penalty is linear\n",
        "- D) Because zero is not permissible for the bias term\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) Because $\\ell_2$ penalty encourages smooth shrinkage but doesn’t induce strict sparsity\n",
        "\n",
        "**Explanation:**  \n",
        "$\\ell_2$ penalty (Ridge) exerts a *quadratic* shrink effect. Coefficients are pushed towards smaller magnitudes but typically remain nonzero. By contrast, $\\ell_1$ can create exact zeros because of its absolute value shape.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 34. **Question:**\n",
        "A data scientist tries both Lasso and Ridge on the same dataset. Lasso sets half the features’ weights to zero, while Ridge keeps them all non-zero but smaller. Which scenario might be best if the data truly has many irrelevant features?\n",
        "\n",
        "- A) Lasso is beneficial here for automatic feature elimination, possibly giving better interpretability and similar or better test performance\n",
        "- B) Ridge is always superior, ignoring irrelevant features\n",
        "- C) Both methods create exactly identical solutions\n",
        "- D) There’s no difference in interpretability\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) Lasso is beneficial here for automatic feature elimination, possibly giving better interpretability and similar or better test performance\n",
        "\n",
        "**Explanation:**  \n",
        "If many features are genuinely irrelevant, Lasso can zero them out, simplifying the model and improving interpretability. This can also help mitigate overfitting from spurious features. Ridge tends to keep all features but with shrunk weights, which might not be as interpretable.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 35. **Question:**\n",
        "In a linear regression with **L1 + L2** penalty (Elastic Net), you discover the L1 ratio is large. Which effect is most prominent?\n",
        "\n",
        "- A) The model is purely a standard linear regression\n",
        "- B) Stronger push for sparsity due to the L1 portion\n",
        "- C) We only get L2-like shrinkage\n",
        "- D) The penalty is negative\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) Stronger push for sparsity due to the L1 portion\n",
        "\n",
        "**Explanation:**  \n",
        "Elastic Net combines $\\ell_1$ and $\\ell_2$. A larger L1 ratio means more emphasis on the L1 component, thus encouraging more zeros. The L2 portion still does smoothing. So a large L1 ratio primarily fosters sparsity.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 36. **Question:**\n",
        "If your dataset is small and you want to *avoid large swings in coefficients*, you might choose:\n",
        "\n",
        "- A) No regularization, for maximum capacity\n",
        "- B) Ridge ($\\ell_2$) regularization, which typically stabilizes weights and reduces variance\n",
        "- C) L1 regularization for immediate zeroing of all weights\n",
        "- D) A random assignment of weights at inference\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) Ridge ($\\ell_2$) regularization, which typically stabilizes weights and reduces variance\n",
        "\n",
        "**Explanation:**  \n",
        "With limited data, a purely unconstrained model might overfit, producing large, unstable coefficients. Ridge’s $\\ell_2$ penalty specifically helps keep coefficients stable (smaller and more uniform), reducing variance, which is beneficial in small-sample scenarios.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 37. **Question:**\n",
        "You see a massive gap between training and test errors. One approach is to apply **L2** penalty. Why might this help reduce that gap?\n",
        "\n",
        "- A) L2 penalty eliminates features entirely\n",
        "- B) Smoothing the weights (reducing their magnitude) helps avoid overfitting’s large test error\n",
        "- C) It always sets training error to zero\n",
        "- D) The cost function becomes non-convex\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) Smoothing the weights (reducing their magnitude) helps avoid overfitting’s large test error\n",
        "\n",
        "**Explanation:**  \n",
        "When the model overfits (high variance), it typically has large or extreme coefficient values for certain features. The $\\ell_2$ penalty shrinks those, making the model less sensitive to data noise. This can align training and test performance better, decreasing the gap.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 38. **Question:**\n",
        "An ML engineer uses Lasso on 100 features, sees 80 coefficients are driven to exactly zero, and the test error is decent. Which statement about interpretability is accurate?\n",
        "\n",
        "- A) It is unclear which features matter because all are zero\n",
        "- B) The 20 non-zero features are presumably the *most* relevant, giving a simpler, more interpretable model\n",
        "- C) L1 penalty always leads to worse interpretability\n",
        "- D) The model is guaranteed to be overfitting\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) The 20 non-zero features are presumably the *most* relevant, giving a simpler, more interpretable model\n",
        "\n",
        "**Explanation:**  \n",
        "L1 (Lasso) can zero out many coefficients, effectively selecting a subset of features. That subset typically contains the most influential features, making the model simpler and more interpretable than one that uses all 100 features.\n",
        "\n",
        "---\n",
        "\n",
        "### 39. **Question:**\n",
        "In **logistic regression**, the model predicts $\\hat{p} = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b)$, where $\\sigma$ is the sigmoid function. Why not directly use a linear regression formula $\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b$ for classification?\n",
        "\n",
        "- A) A linear output can only produce values in [0,1]\n",
        "- B) Logistic regression’s sigmoid ensures probabilities remain between 0 and 1, whereas a raw linear output might produce invalid probabilities (<0 or >1)\n",
        "- C) The linear model easily saturates at 0 or 1 for classification\n",
        "- D) Logistic regression doesn’t require any features\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) Logistic regression’s sigmoid ensures probabilities remain between 0 and 1, whereas a raw linear output might produce invalid probabilities (<0 or >1)\n",
        "\n",
        "**Explanation:**  \n",
        "Classification often needs an output interpreted as probability, so we constrain outputs to $[0,1]$. A linear output can exceed these bounds. The sigmoid $\\sigma(z)= 1/(1+ e^{-z})$ ensures valid probability predictions.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 40. **Question:**\n",
        "Which statement best characterizes the **decision boundary** in logistic regression with a single input $x$?\n",
        "\n",
        "- A) It’s determined by $\\mathbf{w}^\\top \\mathbf{x}+ b =0$, the point where predicted probability is exactly 0.5\n",
        "- B) It’s a horizontal line at $\\hat{p}=0.5$\n",
        "- C) It changes arbitrarily for each data point\n",
        "- D) The decision boundary never depends on $\\mathbf{w}$\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) It’s determined by $\\mathbf{w}^\\top \\mathbf{x}+ b =0$, the point where predicted probability is exactly 0.5\n",
        "\n",
        "**Explanation:**  \n",
        "Logistic regression classifies a sample as “1” if $\\hat{p}>0.5$. The boundary is thus $\\hat{p}=0.5$. Since $\\hat{p}= \\sigma(\\mathbf{w}^\\top\\mathbf{x}+ b)$, $\\sigma(z)=0.5$ exactly at $z=0$. Therefore, the boundary is $\\mathbf{w}^\\top \\mathbf{x}+ b=0$.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 41. **Question:**\n",
        "When training logistic regression, we typically minimize **cross-entropy** (log loss) rather than MSE. Which subtle reason supports cross-entropy for classification?\n",
        "\n",
        "- A) MSE is perfectly suitable for classification, offering the same gradient\n",
        "- B) The gradient from MSE can lead to slower convergence or non-optimal updates for the sigmoid, while cross-entropy aligns with the likelihood interpretation, providing more stable, direct gradient signals for probability estimates\n",
        "- C) MSE ensures linear boundaries\n",
        "- D) Cross-entropy only applies to regression tasks\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) The gradient from MSE can lead to slower convergence or non-optimal updates for the sigmoid, while cross-entropy aligns with the likelihood interpretation, providing more stable, direct gradient signals for probability estimates\n",
        "\n",
        "**Explanation:**  \n",
        "Cross-entropy loss arises from the maximum likelihood principle for Bernoulli data. It yields gradients well-suited to logistic regression, typically converging faster and more reliably than MSE, which can produce problematic gradient behavior with a sigmoid.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 42. **Question:**\n",
        "In logistic regression, the **log-odds** output $\\mathbf{w}^\\top \\mathbf{x}+b$ is interpreted how?\n",
        "\n",
        "- A) It’s the direct probability of class 1\n",
        "- B) It’s $\\log\\frac{\\hat{p}}{1-\\hat{p}}$, the logarithm of the predicted odds for class 1\n",
        "- C) It’s always bounded between 0 and 1\n",
        "- D) A negative log-odds must equal zero probability\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) It’s $\\log\\frac{\\hat{p}}{1-\\hat{p}}$, the logarithm of the predicted odds for class 1\n",
        "\n",
        "**Explanation:**  \n",
        "Logistic regression’s linear combination $\\mathbf{w}^\\top \\mathbf{x}+ b$ equals the log-odds of class 1, i.e. $\\log(\\hat{p}/(1-\\hat{p}))$. Then $\\hat{p}$ is found by applying the sigmoid function $\\sigma$.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 43. **Question:**\n",
        "A logistic regression classifier yields training accuracy near 100%, but test accuracy is only around 60%. Which phenomenon is indicated?\n",
        "\n",
        "- A) **Overfitting**: the model likely memorized training samples but generalizes poorly\n",
        "- B) **Underfitting**: the model is too simple\n",
        "- C) Perfect bias with no variance\n",
        "- D) The model is guaranteed to have a linear boundary\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) **Overfitting**: the model likely memorized training samples but generalizes poorly\n",
        "\n",
        "**Explanation:**  \n",
        "A large gap between near-perfect training accuracy and significantly lower test accuracy typically signals overfitting. The logistic regressor is capturing training specifics that do not hold on unseen data.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 44. **Question:**\n",
        "When using logistic regression on data with **highly imbalanced classes** (e.g., 95% negative, 5% positive), which subtle pitfall might occur if we rely on standard accuracy?\n",
        "\n",
        "- A) Accuracy is unaffected by class imbalance\n",
        "- B) The model might just predict the majority class, achieving high accuracy but ignoring the minority class, leading to poor utility\n",
        "- C) The logistic function saturates for minority classes\n",
        "- D) Overfitting is guaranteed\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) The model might just predict the majority class, achieving high accuracy but ignoring the minority class, leading to poor utility\n",
        "\n",
        "**Explanation:**  \n",
        "In highly imbalanced datasets, the model can output the majority label almost every time and still get high accuracy. This misleads performance if the minority class is the one we care about. Other metrics (e.g., F1, AUC) help capture minority class performance better.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 45. **Question:**\n",
        "Which statement about logistic regression’s **cost function** is correct?\n",
        "\n",
        "- A) The cost is typically the sum of squared errors between predicted probability and label\n",
        "- B) It’s usually a cross-entropy (log loss) that punishes confident wrong predictions heavily, aligning with the maximum likelihood for Bernoulli\n",
        "- C) No cost function is needed; logistic regression is always solved by linear algebra\n",
        "- D) It uses a hinge loss\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) It’s usually a cross-entropy (log loss) that punishes confident wrong predictions heavily, aligning with the maximum likelihood for Bernoulli\n",
        "\n",
        "**Explanation:**  \n",
        "Logistic regression uses cross-entropy (logistic) loss, derived from the negative log-likelihood perspective. It heavily penalizes wrong predictions if the model is very confident, guiding parameter updates accordingly.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 46. **Question:**\n",
        "You find that in your logistic regression, a certain feature $x_j$ has a very **large positive** weight. Interpreting this weight:\n",
        "\n",
        "- A) A large positive coefficient indicates that higher values of $x_j$ strongly push the log-odds toward class 1, raising the probability of predicting class 1\n",
        "- B) The sign is irrelevant for classification\n",
        "- C) It means that feature is never used\n",
        "- D) A large positive weight sets predicted probability to zero\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) A large positive coefficient indicates that higher values of $x_j$ strongly push the log-odds toward class 1, raising the probability of predicting class 1\n",
        "\n",
        "**Explanation:**  \n",
        "In logistic regression, a big positive weight means that as $x_j$ increases, $\\mathbf{w}^\\top\\mathbf{x}+ b$ grows, thus the sigmoid output shifts closer to 1, significantly favoring class 1 predictions.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 47. **Question:**\n",
        "Why do we typically avoid using **mean squared error (MSE)** as a loss for logistic regression?\n",
        "\n",
        "- A) MSE is perfectly aligned with maximum likelihood for binary classification\n",
        "- B) MSE can lead to less stable gradients, slower convergence with the sigmoid function, and lacks the probabilistic interpretation that cross-entropy provides\n",
        "- C) MSE ensures faster training\n",
        "- D) MSE is not differentiable\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) MSE can lead to less stable gradients, slower convergence with the sigmoid function, and lacks the probabilistic interpretation that cross-entropy provides\n",
        "\n",
        "**Explanation:**  \n",
        "While MSE is differentiable, it’s not the typical choice for binary classification because it doesn’t match the Bernoulli likelihood perspective, leading to suboptimal gradient behavior. Cross-entropy is standard, giving better theoretical and practical results.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 48. **Question:**\n",
        "A logistic regression is run with no regularization on high-dimensional data. It achieves extremely **low training error**. Which subtle check might you do to confirm the model is not overfitting?\n",
        "\n",
        "- A) Just check the final loss on training data\n",
        "- B) Verify a similarly low error on a separate validation or test set\n",
        "- C) Confirm the weights are large in magnitude\n",
        "- D) Ensure the log-odds are negative\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) Verify a similarly low error on a separate validation or test set\n",
        "\n",
        "**Explanation:**  \n",
        "A model with very low training error in high dimensions can easily overfit. The standard approach is checking performance on unseen (test or validation) data. If test performance remains good, it’s less likely to be overfitting.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 49. **Question:**\n",
        "When logistic regression’s **$\\mathbf{w}$ and $b$** yield predicted probabilities close to 1 or 0 for certain samples but are wrong, how does the cross-entropy loss respond?\n",
        "\n",
        "- A) It penalizes those misclassifications lightly\n",
        "- B) It’s indifferent to confident misclassifications\n",
        "- C) It imposes a **large** penalty on being confidently incorrect\n",
        "- D) It yields negative cost\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** C) It imposes a **large** penalty on being confidently incorrect\n",
        "\n",
        "**Explanation:**  \n",
        "Cross-entropy heavily punishes wrong predictions made with high confidence. If the model outputs probability ~1 for label=0 or ~0 for label=1, the log term in the loss function can blow up, penalizing the model’s parameters strongly.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 50. **Question:**\n",
        "Consider logistic regression for a 2-class problem. If the weight vector’s **norm** becomes large in magnitude, what happens to the decision boundary?\n",
        "\n",
        "- A) The boundary shifts so that small changes in $\\mathbf{x}$ produce large changes in predicted log-odds\n",
        "- B) It remains unaffected\n",
        "- C) The model always predicts 0.5\n",
        "- D) The boundary disappears\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) The boundary shifts so that small changes in $\\mathbf{x}$ produce large changes in predicted log-odds\n",
        "\n",
        "**Explanation:**  \n",
        "A large norm $\\|\\mathbf{w}\\|$ implies the model is sensitive: a small movement in $\\mathbf{x}$ significantly modifies $\\mathbf{w}^\\top \\mathbf{x}$. This can lead to a sharper transition around the boundary, making the model’s probability switch rapidly from 0 to 1 near the boundary line $\\mathbf{w}^\\top \\mathbf{x}+ b=0$.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 51. **Question:**\n",
        "Logistic regression typically uses an iterative algorithm (like gradient descent) instead of solving a closed-form normal equation. Why?\n",
        "\n",
        "- A) The cross-entropy objective with a sigmoid is non-linear, lacking a simple closed-form solution for $\\mathbf{w}$\n",
        "- B) It has no partial derivatives\n",
        "- C) The model is linear, so normal equations always exist\n",
        "- D) We prefer not to find the global optimum\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) The cross-entropy objective with a sigmoid is non-linear, lacking a simple closed-form solution for $\\mathbf{w}$\n",
        "\n",
        "**Explanation:**  \n",
        "The logistic function leads to a **non-linear** log-likelihood. There's no algebraic closed-form solution like in ordinary least squares. Hence, iterative numerical methods (gradient-based) are used.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 52. **Question:**\n",
        "If your logistic regression model’s **decision boundary** on a 2D feature space is highly curved, is that possible?\n",
        "\n",
        "- A) Yes, if you include feature transformations (like polynomial expansions), otherwise a single linear combination $\\mathbf{w}^\\top\\mathbf{x}+ b=0$ is always a line\n",
        "- B) No, logistic regression always yields a circular boundary\n",
        "- C) The boundary can be any random shape\n",
        "- D) The boundary is never linear in logistic regression\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) Yes, if you include feature transformations (like polynomial expansions), otherwise a single linear combination $\\mathbf{w}^\\top\\mathbf{x}+ b=0$ is always a line\n",
        "\n",
        "**Explanation:**  \n",
        "Vanilla logistic regression is linear in the original features, so the boundary is linear (a hyperplane). But if you transform your features (e.g., polynomial expansions), the boundary in original input space can appear curved. The model is still linear in transformed features, though.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 53. **Question:**\n",
        "A logistic regression’s predicted probability for an example is $\\hat{p}=0.7$. The label is 1. Intuitively, what happens in **gradient-based** training?\n",
        "\n",
        "- A) The model is slightly wrong since 0.7 < 1, but not drastically, so the gradient update for that sample is less severe compared to if $\\hat{p}$ were 0.1\n",
        "- B) The model sees no error if $\\hat{p}>0.5$\n",
        "- C) The cost is infinite\n",
        "- D) The model always sets $\\hat{p}=1$ after one update\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) The model is slightly wrong since 0.7 < 1, but not drastically, so the gradient update for that sample is less severe compared to if $\\hat{p}$ were 0.1\n",
        "\n",
        "**Explanation:**  \n",
        "Logistic regression’s cross-entropy punishes large deviations from the target. A probability 0.7 for a true label 1 is moderately off, so the update is smaller than if it predicted 0.1. The correct class is still “1,” so the error isn’t zero but not huge.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### 54. **Question:**\n",
        "In a binary classification task, **precision** measures which aspect?\n",
        "\n",
        "- A) Among all predicted positives, how many are truly positives  \n",
        "- B) Among all actual positives, how many are correctly identified  \n",
        "- C) How often a negative is incorrectly labeled as positive  \n",
        "- D) The fraction of negatives correctly identified as negatives  \n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) Among all predicted positives, how many are truly positives\n",
        "\n",
        "**Explanation:**  \n",
        "Precision = (True Positives) / (True Positives + False Positives). It focuses on the reliability of positive predictions.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 55. **Question:**\n",
        "A classifier yields a **recall** of 0.95 but only 0.50 precision. Interpreting this:\n",
        "\n",
        "- A) It finds 50% of actual positives, ignoring the rest\n",
        "- B) It correctly identifies 95% of actual positives, but also has many false positives\n",
        "- C) It mislabels 5% of all negatives as positives\n",
        "- D) It consistently misses half of the positives\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) It correctly identifies 95% of actual positives, but also has many false positives\n",
        "\n",
        "**Explanation:**  \n",
        "Recall = (TP) / (TP + FN). A recall of 0.95 means it captures 95% of all true positives. A lower precision (0.50) means half of its positive predictions are false. So the classifier is generous at detecting positives but not always correct about them.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 56. **Question:**\n",
        "You want a **single** metric that balances both precision and recall. Which standard measure is typically used?\n",
        "\n",
        "- A) F1 score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "- B) Accuracy = (TP + TN)/(All samples)\n",
        "- C) AUC from ROC curve\n",
        "- D) Cohen’s Kappa\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) F1 score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "\n",
        "**Explanation:**  \n",
        "F1 is the harmonic mean of precision and recall, giving them equal weighting. Accuracy can be misleading with class imbalance; ROC-AUC measures a different aspect, and Kappa is a separate agreement measure.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 57. **Question:**\n",
        "When generating an ROC curve, you vary a threshold on the classifier’s output probability. On the x-axis is the False Positive Rate (FPR), on the y-axis the True Positive Rate (TPR). If a point on the curve has TPR=1.0 but FPR=1.0, what does that imply?\n",
        "\n",
        "- A) The classifier detects all positives but also wrongly labels all negatives as positives\n",
        "- B) The classifier is perfectly discriminating\n",
        "- C) TPR=1.0 means no false positives\n",
        "- D) The classifier is predicting all instances as negative\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) The classifier detects all positives but also wrongly labels all negatives as positives\n",
        "\n",
        "**Explanation:**  \n",
        "TPR=1.0 means all positives are found (no false negatives). FPR=1.0 means no true negatives remain (all negatives are mislabeled as positive). So the classifier is effectively predicting everything as positive.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 58. **Question:**\n",
        "An ROC curve that is **diagonal** from (0,0) to (1,1) indicates:\n",
        "\n",
        "- A) The classifier is random, no better than chance  \n",
        "- B) The classifier is perfect with AUC=1.0  \n",
        "- C) The classifier always predicts the majority class  \n",
        "- D) The F1 score is guaranteed to be high  \n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) The classifier is random, no better than chance\n",
        "\n",
        "**Explanation:**  \n",
        "A diagonal ROC means that TPR ~ FPR at every threshold, i.e., no discriminative power beyond guessing. That yields AUC=0.5, indicating random performance.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 59. **Question:**\n",
        "Why might **precision** be misleadingly high on a heavily **imbalanced** dataset if you do not also check recall or other metrics?\n",
        "\n",
        "- A) Precision only measures how many predicted positives were correct, ignoring how many positives you missed, thus a model that rarely predicts positive can get high precision but poor recall\n",
        "- B) Precision always equals recall\n",
        "- C) High precision ensures no false negatives\n",
        "- D) With imbalance, accuracy is the only relevant metric\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) Precision only measures how many predicted positives were correct, ignoring how many positives you missed, thus a model that rarely predicts positive can get high precision but poor recall\n",
        "\n",
        "**Explanation:**  \n",
        "If the model predicts very few positives (some correct), it can show high precision but may fail to capture a large portion of actual positives. That’s why looking at recall or F1 can be crucial, especially with imbalanced data.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 60. **Question:**\n",
        "You have two binary classifiers, both with the same F1 score. However, classifier A has higher precision but lower recall, while classifier B has the opposite. Depending on business needs, which scenario might you prefer classifier A?\n",
        "\n",
        "- A) If missing positives is very costly, so we want high TPR\n",
        "- B) If we want minimal false positives, so high precision is key\n",
        "- C) If the dataset is balanced\n",
        "- D) If we only care about capturing all positives\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) If we want minimal false positives, so high precision is key\n",
        "\n",
        "**Explanation:**  \n",
        "Classifiers with high precision and lower recall produce fewer false positives but might miss more actual positives. If the priority is to avoid false alarms, one might choose the high-precision classifier.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 61. **Question:**\n",
        "An F1 score is extremely low, but the ROC AUC is surprisingly high. Which scenario best explains the discrepancy?\n",
        "\n",
        "- A) F1 focuses on thresholded predictions (precision/recall), while ROC AUC integrates a range of thresholds. It’s possible the classifier’s continuous scores separate classes well overall (leading to decent AUC) but at the chosen threshold, precision or recall is poor\n",
        "- B) The data must be linearly separable\n",
        "- C) F1 is always larger than ROC AUC\n",
        "- D) High AUC guarantees high F1\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) F1 focuses on thresholded predictions (precision/recall), while ROC AUC integrates a range of thresholds. It’s possible the classifier’s continuous scores separate classes well overall (leading to decent AUC) but at the chosen threshold, precision or recall is poor\n",
        "\n",
        "**Explanation:**  \n",
        "AUC measures how well scores rank positives vs. negatives across all thresholds. F1 is tied to a specific decision threshold. If the chosen threshold yields a poor trade-off between precision and recall, F1 can be low even if the underlying score separation is good (decent AUC).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 62. **Question:**\n",
        "**Micro-averaged** F1 vs. **Macro-averaged** F1 in multi-class classification differ in:\n",
        "\n",
        "- A) Micro-F1 aggregates global true/false positives across classes, thus weighting large classes more. Macro-F1 averages the F1 of each class equally\n",
        "- B) Both treat each class exactly the same\n",
        "- C) Macro-F1 never uses recall\n",
        "- D) Micro-F1 is always higher\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) Micro-F1 aggregates global true/false positives across classes, thus weighting large classes more. Macro-F1 averages the F1 of each class equally\n",
        "\n",
        "**Explanation:**  \n",
        "Micro-averaging pools all classes’ TP/FP/FN globally, giving bigger classes more influence. Macro-averaging computes F1 per class then averages them, treating each class equally regardless of size.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 63. **Question:**\n",
        "If you see a classifier with ROC AUC=0.95 but a **precision** of only 0.40 at a chosen threshold, how could you reconcile that?\n",
        "\n",
        "- A) The model must be random with no real discriminative power\n",
        "- B) The model can separate positives vs. negatives well in ranking, but the chosen threshold leads to many false positives, lowering precision\n",
        "- C) High AUC requires high precision\n",
        "- D) Precision and AUC are identical metrics\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) The model can separate positives vs. negatives well in ranking, but the chosen threshold leads to many false positives, lowering precision\n",
        "\n",
        "**Explanation:**  \n",
        "A high AUC means the rank ordering (score) is good overall. However, if the threshold is set in a way that yields many predicted positives (some false), precision can be relatively low, even though the overall ranking is quite discriminative.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 64. **Question:**\n",
        "Under severe class imbalance, the ROC curve can sometimes give an overly optimistic view of performance. Why do some prefer the **Precision-Recall** curve in that scenario?\n",
        "\n",
        "- A) Precision-Recall directly focuses on positives, ignoring the typically large number of negatives in the denominator of TPR\n",
        "- B) ROC is always invalid for imbalanced data\n",
        "- C) The PR curve is less stable\n",
        "- D) The negative class is never considered\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) Precision-Recall directly focuses on positives, ignoring the typically large number of negatives in the denominator of TPR\n",
        "\n",
        "**Explanation:**  \n",
        "With heavily imbalanced data, the false positive rate (in ROC) can remain low simply because negatives are plentiful. PR curves are more sensitive to how well the model identifies actual positives, so it can be more informative in that scenario.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 65. **Question:**\n",
        "A classifier yields confusion matrix:  \n",
        "- TP=30, FP=10, FN=20, TN=940.  \n",
        "What is the **precision**?\n",
        "\n",
        "- A) $\\tfrac{30}{30+20} = 0.60$\n",
        "- B) $\\tfrac{30}{30+10} = 0.75$\n",
        "- C) $\\tfrac{30}{30+20 +10} = 0.50$\n",
        "- D) $\\tfrac{30}{30+940} \\approx 0.03$\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) $\\tfrac{30}{30+10} = 0.75$\n",
        "\n",
        "**Explanation:**  \n",
        "Precision=TP/(TP+FP)= 30/(30+10)=30/40=0.75. This is how many predicted positives are actually positive.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 66. **Question:**\n",
        "Referring to the same confusion matrix (TP=30, FP=10, FN=20, TN=940), the **recall** is:\n",
        "\n",
        "- A) $\\tfrac{30}{30+10} = 0.75$\n",
        "- B) $\\tfrac{30}{30+20} = 0.60$\n",
        "- C) $\\tfrac{10}{30+20} = 0.33$\n",
        "- D) $\\tfrac{940}{30+20+10+940}\\approx 0.93$\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) $\\tfrac{30}{30+20} = 0.60$\n",
        "\n",
        "**Explanation:**  \n",
        "Recall= TP/(TP+FN)= 30/(30+20)=30/50=0.60. This measures how many actual positives were found.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 67. **Question:**\n",
        "Using that same confusion matrix (TP=30, FP=10, FN=20, TN=940), the **F1** score is:\n",
        "\n",
        "- A) 2 * (0.75 * 0.60)/(0.75 + 0.60)= 2*(0.45)/1.35= 0.67\n",
        "- B) 2 * (0.60 * 0.93)/(0.60 + 0.93)= 0.75\n",
        "- C) 0.75 + 0.60=1.35\n",
        "- D) 2 * (0.60 * 0.75)/(0.60 + 0.75)= 2*(0.45)/1.35=0.67\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** D) 2 * (0.60 * 0.75)/(0.60 + 0.75)= 2*(0.45)/1.35=0.67\n",
        "\n",
        "**Explanation:**  \n",
        "Precision=0.75, Recall=0.60. F1= 2*(P*R)/(P+R)=2*(0.75*0.60)/(0.75+0.60)=2*(0.45)/1.35=0.67. Make sure the arithmetic is correct: 0.45*2=0.90, 0.90/1.35=0.666..., ~0.67.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 68. **Question:**\n",
        "A model has an AUC of 0.99. However, at your chosen threshold, precision=0.10. Which perspective addresses this disparity?\n",
        "\n",
        "- A) The ranking of positives vs. negatives is strong overall (high AUC), but the threshold picks many positives with numerous false positives, resulting in low precision\n",
        "- B) A high AUC ensures high precision at any threshold\n",
        "- C) The model has no actual discriminative ability\n",
        "- D) Precision must exceed recall\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) The ranking of positives vs. negatives is strong overall (high AUC), but the threshold picks many positives with numerous false positives, resulting in low precision\n",
        "\n",
        "**Explanation:**  \n",
        "AUC reflects the model’s ability to rank samples across thresholds. At a specific threshold, we might accept many positives, including many false positives, thus a low precision. High AUC doesn’t guarantee high precision unless we adjust the threshold appropriately.\n",
        "\n",
        "---\n",
        "\n",
        "### 69. **Question:**\n",
        "You have a dataset with 1% positive class (rare) and 99% negative class. If your classifier simply predicts “negative” for all samples, it obtains 99% accuracy. From a **class imbalance** perspective, which statement is most accurate?\n",
        "\n",
        "- A) This accuracy is misleadingly high because the model ignores almost all positives, indicating poor utility on the minority class\n",
        "- B) The classifier must be optimal since 99% accuracy is always good\n",
        "- C) The data is not imbalanced with 1% positives\n",
        "- D) A balanced dataset always yields 99% accuracy\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) This accuracy is misleadingly high because the model ignores almost all positives, indicating poor utility on the minority class\n",
        "\n",
        "**Explanation:**  \n",
        "A naive classifier can exploit the imbalance by always predicting the majority class to achieve high accuracy, but it fails to detect the crucial minority positives. Accuracy alone is not very informative in highly imbalanced scenarios.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 70. **Question:**\n",
        "When dealing with **highly imbalanced** data (say 5% positives), which metric often provides more insight than simple accuracy?\n",
        "\n",
        "- A) Training error alone\n",
        "- B) The fraction of predicted positives that are correct (precision), or how many actual positives are found (recall), possibly combined into F1 or reviewing PR curves\n",
        "- C) The negative predictive value\n",
        "- D) The confusion matrix diagonal sum\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) The fraction of predicted positives that are correct (precision), or how many actual positives are found (recall), possibly combined into F1 or reviewing PR curves\n",
        "\n",
        "**Explanation:**  \n",
        "With heavy imbalance, accuracy can be skewed by the huge negative class. Metrics focusing specifically on how well the minority positives are handled (precision, recall, F1, or PR curves) are more revealing about real performance on that underrepresented class.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 71. **Question:**\n",
        "You have 10,000 samples: 100 positives, 9,900 negatives. Your classifier guesses “positive” for 200 samples, among which 50 are real positives. Which statement about evaluating performance is most correct?\n",
        "\n",
        "- A) The model’s accuracy is 99.5%, guaranteeing robust detection\n",
        "- B) Precision for the positive predictions is 50/200 = 25%, recall is 50/100 = 50%. This indicates moderate success but still many positives missed\n",
        "- C) The classifier must be random\n",
        "- D) The negative class is trivial, so the model must have zero FN\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) Precision for the positive predictions is 50/200 = 25%, recall is 50/100 = 50%. This indicates moderate success but still many positives missed\n",
        "\n",
        "**Explanation:**  \n",
        "It’s a classic imbalanced scenario. The accuracy seems high but not indicative of minority detection. Among 200 predicted positives, 50 are correct => 25% precision. Also, it catches 50 out of 100 actual positives => 50% recall. So half the actual positives remain undetected, though it’s still better than predicting all negatives.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 72. **Question:**\n",
        "Which approach can *directly* address **class imbalance** in training?\n",
        "\n",
        "- A) Minimizing mean squared error\n",
        "- B) **Oversampling** the minority class (e.g., SMOTE) or **undersampling** the majority, giving more balanced representation to help the model pay more attention to minority\n",
        "- C) Applying a standard random forest with no parameter changes\n",
        "- D) Relying on a single threshold that yields the highest accuracy\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** B) **Oversampling** the minority class (e.g., SMOTE) or **undersampling** the majority, giving more balanced representation to help the model pay more attention to minority\n",
        "\n",
        "**Explanation:**  \n",
        "Methods such as oversampling minority (e.g., synthetic sampling) or undersampling majority are common ways to handle data skew, ensuring the model sees a more balanced distribution and doesn’t trivially learn to predict majority.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 73. **Question:**\n",
        "You tune your model’s **decision threshold** specifically to improve detection of the rare positive class, even if it means more false positives. In the context of class imbalance, which direct effect occurs?\n",
        "\n",
        "- A) Recall typically increases, but precision might drop, as you label more samples as positive\n",
        "- B) Fewer positives are caught\n",
        "- C) Accuracy always improves\n",
        "- D) The negative class disappears\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Correct Answer:** A) Recall typically increases, but precision might drop, as you label more samples as positive\n",
        "\n",
        "**Explanation:**  \n",
        "Lowering the threshold means you’ll predict “positive” more often, capturing more actual positives (higher recall) but at the risk of additional false positives, reducing precision. This threshold tuning is common when you can’t rely solely on accuracy in imbalanced data."
      ],
      "metadata": {
        "id": "JafSruE1Q_NO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtu44TDHMlXK"
      },
      "outputs": [],
      "source": []
    }
  ]
}