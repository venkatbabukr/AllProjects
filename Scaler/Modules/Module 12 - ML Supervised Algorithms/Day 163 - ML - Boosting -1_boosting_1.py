# -*- coding: utf-8 -*-
"""Boosting - 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NjzxK9cMh5gJFUanJXI7fhvkKA19i4md

# Interview Questions

#### 1. **What is the main advantage of Random Forest over a single Decision Tree?**
   - A) It uses more features for each split.
   - B) It reduces overfitting by averaging multiple trees.
   - C) It requires less training data compared to a Decision Tree.
   - D) It uses linear models at the leaf nodes.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) It reduces overfitting by averaging multiple trees.

   **Explanation:** Random Forest combines multiple Decision Trees and averages their predictions, which helps reduce overfitting and increases model robustness.

#### 2. **Which of the following techniques can be used to reduce overfitting in Decision Trees?**
   - A) Use cross-validation and bagging.
   - B) Increase the number of leaf nodes.
   - C) Increase the depth of the tree.
   - D) Use fewer features for splitting.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) Use cross-validation and bagging.

   **Explanation:** Cross-validation helps tune hyperparameters to prevent overfitting, and bagging (Bootstrap Aggregating) reduces overfitting by averaging multiple models.

#### 3. **In KNN, which of the following distance metrics is most commonly used for continuous variables?**
   - A) Cosine Similarity
   - B) Euclidean Distance
   - C) Manhattan Distance
   - D) Hamming Distance

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) Euclidean Distance

   **Explanation:** Euclidean distance is the most commonly used metric in KNN when the features are continuous because it calculates the straight-line distance between two points.

#### 4. **How does Logistic Regression handle non-linear decision boundaries?**
   - A) By transforming the target variable using a logarithmic function.
   - B) By using polynomial features or interaction terms.
   - C) By applying the Gini index as a split criterion.
   - D) By increasing the number of training iterations.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) By using polynomial features or interaction terms.

   **Explanation:** Logistic Regression can capture non-linear relationships by transforming input features, such as using polynomial terms or feature interactions.

#### 5. **What is the purpose of feature scaling in KNN?**
   - A) To reduce the number of dimensions in the dataset.
   - B) To improve the interpretability of the distance metric.
   - C) To ensure that all features contribute equally to the distance calculation.
   - D) To improve the computational efficiency of the algorithm.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** C) To ensure that all features contribute equally to the distance calculation.

   **Explanation:** Feature scaling is crucial in KNN because the algorithm uses a distance metric, and unscaled features with different units can disproportionately affect the distances.

#### 6. **In Random Forest, what is "out-of-bag" (OOB) error?**
   - A) Error on the test set.
   - B) Error calculated from a validation set created by cross-validation.
   - C) Error calculated on samples not used for training a specific tree.
   - D) Error due to underfitting when the trees are too shallow.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** C) Error calculated on samples not used for training a specific tree.

   **Explanation:** OOB error is calculated using the bootstrap samples that were left out when building individual trees in the Random Forest and provides an unbiased estimate of model accuracy.

#### 7. **In Linear Regression, what does the R-squared value represent?**
   - A) The proportion of total variation in the response variable explained by the model.
   - B) The accuracy of the predicted values.
   - C) The total error in the model.
   - D) The ratio of the number of features to the number of observations.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) The proportion of total variation in the response variable explained by the model.

   **Explanation:** R-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variables, indicating the model's goodness-of-fit.

#### 8. **Which of the following is a key assumption in Linear Regression?**
   - A) The residuals are normally distributed.
   - B) The features are highly correlated with each other.
   - C) The target variable follows a binomial distribution.
   - D) The independent variables are binary.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) The residuals are normally distributed.

   **Explanation:** Linear Regression assumes that the residuals (errors) follow a normal distribution, which is necessary for making inferences from the model.

#### 9. **In Logistic Regression, which of the following techniques can be applied to handle multicollinearity?**
   - A) Use L2 regularization (Ridge) to remove some features.
   - B) Use KNN to cluster the features.
   - C) Increase the number of iterations during training.
   - D) Use Euclidean distance to reduce feature dependence.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) Use L2 regularization (Ridge) to remove some features.

   **Explanation:** L2 regularization (Ridge) can handle multicollinearity by shrinking the coefficients of less important features to zero, effectively performing feature selection.

#### 10. **How does increasing the number of trees in a Random Forest affect its performance?**
   - A) It increases bias in the model.
   - B) It leads to underfitting of the data.
   - C) It reduces the variance without increasing bias.
   - D) It decreases the number of features used for splitting at each node.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** C) It reduces the variance without increasing bias.

   **Explanation:** Adding more trees in a Random Forest reduces the variance of the model by averaging out the predictions from individual trees, leading to more stable and accurate predictions without increasing bias.

#### 11. **What does the Gini impurity in a Decision Tree represent?** (HOMEWORK QUESTION)
   - A) The probability that a randomly chosen element will be classified correctly.
   - B) The degree of uncertainty in selecting a feature for a split.
   - C) The probability of misclassifying a randomly chosen element.
   - D) The importance of a feature in the final prediction.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** C) The probability of misclassifying a randomly chosen element.

   **Explanation:** Gini impurity measures the probability that a randomly chosen sample would be incorrectly classified if randomly assigned according to the distribution of class labels at that node.

#### 12. **In KNN, how can we address the issue of class imbalance?**
   - A) By using a smaller value of **k**.
   - B) By normalizing the feature set.
   - C) By using weighted voting, giving more weight to nearer neighbors.
   - D) By reducing the number of features used in the model.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** C) By using weighted voting, giving more weight to nearer neighbors.

   **Explanation:** Weighted voting in KNN gives more influence to closer neighbors, which can help mitigate the issue of class imbalance by ensuring that distant points from the majority class donâ€™t dominate predictions.

#### 13. **In Random Forest, why is feature selection considered less critical than in other models?**
   - A) It uses all the features to make splits.
   - B) It builds a large number of trees with random feature selection for each tree.
   - C) It uses feature scaling to balance the importance of features.
   - D) It does not rely on any single feature to make predictions.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) It builds a large number of trees with random feature selection for each tree.

   **Explanation:** In Random Forest, each tree is trained on a random subset of features, so the model is less sensitive to individual features, and feature selection becomes less critical.

#### 14. **What is the primary drawback of using high-degree polynomials in Polynomial Regression (a form of Linear Regression)?**
   - A) It increases the risk of underfitting.
   - B) It leads to a lower variance in the model's predictions.
   - C) It can cause overfitting, especially when there is noise in the data.
   - D) It simplifies the model too much, leading to higher bias.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** C) It can cause overfitting, especially when there is noise in the data.

   **Explanation:** High-degree polynomials can fit the training data too closely, capturing noise as part of the model, which leads to overfitting and poor generalization to unseen data.

#### 15. **Which of the following techniques can help improve the accuracy of a Logistic Regression model?**
   - A) Adding more training data.
   - B) Using linear features without transformation.
   - C) Increasing the depth of the decision tree.
   - D) Using a high value of **k** in KNN.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) Adding more training data.

   **Explanation:** Logistic Regression can benefit from more training data, especially in high-dimensional spaces, to learn better decision boundaries and avoid overfitting.

# GBDT implementation
"""

import pandas as pd


!gdown 19L3rYatfhbBL1r5MHrv-p_oM2wlvrhqk
!gdown 1OHLKJwA3qZopKPvlKoRldM6BvA1A4dYF
!gdown 1N7O_fWCTJLu8SIa_paKcDEzllgpMk8sK
!gdown 12Bh2AN8LcZAlg20ehpQrEWccUDaSdsOG

X_train = pd.read_pickle('preprocessed_X_sm.pickle')
X_test = pd.read_pickle('X_test.pickle')
y_train = pd.read_pickle('y_sm.pickle')
y_test = pd.read_pickle('y_test.pickle') #$use this if prevous code nor wor

from sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier(n_estimators=150, max_depth=1, loss = 'log_loss')

gbc.fit(X_train, y_train)

gbc.score(X_train, y_train)

gbc.score(X_test, y_test)

import pickle
import os

# Create an empty file
empty_file_path = 'empty_file.txt'
with open(empty_file_path, 'w') as f:
    pass  # Creates an empty file

print(f"Empty file created at: {empty_file_path}")

# Create a text file with dummy data
text_file_path = 'dummy_data.txt'
dummy_data = "This is some dummy text data for the file."
with open(text_file_path, 'w') as f:
    f.write(dummy_data)

print(f"Text file created at: {text_file_path}")

# Data to be pickled
data_to_pickle = {'name': 'John Doe', 'age': 30, 'city': 'New York'}

# Pickle the data into a binary file
pickle_file_path = 'data.pkl'
with open(pickle_file_path, 'wb') as f:
    pickle.dump(data_to_pickle, f)

print(f"Data pickled into binary file at: {pickle_file_path}")

# To verify, you can load the pickled data back
with open(pickle_file_path, 'rb') as f:
    loaded_data = pickle.load(f)

print(f"Loaded data from pickle file: {loaded_data}")

# Clean up the created files (optional)
# os.remove(empty_file_path)
# os.remove(text_file_path)
# os.remove(pickle_file_path)
# print("Created files removed.")

import joblib
import os
import numpy as np

# Data to be dumped using joblib
data_to_dump = {'array_data': np.random.rand(10, 10), 'string_data': 'This is some string data'}

# Define the file path
joblib_file_path = 'joblib_data.joblib'

# Dump the data to a file using joblib
joblib.dump(data_to_dump, joblib_file_path)

print(f"Data dumped to {joblib_file_path}")

# Load the data back from the file using joblib
loaded_data_joblib = joblib.load(joblib_file_path)

print(f"Loaded data from joblib file: {loaded_data_joblib}")

# You can access the data like a dictionary
print(f"Loaded array data shape: {loaded_data_joblib['array_data'].shape}")
print(f"Loaded string data: {loaded_data_joblib['string_data']}")

# Clean up the created file (optional)
# os.remove(joblib_file_path)
# print(f"Removed {joblib_file_path}")