# -*- coding: utf-8 -*-
"""Boosting-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PVf1zQYy3B7eUdo7_hGAKIRv5h0Myom0

# Interview Questions

#### 1. **How does boosting differ from bagging in ensemble methods like Decision Trees?**
   - A) Boosting combines weak learners sequentially, whereas bagging builds independent models.
   - B) Boosting increases bias, while bagging decreases variance.
   - C) Boosting reduces overfitting by subsampling the dataset, while bagging reduces overfitting by pruning trees.
   - D) Boosting requires deep trees, while bagging works with shallow trees.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A)

   **Explanation:** Boosting builds models sequentially, focusing on errors made by previous models, while bagging trains models independently in parallel.

#### 2. **In KNN, what is the effect of using a weighted distance metric instead of the standard Euclidean distance?**
   - A) It reduces the model’s complexity.
   - B) It gives more importance to features with smaller ranges.
   - C) It accounts for feature correlations during classification.
   - D) It gives more importance to nearer neighbors.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** D) It gives more importance to nearer neighbors.

   **Explanation:** Weighted distance metrics in KNN assign more influence to closer neighbors, improving prediction accuracy, especially in imbalanced or noisy datasets.


#### 4. **In Decision Trees, how is the split criterion (such as Gini impurity or information gain) selected for a node?**
   - A) The split that maximizes the number of data points in each leaf node.
   - B) The split that minimizes the number of leaf nodes.
   - C) The split that minimizes the impurity or maximizes information gain at that node.
   - D) The split that reduces the depth of the tree.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** C) The split that minimizes the impurity or maximizes information gain at that node.

   **Explanation:** Decision Trees select splits that minimize impurity (e.g., Gini, entropy) or maximize information gain, aiming to create pure nodes with homogeneous class labels.

#### 5. **What is the primary reason for using regularization techniques in Linear Regression?**
   - A) To improve model interpretability by adding more features.
   - B) To avoid overfitting by penalizing large coefficients.
   - C) To increase the complexity of the model to better fit the data.
   - D) To improve the computation time of model training.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) To avoid overfitting by penalizing large coefficients.

   **Explanation:** Regularization techniques (like Lasso or Ridge) penalize large coefficients, reducing overfitting by preventing the model from fitting the noise in the training data.

#### 6. **In boosting algorithms like Gradient Boosting, how is the contribution of each weak learner adjusted during training?**
   - A) By assigning random weights to each weak learner.
   - B) By optimizing the residuals of the previous learner.
   - C) By decreasing the learning rate after each iteration.
   - D) By discarding previous weak learners at each step.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) By optimizing the residuals of the previous learner.

   **Explanation:** Gradient Boosting adjusts the contribution of each weak learner by fitting the residual errors of the previous learners to minimize the overall loss.

#### 7. **In KNN, how can you reduce the impact of irrelevant features?**
   - A) By using a larger value of **k**.
   - B) By increasing the number of neighbors considered for classification.
   - C) By applying dimensionality reduction techniques like PCA.
   - D) By applying boosting to select relevant features.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** C) By applying dimensionality reduction techniques like PCA.

   **Explanation:** Dimensionality reduction techniques like Principal Component Analysis (PCA) can help reduce the impact of irrelevant features by projecting data into a lower-dimensional space.

#### 8. **Why is Logistic Regression not suitable for a multi-class classification problem without modification?**
   - A) It cannot handle continuous target variables.
   - B) It only models binary outcomes directly.
   - C) It assumes linear separability of the data.
   - D) It uses a non-parametric model structure.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) It only models binary outcomes directly.

   **Explanation:** Logistic Regression inherently models binary outcomes, and extensions like One-vs-All (OvA) or Softmax are required to handle multi-class classification problems.

#### 9. **Which of the following is a limitation of Linear Regression?**
   - A) It cannot be used with continuous data.
   - B) It assumes a non-linear relationship between features and target.
   - C) It assumes that the features are highly correlated.
   - D) It assumes a linear relationship between features and the target.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** D) It assumes a linear relationship between features and the target.

   **Explanation:** A key limitation of Linear Regression is the assumption that the relationship between the features and the target variable is linear, which may not hold in many real-world scenarios.

#### 10. **How does the learning rate in Gradient Boosting affect the model's performance?**
   - A) A high learning rate always improves performance by fitting data more quickly.
   - B) A low learning rate makes the model train faster but generalize poorly.
   - C) A low learning rate improves generalization but requires more iterations to converge.
   - D) The learning rate controls the number of weak learners used in the model.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** C) A low learning rate improves generalization but requires more iterations to converge.

   **Explanation:** A lower learning rate leads to better generalization as the model makes smaller updates, but it requires more iterations (weak learners) to converge.

#### 11. **In KNN, what is the primary drawback of using a very large value of **k**?**
   - A) It increases the risk of overfitting the training data.
   - B) It increases the complexity of distance calculations.
   - C) It smooths predictions too much, leading to underfitting.
   - D) It increases sensitivity to noise in the data.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** C) It smooths predictions too much, leading to underfitting.

   **Explanation:** A very large **k** considers many neighbors for classification, which can result in over-smoothing of the decision boundary and lead to underfitting.

#### 12. **How can you assess the feature importance in a Random Forest model?**
   - A) By evaluating the Gini impurity reduction for each feature.
   - B) By measuring the distance metric for each feature.
   - C) By calculating the linear coefficient for each feature.
   - D) By using One-vs-Rest classification on each feature.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) By evaluating the Gini impurity reduction for each feature.

   **Explanation:** Random Forests calculate feature importance by measuring the reduction in Gini impurity (or information gain) attributed to each feature across all trees in the ensemble.

#### 13. **In Logistic Regression, what is the relationship between log-odds and the predicted probability?**
   - A) Log-odds is the linear predictor, and probability is obtained by applying the sigmoid function to it.
   - B) Log-odds is the predicted probability squared.
   - C) Log-odds is the difference between the predicted probability and 1.
   - D) Log-odds is the reciprocal of the predicted probability.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) Log-odds is the linear predictor, and probability is obtained by applying the sigmoid function to it.

   **Explanation:** Logistic Regression models the log-odds (linear predictor), and the predicted probability is derived by applying the sigmoid function to the log-odds.

#### 14. **What is a key advantage of boosting algorithms over traditional Decision Trees?**
   - A) Boosting reduces the bias by training trees sequentially on misclassified examples.
   - B) Boosting increases the interpretability of the model.
   - C) Boosting reduces computation time by parallelizing the training process.
   - D) Boosting increases the depth of each tree to handle more complex data.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) Boosting reduces the bias by training trees sequentially on misclassified examples.

   **Explanation:** Boosting algorithms  focus on reducing bias by iteratively training on the errors of the previous trees, allowing the ensemble to improve over time.

#### 15. **How does Ridge Regression (L2 regularization) handle multicollinearity in Linear Regression models?**
   - A) It removes features that are highly correlated.
   - B) It shrinks the coefficients of correlated features, distributing the weight more evenly.
   - C) It increases the weights of correlated features.
   - D) It normalizes the data to remove correlations.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) It shrinks the coefficients of correlated features

, distributing the weight more evenly.

   **Explanation:** Ridge Regression handles multicollinearity by penalizing large coefficients, effectively shrinking the values of highly correlated features and distributing the model’s weight more evenly across features.


#### 16. **In Decision Trees, why might using a smaller minimum samples split result in overfitting?**
   - A) It allows for deeper trees that capture more noise from the data.
   - B) It reduces the number of leaves in the tree.
   - C) It forces the model to make splits based on fewer data points, reducing complexity.
   - D) It leads to underfitting by making the tree too shallow.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) It allows for deeper trees that capture more noise from the data.

   **Explanation:** A smaller minimum samples split allows the tree to grow deeper, which increases the likelihood of fitting noise and overfitting the model to the training data.

#### 17. **In KNN, what is the primary limitation when the number of features (dimensionality) increases?**
   - A) The accuracy of the model always decreases with more features.
   - B) The curse of dimensionality makes distance measures less meaningful.
   - C) The training time increases exponentially with the number of features.
   - D) The model becomes highly prone to underfitting with more features.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) The curse of dimensionality makes distance measures less meaningful.

   **Explanation:** As dimensionality increases, the data becomes sparse, and distances between points lose meaning, leading to poor performance in KNN, which relies on distance metrics.

#### 18. **What is the role of shrinkage (learning rate) in boosting algorithms like Gradient Boosting?**
   - A) It controls the number of features used in each iteration.
   - B) It scales the contribution of each weak learner to prevent overfitting.
   - C) It adjusts the decision threshold for classification.
   - D) It increases the tree depth to improve accuracy.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) It scales the contribution of each weak learner to prevent overfitting.

   **Explanation:** Shrinkage (learning rate) reduces the influence of each weak learner, making the model more robust and preventing overfitting by making smaller, more gradual updates.

#### 19. **In Linear Regression, what does multicollinearity between features cause?**
   - A) It leads to more interpretable coefficients.
   - B) It inflates the standard errors of the coefficient estimates.
   - C) It improves the predictive power of the model.
   - D) It increases the number of significant features in the model.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) It inflates the standard errors of the coefficient estimates.

   **Explanation:** Multicollinearity inflates the variance of the coefficient estimates, leading to unreliable estimates and making it difficult to assess the contribution of individual features.

#### 20. **How does regularization in Logistic Regression improve the model’s generalization performance?**
   - A) By adding a penalty term to the cost function that limits the size of the coefficients.
   - B) By increasing the complexity of the model to fit the data more closely.
   - C) By adding more decision boundaries for classification.
   - D) By using the logistic function to shrink the input data.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) By adding a penalty term to the cost function that limits the size of the coefficients.

   **Explanation:** Regularization adds a penalty to the cost function, which prevents the coefficients from becoming too large, reducing overfitting and improving the model's ability to generalize to new data.

#### 21. **In Decision Trees, what is the impact of using entropy instead of Gini impurity as a split criterion?**
   - A) Entropy results in more balanced trees.
   - B) Entropy is computationally less expensive than Gini impurity.
   - C) Entropy leads to deeper trees compared to Gini impurity.
   - D) Both perform similarly, but entropy may be more expensive.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** D) Both perform similarly, but entropy may be more expensive.

   **Explanation:** Entropy and Gini impurity often lead to similar trees, but entropy can be more sensitive to class imbalance, as it measures the information content more rigorously than Gini impurity.

#### 22. **Why does boosting tend to outperform bagging in terms of accuracy for some datasets?**
   - A) Boosting decreases variance while bagging decreases bias.
   - B) Boosting focuses on difficult-to-predict samples by iteratively adjusting weights.
   - C) Boosting combines deep trees, while bagging combines shallow trees.
   - D) Boosting uses a higher learning rate, which improves accuracy.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) Boosting focuses on difficult-to-predict samples by iteratively adjusting weights.

   **Explanation:** Boosting iteratively adjusts the weights of misclassified samples, focusing on correcting errors from previous learners, which often leads to improved accuracy on challenging datasets.

#### 23. **In KNN, what is the effect of increasing the value of **k** in a highly imbalanced dataset?**
   - A) It makes the model more sensitive to the majority class.
   - B) It reduces overfitting by increasing the decision boundary’s complexity.
   - C) It improves the model’s ability to detect minority class examples.
   - D) It increases the variance in the model’s predictions.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) It makes the model more sensitive to the majority class.

   **Explanation:** A larger value of **k** in KNN tends to smooth predictions by considering more neighbors, which often results in bias towards the majority class in an imbalanced dataset.

#### 25. **Which of the following is a key disadvantage of using Linear Regression when the relationship between features and the target is non-linear?**
   - A) Linear Regression tends to overfit the training data.
   - B) Linear Regression can only model binary target variables.
   - C) Linear Regression performs poorly when the true relationship is non-linear.
   - D) Linear Regression requires a large number of features to work properly.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** C) Linear Regression performs poorly when the true relationship is non-linear.

   **Explanation:** Linear Regression assumes a linear relationship between features and the target, and when the true relationship is non-linear, the model will have high bias and perform poorly.

# How Boosting works
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import r2_score, mean_squared_error

# Generate synthetic regression data
np.random.seed(42)
X = np.linspace(0, 10, 200)
y = np.sin(X) + np.random.normal(0, 0.5, size=X.shape)

# Reshape X for sklearn compatibility
X = X.reshape(-1, 1)

# Sort X and y for plotting purposes
sort_idx = np.argsort(X.ravel())
X_sorted = X[sort_idx]
y_sorted = y[sort_idx]

# Initial prediction using the mean of y (mean model)
y_pred_mean = np.full_like(y, np.mean(y))


plt.figure(figsize=(10, 6))
plt.scatter(X, y, label='Data', alpha=0.6)
plt.plot(X_sorted, y_pred_mean, color='red', label='Mean Model')
plt.title('Iteration 0 (Mean Model)')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()

# Initialize the Gradient Boosting Regressor with 5 estimators
model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)
model.fit(X, y)


r2 = r2_score(y, y_pred_mean)
mse = mean_squared_error(y, y_pred_mean)
print(f"Iteration 0 (Mean Model): R² = {r2:.4f}, MSE = {mse:.4f}")

# Plot the mean model
plt.figure(figsize=(10, 6))
plt.scatter(X_sorted, y_sorted, label='Data', alpha=0.6)
plt.plot(X_sorted, y_pred_mean[sort_idx], color='red', label='Mean Model')
plt.title('Iteration 0 (Mean Model)')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()

ano = model.staged_predict(X)

# Sequentially add base learners and plot the regression line
for idx, y_pred in enumerate(model.staged_predict(X)):
    iteration = idx + 1
    r2 = r2_score(y, y_pred)
    mse = mean_squared_error(y, y_pred)
    print(f"Iteration {iteration}: R² = {r2:.4f}, MSE = {mse:.4f}")

    # Plotting
    plt.figure(figsize=(10, 6))
    plt.scatter(X_sorted, y_sorted, label='Data', alpha=0.6)
    plt.plot(X_sorted, y_pred, color='red', label=f'Iteration {iteration} Model')
    plt.title(f'Iteration {iteration}')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.legend()
    plt.show()

"""# Sample case-study"""

import pickle

!gdown 171Yoe_GSapyrmOnD9oBzHWNOD_OnQs0F
!gdown 1hnIlTPW3AMeB69EbeaXCRIrpMVT1Vwmc
!gdown 1nZtB_RtxMg_MgoRczb8UWQX-AEK_l3qE
!gdown 1zLDUErwKdmF-RacOyHEuI_z_46LssQtP

import pandas as pd

X_train = pd.read_pickle('X_train.pickle')


X_test = pd.read_pickle('X_test.pickle')

y_train = pd.read_pickle('Y_train.pickle')

y_test = pd.read_pickle('Y_test.pickle')

from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn import tree
from sklearn.model_selection import GridSearchCV

params = {
    "max_depth" : [3, 5, 7, 10, 15],
    "max_leaf_nodes" : [20, 40, 60]
}

model1 = DTC()
clf = GridSearchCV(model1, params, scoring = "accuracy", cv=5)

clf.fit(X_train, y_train)

res = clf.cv_results_

for i in range(len(res["params"])):
  print(f"Parameters:{res['params'][i]} Mean_score: {res['mean_test_score'][i]} Rank: {res['rank_test_score'][i]}")

print(clf.best_estimator_)

clf = DTC(**clf.best_params_)
clf.fit(X_train, y_train)

clf.score(X_train, y_train)

clf.score(X_test, y_test)

"""## RF"""

from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn import tree
from sklearn.model_selection import RandomizedSearchCV
import datetime as dt

params = {
    "n_estimators": [10,25,50,100,150,200],
    "max_depth" : [3, 5, 10, 15, 20],
    "max_leaf_nodes" : [20, 40, 80]
}

rfc = RFC(n_jobs = -1)
clf = RandomizedSearchCV(rfc, params, scoring = "accuracy", cv=3, n_jobs = -1, verbose = 1)

start = dt.datetime.now()
clf.fit(X_train, y_train)
end = dt.datetime.now()

res = clf.cv_results_

for i in range(len(res["params"])):
  print(f"Parameters:{res['params'][i]} Mean_score: {res['mean_test_score'][i]} Rank: {res['rank_test_score'][i]}")

print(f"Time taken for fits : {end - start}")

print(clf.best_estimator_)
rf = clf.best_estimator_
rf.fit(X_train, y_train)
print("Model acc",rf.score(X_test, y_test))

"""## GBDT"""

params = {
    "n_estimators": [50,100,150,200],
    "max_depth" : [3, 4, 5, 7],
    "max_leaf_nodes" : [20, 40, 80],
    "learning_rate": [0.1, 0.2, 0.3]
}

from sklearn.ensemble import GradientBoostingClassifier as GBC
from sklearn.model_selection import RandomizedSearchCV
import datetime as dt


gbc = GBC()
clf = RandomizedSearchCV(gbc, params, scoring = "accuracy", cv=3, n_jobs = -1, verbose = 1)

start = dt.datetime.now()

clf.fit(X_train, y_train)

end = dt.datetime.now()

print(f"Time taken for fits : {end - start}")
print(clf.best_estimator_)
gbc = clf.best_estimator_
gbc.fit(X_train, y_train)
print("Model acc",gbc.score(X_test, y_test))

# Test accuracy: 0.96 (better than RF)