# -*- coding: utf-8 -*-
"""12-Notes-SVM2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L0O73-XVo5N2r9ewqVdpM1ZUSvCMVEvT

### Content

- **Recap**


- **Dual formulation of SVM**



- **Runtime of SVM**


- **Kernel SVM**
    - Polynomial kernel
    - RBF kernel
    
- **Code Walkthrough**



- **Impact of Outliers**

---

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/192/original/z.png?1705479141' width='800'>

---

### Optimization Function

\
The optimization function along with constraints is:

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/193/original/z.png?1705479172' width='800'>

---

### Primal Dual Equivalence

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/194/original/z.png?1705479200' width='800'>

---

### Dual formulation of SVM

\
The dual formulation of SVM is given as:

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/195/original/z.png?1705479231' width='800'>

Say, we have
- solved the dual form optimization equation
- and found the optimal $α's$

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/196/original/z.png?1705479257' width='800'>

---

**But why are we learning all this?** </br>
**What's the significance of $α_i$ values?**

Let's see...

### Runtime of SVM

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/197/original/z.png?1705479345' width='800'>

Now, recall that

- $α_i$ = 0 , for Non support vectors
- $α_i$ > 0 , for Support Vectors

\
<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/198/original/z.png?1705479375' width='800'>

---

Let's take an example :

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/199/original/z.png?1705479403' width='800'>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/200/original/z.png?1705479430' width='800'>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/202/original/z.png?1705479460' width='800'>

---

### Kernel SVM

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/203/original/z.png?1705479488' width='800'>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/204/original/z.png?1705479655' width='800'>

Say, we have a data distribution which looks like :

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/205/original/z.png?1705479681' width='800'>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/206/original/z.png?1705479708' width='800'>

Let's see how it does that.

---

### Polynomial Kernel

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/207/original/z.png?1705479745' width='800'>

### Quadratic Kernel

Let's write the equation for this.

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/209/original/z.png?1705479773' width='800'>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/210/original/z.png?1705479811' width='800'>

This means
- quadratic kernel outcome is result of 6-dim dot product
- but we were able to get that using 2-dim vector only ($x_1, x_2$)

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/211/original/z.png?1705479838' width='800'>

In simple terms,
- while being the 2 dim space
- we are getting the result of 6 - dim space
- without explicitly adding features.

\
This is true power of **Kernelization**

---

### RBF - Radial Basis Kernel function

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/212/original/z.png?1705479864' width='800'>

Let's plot its curve and see how it looks.

Desmos plot: https://www.desmos.com/calculator/maidcnyvlz
"""

from IPython.display import IFrame

IFrame(src="https://www.desmos.com/calculator/maidcnyvlz", width=700, height=375)

"""<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/213/original/z.png?1705479903' width='800'>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/214/original/z.png?1705479927' width='800'>

Let's see how SVM with RBF kernel will help us here.

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/215/original/z.png?1705479954' width='800'>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/216/original/z.png?1705479978' width='800'>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/217/original/z.png?1705480003' width='800'>

---

### Time Complexity

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/218/original/z.png?1705480031' width='800'>

---

Let's implement Kernel SVM on a dataset.

### Code Walkthrough
"""

from matplotlib import pyplot as plt
import numpy as np
from sklearn.inspection import DecisionBoundaryDisplay

# create a non-linear dataset

from sklearn.datasets import make_circles
X, y = make_circles(100, factor=.1, noise=.1, random_state=0)
plt.scatter(X[:, 0], X[:, 1], c=y)
plt.show()

"""### Linear kernel SVM

SVM sklearn doc: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn-svm-svc
"""

from sklearn.svm import SVC

# try using svm with linear kernel
clf = SVC(kernel='linear').fit(X, y)

# plot the decision function

plt.scatter(X[:, 0], X[:, 1], c=y, s=50)
plt.title("SVM with linear kernel on non-linear dataset")

ax = plt.gca()
DecisionBoundaryDisplay.from_estimator(
    clf,
    X,
    plot_method="contour",
    colors="k",
    levels=[-1, 0, 1],
    alpha=0.5,
    linestyles=["--", "-", "--"],
    ax=ax,
)
# plot support vectors
ax.scatter(
    clf.support_vectors_[:, 0],
    clf.support_vectors_[:, 1],
    s=100,
    linewidth=1,
    facecolors="none",
    edgecolors="k",
)
plt.show()

"""### RBF kernel SVM"""

clf = SVC(kernel='rbf', C=1E6)
clf.fit(X, y)

#plot RBF kernel

plt.scatter(X[:, 0], X[:, 1], c=y, s=50)
plt.title("SVM with RBF kernel on non-linear dataset")

ax = plt.gca()
DecisionBoundaryDisplay.from_estimator(
    clf,
    X,
    plot_method="contour",
    colors="k",
    levels=[-1, 0, 1],
    alpha=0.5,
    linestyles=["--", "-", "--"],
    ax=ax,
)
# plot support vectors
ax.scatter(
    clf.support_vectors_[:, 0],
    clf.support_vectors_[:, 1],
    s=100,
    linewidth=1,
    facecolors="none",
    edgecolors="k",
)
plt.show()

"""---

### Impact of Outliers

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/219/original/z.png?1705480067' width='800'>

---

### Drawbacks of SVM

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/220/original/z.png?1705480093' width='800'>

---

### Extra Read - SVR (Support Vector Regression)

Colab link: https://colab.research.google.com/drive/1RF1_isWlDEQFeLKrfmprWP6IMwGEVIfr?usp=sharing
"""