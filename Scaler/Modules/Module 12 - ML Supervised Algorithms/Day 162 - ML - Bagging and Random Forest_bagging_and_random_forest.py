# -*- coding: utf-8 -*-
"""Bagging and Random Forest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i1aKiBBwbFhdOxglgHde5Y2THuGtUzee

# Interview Questions

#### 1. What happens if the value of **k** in KNN is set to 1?
   - A) The algorithm always predicts the most frequent class.
   - B) The algorithm behaves like a random classifier.
   - C) The algorithm classifies the new point as the class of its nearest neighbor.
   - D) The algorithm smooths out predictions based on nearest neighbors.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** C) The algorithm classifies the new point as the class of its nearest neighbor.

   **Explanation:** When **k** is 1, KNN simply assigns the label of the closest training data point, which can make it highly sensitive to noise and outliers.

#### 2. **How does KNN handle missing data during classification?**
   - A) It uses imputation techniques to fill missing values.
   - B) It discards data points with missing values.
   - C) It cannot handle missing data, so pre-processing is required.
   - D) It assigns missing values based on the majority class in neighbors.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** C) It cannot handle missing data, so pre-processing is required.

   **Explanation:** KNN doesn't handle missing data directly. Pre-processing steps, such as imputation, are required to deal with missing values.

#### 3. **In Decision Trees, what criterion is used to decide the best split at a node?**
   - A) Entropy or Gini Impurity
   - B) Information Gain Ratio
   - C) Classification Accuracy
   - D) Mean Squared Error (MSE)

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) Entropy or Gini Impurity

   **Explanation:** Decision Trees use criteria like Entropy and Gini Impurity to find the optimal splits at each node to partition the data effectively.

#### 4. **Which of the following can cause overfitting in a Decision Tree model?**
   - A) Using the Gini index as the split criterion.
   - B) Using a small **k** value in KNN.
   - C) Having too many features without regularization.
   - D) Growing the tree too deep without pruning.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** D) Growing the tree too deep without pruning.

   **Explanation:** Deep Decision Trees can capture noise in the training data, leading to overfitting. Pruning helps mitigate this issue by limiting the tree depth.

#### 5. **Which regularization technique can be used in Logistic Regression to prevent overfitting?**
   - A) Lasso Regularization (L1)
   - B) Ridge Regularization (L2)
   - C) Elastic Net Regularization
   - D) All of the above

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** D) All of the above

   **Explanation:** Logistic Regression can use L1 (Lasso), L2 (Ridge), or Elastic Net regularization to control model complexity and prevent overfitting by penalizing large coefficient values.

#### 6. **In Logistic Regression, what does the term 'log-odds' refer to?**
   - A) The probability of an event occurring.
   - B) The logarithm of the odds of an event occurring.
   - C) The exponential of the odds ratio.
   - D) The likelihood of classification error.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) The logarithm of the odds of an event occurring.

   **Explanation:** The 'log-odds' is the logarithmic transformation of the odds ratio, which Logistic Regression uses to model the relationship between input features and the probability of an event.

#### 7. **Which of the following is true for Linear Regression but not Logistic Regression?**
   - A) The target variable is continuous.
   - B) The features are assumed to be independent.
   - C) The model is prone to overfitting if there are too many features.
   - D) Regularization techniques can be applied to avoid overfitting.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) The target variable is continuous.

   **Explanation:** Linear Regression models a continuous target variable, while Logistic Regression models a binary or categorical target variable.

#### 8. **What assumption does Linear Regression make about the residuals (errors)?**
   - A) Residuals are normally distributed with constant variance.
   - B) Residuals are correlated across observations.
   - C) Residuals follow a Poisson distribution.
   - D) Residuals have an increasing variance with increasing values of the predictor.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) Residuals are normally distributed with constant variance.

   **Explanation:** Linear Regression assumes homoscedasticity (constant variance) and that residuals are normally distributed, which are key assumptions for inference and prediction.

#### 9. **Which of the following metrics is most appropriate for evaluating the performance of Logistic Regression?**
   - A) Mean Squared Error
   - B) Root Mean Squared Error
   - C) Accuracy, Precision, and Recall
   - D) R-squared

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** C) Accuracy, Precision, and Recall

   **Explanation:** Logistic Regression is commonly used for classification tasks, so evaluation metrics like Accuracy, Precision, Recall, and F1-score are most relevant.

#### 10. **What is the role of the sigmoid function in Logistic Regression?**
   - A) It models the linear relationship between features and output.
   - B) It transforms the linear output to a probability between 0 and 1.
   - C) It computes the loss during gradient descent.
   - D) It is used to regularize the coefficients in the model.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) It transforms the linear output to a probability between 0 and 1.

   **Explanation:** The sigmoid function maps the linear combination of input features into a probability range (0, 1), which is the core of Logistic Regression.

#### 11. **In Decision Trees, why does pruning improve the model's performance?**
   - A) It reduces the model’s complexity by limiting the depth of the tree.
   - B) It increases the number of features considered for splits.
   - C) It changes the split criterion to reduce overfitting.
   - D) It allows the tree to fit the noise in the training data.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) It reduces the model’s complexity by limiting the depth of the tree.

   **Explanation:** Pruning helps prevent overfitting by reducing the depth of the tree, thus simplifying the model and making it less likely to capture noise in the data.

#### 12. **What is the impact of using a very high value of **k** in the K-Nearest Neighbors algorithm?**
   - A) It increases the algorithm's sensitivity to noise.
   - B) It makes the decision boundary more complex.
   - C) It smooths out predictions but can lead to underfitting.
   - D) It increases the number of iterations required to converge.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** C) It smooths out predictions but can lead to underfitting.

   **Explanation:** A high value of **k** causes the algorithm to consider many neighbors, which can smooth out predictions but may also cause the model to miss subtle patterns in the data, leading to underfitting.

#### 13. **In Logistic Regression, what does the coefficient of a feature represent?**
   - A) The change in log-odds for a one-unit change in the feature.
   - B) The probability of a positive outcome given the feature's value.
   - C) The residual error associated with that feature.
   - D) The slope of the linear decision boundary for classification.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) The change in log-odds for a one-unit change in the feature.

   **Explanation:** In Logistic Regression, the coefficients represent the change in log-odds of the outcome for each one-unit change in the feature, keeping other features constant.

#### 14. **How does Linear Regression handle multicollinearity between predictor variables?**
   - A) By using cross-validation to select the best subset of predictors.
   - B) By applying L2 regularization to shrink correlated coefficients.
   - C) By increasing the learning rate during training.
   - D) By adding interaction terms between the correlated variables.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) By applying L2 regularization to shrink correlated coefficients.

   **Explanation:** L2 regularization (Ridge Regression) is often used to handle multicollinearity by shrinking the coefficients of correlated variables, reducing the model’s sensitivity to multicollinearity.

#### 15. **In the context of Logistic Regression, which loss function is commonly used during training?**
   - A) Mean Squared Error
   - B) Hinge Loss
   - C) Cross-Entropy Loss
   - D) Least Absolute Deviations

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** C) Cross-Entropy Loss

   **Explanation:** Logistic Regression typically uses Cross-Entropy Loss (also called log loss) to measure the difference between the predicted probability and the actual class label during training.

# Hyper-parameter tuning for Sklearn Models (or any model in general with fit/predict api)

We'll use the **Iris dataset** from `sklearn` for this demonstration. The Iris dataset is a popular dataset used for classification tasks and is included in the `sklearn.datasets` module.

We'll perform the following steps:
1. Load the Iris dataset.
2. Train a **Decision Tree** and **Logistic Regression** model.
3. Implement **Grid Search** and **Random Search** for hyperparameter tuning using `GridSearchCV` and `RandomizedSearchCV` from `sklearn.model_selection`.

### Step-by-Step Implementation

#### **1. Import the Required Libraries**
"""

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from scipy.stats import randint

"""
#### **2. Load the Dataset**
"""

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Target (class labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""#### **3. Grid Search for Decision Tree**
Grid search exhaustively searches over a grid of hyperparameters to find the best combination.
"""

# Define the hyperparameters grid for Decision Tree
param_grid_dt = {
    'criterion': ['gini', 'entropy'],  # Criteria for splitting
    'max_depth': [None, 5, 10, 15],    # Maximum depth of the tree
    'min_samples_split': [2, 10, 20],  # Minimum number of samples required to split a node
    'min_samples_leaf': [1, 5, 10],    # Minimum number of samples required to be at a leaf node
}

# Initialize the Decision Tree classifier
dt_classifier = DecisionTreeClassifier(random_state=42)

# Set up GridSearchCV
grid_search_dt = GridSearchCV(estimator=dt_classifier, param_grid=param_grid_dt, cv=5, n_jobs=-1, verbose=1)

# Fit GridSearchCV to the data
grid_search_dt.fit(X_train, y_train)

# Best parameters from Grid Search
print(f"Best parameters for Decision Tree: {grid_search_dt.best_params_}")

# Predict using the best estimator from GridSearchCV
y_pred_dt = grid_search_dt.best_estimator_.predict(X_test)

# Accuracy score for Decision Tree
accuracy_dt = accuracy_score(y_test, y_pred_dt)
print(f"Decision Tree Accuracy with Grid Search: {accuracy_dt:.4f}")

"""
#### **4. Random Search for Decision Tree**
Random search samples hyperparameters from the distribution you specify for a fixed number of iterations.
"""

# Define the hyperparameter distributions for Random Search
param_dist_dt = {
    'criterion': ['gini', 'entropy'],
    'max_depth': randint(1, 20),         # Randomly search for max_depth between 1 and 20
    'min_samples_split': randint(2, 20), # Randomly search for min_samples_split between 2 and 20
    'min_samples_leaf': randint(1, 10),  # Randomly search for min_samples_leaf between 1 and 10
}

# Set up RandomizedSearchCV
random_search_dt = RandomizedSearchCV(estimator=dt_classifier, param_distributions=param_dist_dt,
                                      n_iter=20, cv=5, n_jobs=-1, random_state=42, verbose=1)

# Fit RandomizedSearchCV to the data
random_search_dt.fit(X_train, y_train)

# Best parameters from Random Search
print(f"Best parameters for Decision Tree (Random Search): {random_search_dt.best_params_}")

# Predict using the best estimator from RandomizedSearchCV
y_pred_dt_random = random_search_dt.best_estimator_.predict(X_test)

# Accuracy score for Decision Tree (Random Search)
accuracy_dt_random = accuracy_score(y_test, y_pred_dt_random)
print(f"Decision Tree Accuracy with Random Search: {accuracy_dt_random:.4f}")

"""#### **5. Grid Search for Logistic Regression**"""

# Define the hyperparameters grid for Logistic Regression
param_grid_lr = {
    'penalty': ['l1', 'l2'], # Regularization type
    'C': [0.01, 0.1, 1, 10, 100],                # Inverse of regularization strength            # Solver for optimization
    'max_iter': [100, 200, 500]                   # Maximum number of iterations
}

# Initialize the Logistic Regression classifier
lr_classifier = LogisticRegression(random_state=42, solver='liblinear')

# Set up GridSearchCV
grid_search_lr = GridSearchCV(estimator=lr_classifier, param_grid=param_grid_lr, cv=5, n_jobs=-1, verbose=1)

# Fit GridSearchCV to the data
grid_search_lr.fit(X_train, y_train)

# Best parameters from Grid Search
print(f"Best parameters for Logistic Regression: {grid_search_lr.best_params_}")

# Predict using the best estimator from GridSearchCV
y_pred_lr = grid_search_lr.best_estimator_.predict(X_test)

# Accuracy score for Logistic Regression
accuracy_lr = accuracy_score(y_test, y_pred_lr)
print(f"Logistic Regression Accuracy with Grid Search: {accuracy_lr:.4f}")

"""#### **6. Random Search for Logistic Regression**

"""

# Define the hyperparameter distributions for Random Search
param_dist_lr = {
    'penalty': ['l1', 'l2'], # Regularization type
    'C': [0.01, 0.1, 1, 10, 100],                # Inverse of regularization strength            # Solver for optimization
    'max_iter': [100, 200, 500]    # Random number of iterations between 100 and 500
}

# Set up RandomizedSearchCV
random_search_lr = RandomizedSearchCV(estimator=lr_classifier, param_distributions=param_dist_lr,
                                      n_iter=20, cv=5, n_jobs=-1, random_state=42, verbose=1)

# Fit RandomizedSearchCV to the data
random_search_lr.fit(X_train, y_train)

# Best parameters from Random Search
print(f"Best parameters for Logistic Regression (Random Search): {random_search_lr.best_params_}")

# Predict using the best estimator from RandomizedSearchCV
y_pred_lr_random = random_search_lr.best_estimator_.predict(X_test)

# Accuracy score for Logistic Regression (Random Search)
accuracy_lr_random = accuracy_score(y_test, y_pred_lr_random)
print(f"Logistic Regression Accuracy with Random Search: {accuracy_lr_random:.4f}")

"""### **Explanation:**

1. **Grid Search**: Tries all possible combinations of the hyperparameters provided in `param_grid`. It's exhaustive but can be time-consuming for large grids.
   
2. **Random Search**: Randomly samples combinations of hyperparameters from the distributions provided in `param_dist`. It's faster than grid search, especially when the search space is large.

3. **Evaluation**: After training using `GridSearchCV` and `RandomizedSearchCV`, we print the best hyperparameters found and evaluate the model's performance on the test set using accuracy.

4. **`GridSearchCV` and `RandomizedSearchCV`**: Both these methods take a cross-validation (CV) approach to assess the performance of each hyperparameter configuration, ensuring the model generalizes well.

### **Results**:
The output of this code will include the best hyperparameters for each model and the corresponding accuracy scores. You can experiment with different datasets and hyperparameter ranges to see how the models perform in various scenarios.

# EY Attrition Rate
"""

import pandas as pd
import numpy as np

!gdown 16KtxSt_QEGQvfluEaMls5cCHPwhRXgCk
!gdown 19L3rYatfhbBL1r5MHrv-p_oM2wlvrhqk
!gdown 1OHLKJwA3qZopKPvlKoRldM6BvA1A4dYF
!gdown 1N7O_fWCTJLu8SIa_paKcDEzllgpMk8sK
!gdown 12Bh2AN8LcZAlg20ehpQrEWccUDaSdsOG

import pickle
# Load data (deserialize)
# with open('preprocessed_X_sm.pickle', 'rb') as handle:
#     X_train = pickle.load(handle)

# with open('X_test.pickle', 'rb') as handle:
#     X_test = pickle.load(handle)

# with open('y_sm.pickle', 'rb') as handle:
#     y_train = pickle.load(handle)

# with open('y_test.pickle', 'rb') as handle:
#     y_test = pickle.load(handle)

# Any transformations must be done post splitting into training/test,
# test on basis of training.


X_train = pd.read_pickle('preprocessed_X_sm.pickle')
X_test = pd.read_pickle('X_test.pickle')
y_train = pd.read_pickle('y_sm.pickle')
y_test = pd.read_pickle('y_test.pickle') #$use this if prevous code nor working

from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier


rf_clf = RandomForestClassifier(random_state=7, max_depth=4, n_estimators=100)


from sklearn.model_selection import KFold, cross_validate

kfold = KFold(n_splits=10)
cv_acc_results = cross_validate(rf_clf, X_train, y_train, cv=kfold, scoring='accuracy', return_train_score=True)

print(f"K-Fold Accuracy Mean: \n Train: {cv_acc_results['train_score'].mean()*100:.2f} \n Validation: {cv_acc_results['test_score'].mean()*100:.2f}")
print(f"K-Fold Accuracy Std: \n Train: {cv_acc_results['train_score'].std()*100:.2f}, \n Validation: {cv_acc_results['test_score'].std()*100:.2f}")

dt_clf = DecisionTreeClassifier(random_state=7, max_depth=4)


from sklearn.model_selection import KFold, cross_validate

kfold = KFold(n_splits=10)
cv_acc_results = cross_validate(dt_clf, X_train, y_train, cv=kfold, scoring='accuracy', return_train_score=True)

print(f"K-Fold Accuracy Mean: \n Train: {cv_acc_results['train_score'].mean()*100:.2f} \n Validation: {cv_acc_results['test_score'].mean()*100:.2f}")
print(f"K-Fold Accuracy Std: \n Train: {cv_acc_results['train_score'].std()*100:.2f}, \n Validation: {cv_acc_results['test_score'].std()*100:.2f}")