# -*- coding: utf-8 -*-
"""11-Notes-SVM-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15gmKWjlJpzbYA5LmFr7nhgO8x3E3umVX

## Content

- Problem Statement
- Geometric Intuition of SVM

  
- Hard Margin SVM

- Soft Margin SVM

- Algebric Intuition of SVM

- Intution of Hinge Loss

- SVM Imbalance

- Code Implementation of Linear SVM

---

### Problem Statement

<img src="https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/338/original/z.png?1705489394" width=800>

---

### SVM - Support Vector Machine

<img src="https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/339/original/z.png?1705489418" width=800>

---

### Geometric intution behind SVM

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/340/original/z.png?1705489441' width='800'>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/341/original/z.png?1705489464' width='800'>

---

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/342/original/z.png?1705489489' width='800'>

---

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/343/original/z.png?1705489521' width='800'>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/344/original/z.png?1705489546' width='800'>

---

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/345/original/z.png?1705489572' width='800'>

---

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/346/original/z.png?1705489599' width='800'>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/337/original/z.png?1705489355' width='800'>

---

### <b> SVM Demo </b>

https://jgreitemann.github.io/svm-demo

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/336/original/download.gif?1705489242' height='400' width='650'>

---

### Hard Margin SVM

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/335/original/z.png?1705489104' width='800'>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/334/original/z.png?1705489079' width='800'>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/332/original/z.png?1705489053' width='800'>

Example -

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/331/original/z.png?1705489026' width='800'>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/330/original/z.png?1705489000' width='800'>

---

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/329/original/z.png?1705488973' width='800'>

---

### Soft Margin SVM

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/328/original/z.png?1705488941' width='800'>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/327/original/z.png?1705488915' width='800'>

Now, our optimization problem becomes:

- $ max \ \frac{2}{||w||} $ i.e., the margin
- along with minimizing error $ \zeta_i's $

because we're try to get the best possible classificaton.

</br>

Can we think of another way to write this?

Reciprocating above equation,

- $ min \ \frac{||w||}{2} $ with $ \zeta_i's $

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/326/original/z.png?1705488886' width='800'>

---

### Hyperparameters in SVM

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/325/original/z.png?1705488851' width='800'>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/324/original/z.png?1705488501' width='800'>

Therefore, we need to find a balance here.

---

### Algebraic intuition behind SVM

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/323/original/z.png?1705488473' width='800'>

---

### Hinge Loss



<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/322/original/z.png?1705488444' width='800'>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/321/original/z.png?1705488415' width='800'>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/320/original/z.png?1705488341' width='800'>

---

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/319/original/z.png?1705488312' width='800'>
"""



"""---

### Comparison with Log Loss

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/318/original/z.png?1705488263' width='800'>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/317/original/z.png?1705488237' width='800'>

\
We will not be deriving how we get this equation.

---

### Data Imbalance

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/316/original/z.png?1705488198' width='800'>

---

### Code implementation of Linear SVM
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter
from sklearn import feature_extraction, model_selection, naive_bayes, metrics, svm
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
import warnings

warnings.filterwarnings('ignore')
# %matplotlib inline

!gdown 1QViUZJ5UIBCgxB_qbOXTLs_2V48w7MWo

df = pd.read_csv('Spam_processed.csv', encoding='latin-1')
df.dropna(inplace = True)

df

"""- Performing train-test split
- with [CountVectorization](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)
- and StandardScaler.
"""

from sklearn.model_selection import train_test_split

df_X_train, df_X_test, y_train, y_test = train_test_split(df['cleaned_message'], df['type'],
                                                          test_size=0.25, random_state=47)
print([np.shape(df_X_train), np.shape(df_X_test)])

# CountVectorizer
f = feature_extraction.text.CountVectorizer()
X_train = f.fit_transform(df_X_train)
X_test = f.transform(df_X_test)

# StandardScaler
scaler = StandardScaler(with_mean=False) # problems with dense matrix
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

print([np.shape(X_train), np.shape(X_test)])
print(type(X_train))

"""Let's train Linear SVM on the given Spam/Ham data.

"""

# SVC

from sklearn.svm import SVC

from sklearn.model_selection import GridSearchCV

params = {
          'C': [1e-4,  0.001, 0.01, 0.1, 1,10] # which hyperparam value of C do you think will work well?
         }

svc = SVC(class_weight={ 0:0.1, 1:0.5 }, kernel='linear')
clf = GridSearchCV(svc, params, scoring = "f1", cv=3)

clf.fit(X_train, y_train)

res = clf.cv_results_

for i in range(len(res["params"])):
  print(f"Parameters:{res['params'][i]} \n Mean score: {res['mean_test_score'][i]} \n Rank: {res['rank_test_score'][i]}")

"""As you can see,
- we get the best performance when $C=0.001$,
- with F1 Score of 0.77.

\
Now implementing this SVM on the test data.
"""

svc = SVC(C=0.001,class_weight={ 0:0.1, 1:0.5 }, kernel='linear')

svc.fit(X_train, y_train)

y_pred = svc.predict(X_test)

print(metrics.f1_score(y_test,y_pred))

"""Linear SVM performs much well
- on the Spam/Ham data
- with F1 Score of 0.88
- when using class weights.
"""