# -*- coding: utf-8 -*-
"""6-Notes-Boosting-1-Digital.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DsvExP4xaBoyJngHauSGAVFYURiYKj9C

## Content

- **Bagging recap**

- **Boosting Introduction**

    
    


- **Boosting Intuition - How do we combine base learners ?**
    - Residual
    - Geometrical Intuition



- **What happens at train and test time ?**

- **Why Boosting ? - GBDT Intuition**
    - Pseudo residual



- **Sklearn implementation**

## Bagging recap

#### What did we do in Bagging?

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/222/original/z.png?1705481766' width=800></center>

## Boosting Introduction

On the other hand, in boosting, we again have **base learners**

<br>

#### So, what's the nature of base learners in boosting ?

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/224/original/z.png?1705481907' width=800></center>

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/226/original/z.png?1705481977' width=800></center>

## Boosting Intuition - How do we combine base learners ?

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/227/original/z.png?1705482004' width=800></center>

### Step 0 : $M_0$

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/228/original/z.png?1705482031' width=800></center>

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/229/original/z.png?1705482061' width=800></center>

Now, there'll be some error associated with these predictions

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/230/original/z.png?1705482089' width=800></center>

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/231/original/z.png?1705482116' width=800></center>

#### What is our goal ?

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/232/original/z.png?1705482146' width=800></center>

#### How would predicting the error help in reducing it ?

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/233/original/z.png?1705482171' width=800></center>

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/234/original/z.png?1705482196' width=800></center>

### Step 1 : $M_1$

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/235/original/z.png?1705482226' width=800></center>

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/236/original/z.png?1705482254' width=800></center>

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/237/original/z.png?1705482279' width=800></center>

#### Issues with simple addition

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/238/original/z.png?1705482304' width=800></center>

#### How do we resolve this issue ?

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/239/original/z.png?1705482334' width=800></center>

#### How do we learn these weights ?

We do so using optimization (will be discussed later)

But, we don't stop here.

We move onto next step

### Step 2: $M_2$

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/241/original/z.png?1705482367' width=800></center>

#### What will be residual used in step 2 ?

Here, $err^i_1$ will be
-  residual left after Stage 1 final model prediction

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/242/original/z.png?1705482396' width=800></center>

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/243/original/z.png?1705482422' width=800></center>

### Step M :

The final model @ end of stage M will be :

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/245/original/z.png?1705482448' width=800></center>

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/246/original/z.png?1705482477' width=800></center>

### Example

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/247/original/z.png?1705482505' width=800></center>

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/248/original/z.png?1705482530' width=800></center>

Now, we use the features (height , gender) and these error value to fit a new model to it
- i.e. Stage 1 model

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/249/original/z.png?1705482555' width=800></center>

Then we calculate the final predicition

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/250/original/z.png?1705482581' width=800></center>

followed by calculating error i.e. (y - final prediction @ end of stage 1 )

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/251/original/z.png?1705482608' width=800></center>

### Geometrical Intution

#### How does model fitting looks like geometrically ?

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/253/original/gbdt27.svg?1705482782' width=800></center>

Zoomed in version, for reference (gif):

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/254/original/svg.gif?1705482812' width=700></center>

The whole process along with the reduction in residual looks as follows :

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/255/original/download.jpeg?1705482949' width=700></center>

Notice that
- as we add the stage K's prediction to the final prediction
    - the regression like fits better to the data
- also the residual is reducing for each iteration.

## What happens at train and test time ?

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/256/original/z.png?1705482988' width=800></center>

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/257/original/z.png?1705483017' width=800></center>

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/258/original/z.png?1705483046' width=800></center>

## Why Boosting ? - Gradient Boosting Intuition

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/259/original/z.png?1705483071' width=800></center>

### Pseudo residuals

Say, we want to minimize the squared loss

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/260/original/z.png?1705483097' width=800></center>

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/261/original/z.png?1705483124' width=800></center>

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/262/original/z.png?1705483151' width=800></center>

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/263/original/z.png?1705483175' width=800></center>

#### But, how do we use these pseudo residuals ?

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/264/original/z.png?1705483204' width=800></center>

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/265/original/z.png?1705483226' width=800></center>

#### How does using pseudo residual helping us ?

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/266/original/z.png?1705483253' width=800></center>

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/267/original/z.png?1705483277' width=800></center>

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/062/268/original/z.png?1705483300' width=800></center>

#### (Optional) Extra Read - Pseudo residual Log loss

The computation of Log loss pseudo residual is a bit math heavy.
- hence, has been provided as extra read

Do check it out: https://colab.research.google.com/drive/1TAp1vuROQyVyhntq8Bv4VhfshouSKvnB?usp=sharing

## Sklearn implemntation

Let's implement it on the one of the previously used dataset
- Employee attrition dataset (used in DT)
"""

!gdown 19L3rYatfhbBL1r5MHrv-p_oM2wlvrhqk
!gdown 1OHLKJwA3qZopKPvlKoRldM6BvA1A4dYF
!gdown 1N7O_fWCTJLu8SIa_paKcDEzllgpMk8sK
!gdown 12Bh2AN8LcZAlg20ehpQrEWccUDaSdsOG

import pickle
# Load data (deserialize)
with open('preprocessed_X_sm.pickle', 'rb') as handle:
    X_train = pickle.load(handle)

with open('X_test.pickle', 'rb') as handle:
    X_test = pickle.load(handle)

with open('y_sm.pickle', 'rb') as handle:
    y_train = pickle.load(handle)

with open('y_test.pickle', 'rb') as handle:
    y_test = pickle.load(handle)

"""<br>

But before that
- we need to know what are the hyperparams we need to tune

As we learnt that
- there are M step in making the model
- we need to control how many steps/iteration we want i.e. how many base learners we want

As learners should be high bias low variance
- we would have to experiment with depth of the tree
- hence, it is again a hyperparam


Boosting also gives us flexibility to choose the loss function
- Hence, it is also an hyperparam.


<br>

Other than that,
- there are hyperparam related to DT which we have already seen.

:Now, let's import the class from sklearn

sklearn GBDT: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html
"""

from sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier(n_estimators=150, max_depth=2, loss = 'log_loss')

gbc.fit(X_train, y_train)

gbc.score(X_test, y_test)

gbc.score(X_train, y_train)

"""Model is slightly overfitting.


Do Note that:
- It is very easy to overfit the boosting model
- So, we have to careful while tuning it.

We'll see in next lecture how to regularize GBDT


"""