# -*- coding: utf-8 -*-
"""Other Ensemble Techniques.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xm7yoTeuOOwOiQ6KynybGg9AwxuTcBv9

# Interview Questions

#### 1. **Why is KNN considered a non-parametric model?**
   - A) It does not make any assumptions about the underlying data distribution.
   - B) It adjusts model parameters based on the number of neighbors.
   - C) It assumes a fixed number of features for all classifications.
   - D) It learns a set of parameters during training.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) It does not make any assumptions about the underlying data distribution.

   **Explanation:** KNN is a non-parametric model because it does not assume any particular form for the data distribution; instead, it classifies based on the data points' proximity to one another.

#### 2. **How can decision trees lead to overfitting, and what is one common technique to prevent this?**
   - A) Overfitting occurs when the tree is too deep; pruning can prevent this.
   - B) Overfitting occurs when the tree is too shallow; increasing tree depth prevents this.
   - C) Overfitting occurs when too many features are used; reducing the number of features prevents this.
   - D) Overfitting occurs when entropy is used as a split criterion; using Gini impurity prevents this.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) Overfitting occurs when the tree is too deep; pruning can prevent this.

   **Explanation:** Deep trees tend to overfit the training data by capturing noise. Pruning reduces the depth of the tree, preventing overfitting by simplifying the model.

#### 3. **Why is feature scaling important in KNN, and what happens if it is not done?**
   - A) KNN is sensitive to outliers, and feature scaling increases model accuracy.
   - B) Without feature scaling, features with larger magnitudes dominate distance calculations.
   - C) Feature scaling is not important in KNN, as it only affects the number of neighbors.
   - D) KNN does not rely on feature scaling because it normalizes data internally.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) Without feature scaling, features with larger magnitudes dominate distance calculations.

   **Explanation:** KNN uses distance metrics like Euclidean distance, where features with larger scales dominate the distance calculation, leading to biased predictions.

#### 5. **How does bagging reduce variance in model predictions?**
   - A) By increasing the number of features used in each decision.
   - B) By averaging predictions from multiple models trained on random subsets of data.
   - C) By reducing the depth of each individual tree in the ensemble.
   - D) By focusing on difficult-to-classify data points.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) By averaging predictions from multiple models trained on random subsets of data.

   **Explanation:** Bagging reduces variance by training multiple models on different subsets of the data and averaging their predictions, which smooths out noise and improves generalization.

#### 6. **What is the primary assumption of Linear Regression regarding the relationship between features and the target variable?**
   - A) The relationship between features and the target is nonlinear.
   - B) The features are independent and follow a Gaussian distribution.
   - C) There is a linear relationship between features and the target variable.
   - D) The features are categorical and independent.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** C) There is a linear relationship between features and the target variable.

   **Explanation:** Linear Regression assumes a linear relationship between the independent variables (features) and the dependent variable (target), which is a key limitation of the model.

#### 7. **What is the key difference between bagging and boosting?**
   - A) Bagging increases variance, while boosting decreases bias.
   - B) Bagging builds models sequentially, while boosting builds models in parallel.
   - C) Bagging builds models in parallel, while boosting builds models sequentially.
   - D) Bagging focuses on difficult examples, while boosting reduces the depth of trees.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** C) Bagging builds models in parallel, while boosting builds models sequentially.

   **Explanation:** Bagging involves training multiple models in parallel on random subsets of the data, while boosting trains models sequentially, with each new model focusing on errors from the previous ones.

#### 8. **Why might Logistic Regression struggle with highly imbalanced datasets?**
   - A) The model fits all data points equally, which reduces generalization.
   - B) The model is sensitive to outliers, causing it to overfit the minority class.
   - C) The predicted probabilities may be skewed toward the majority class, reducing accuracy for the minority class.
   - D) Logistic Regression cannot be trained on imbalanced datasets.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** C) The predicted probabilities may be skewed toward the majority class, reducing accuracy for the minority class.

   **Explanation:** Logistic Regression tends to predict the majority class more frequently in imbalanced datasets, leading to poor performance on the minority class unless additional techniques like class weighting are used.

#### 9. **In KNN, how does the choice of distance metric affect the model’s performance?**
   - A) The choice of distance metric has no effect on KNN’s performance.
   - B) Different distance metrics can give different importance to features, impacting accuracy.
   - C) KNN always uses Euclidean distance, regardless of feature types.
   - D) Manhattan distance is always better than Euclidean distance for all datasets.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) Different distance metrics can give different importance to features, impacting accuracy.

   **Explanation:** The choice of distance metric (e.g., Euclidean, Manhattan) affects how distances between points are calculated, which can impact model performance, especially with high-dimensional or mixed-type data.

#### 10. **What is the significance of regularization in Logistic Regression?**
   - A) Regularization increases the number of features used for classification.
   - B) Regularization adds a penalty for large coefficients, reducing overfitting.
   - C) Regularization focuses on improving the decision boundary’s accuracy.
   - D) Regularization changes the cost function to minimize residuals.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) Regularization adds a penalty for large coefficients, reducing overfitting.

   **Explanation:** Regularization (L1 or L2) penalizes large coefficient values in Logistic Regression, reducing the risk of overfitting by encouraging simpler models with smaller coefficients.

#### 11. **How does boosting improve model accuracy over bagging in some scenarios?**
   - A) Boosting combines deep decision trees to improve generalization.
   - B) Boosting reduces bias by focusing on hard-to-classify data points in each iteration.
   - C) Boosting reduces variance by training on random subsets of data.
   - D) Boosting uses feature scaling to improve model performance.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) Boosting reduces bias by focusing on hard-to-classify data points in each iteration.

   **Explanation:** Boosting sequentially trains models that focus on correcting errors from previous iterations, which reduces bias and improves performance, especially on difficult data points.

#### 12. **In Linear Regression, how does multicollinearity between features affect the model?**
   - A) Multicollinearity improves model accuracy by using highly correlated features.
   - B) Multicollinearity increases the standard errors of the coefficients, making them unreliable.
   - C) Multicollinearity leads to a higher R-squared value, indicating a better fit.
   - D) Multicollinearity improves interpretability by highlighting important features.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) Multicollinearity increases the standard errors of the coefficients, making them unreliable.

   **Explanation:** Multicollinearity inflates the standard errors of the coefficient estimates, making it difficult to determine the individual effect of each feature on the target variable.

#### 13. **How does bagging improve the stability of decision trees?**
   - A) By averaging predictions from deep trees, reducing overfitting.
   - B) By reducing the number of features used for each split.
   - C) By increasing the depth of each tree to capture complex patterns.
   - D) By using fewer data points for each model, reducing bias.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) By averaging predictions from deep trees, reducing overfitting.

   **Explanation:** Bagging improves the stability of decision trees by averaging predictions from multiple trees, reducing overfitting and increasing the model's generalizability.

#### 14. **What is the role of cross-entropy loss in Logistic Regression?**
   - A) It measures the error in predicting continuous variables.
   - B) It quantifies the difference between predicted probabilities and actual class labels.
   - C) It penalizes large coefficients to prevent overfitting.
   - D) It increases the complexity of the model to fit the data.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** B) It quantifies the difference between predicted probabilities and actual class labels.

   **Explanation:** Cross-entropy loss measures the difference between the predicted probabilities of the Logistic Regression model and the actual class labels, helping optimize the model during training.

#### 15. **Why does increasing the number of weak learners in boosting sometimes lead to overfitting?**
   - A) The model becomes too complex, fitting noise in the training data.
   - B) The model decreases in complexity, resulting in underfitting.
   - C) Boosting always reduces bias and variance, so overfitting is not an issue.
   - D) The model is trained on random subsets, which introduces more bias.

   <br>
<br>
<br>
<br>
<br>
<br>
   **Correct Answer:** A) The model becomes too complex, fitting noise in the training data.

   **Explanation:** Adding too many weak learners in boosting can lead to overfitting, where the model starts to fit noise in the training data, resulting in poor generalization to new data.

# XGboost
"""

import pickle
import pandas as pd


!gdown 171Yoe_GSapyrmOnD9oBzHWNOD_OnQs0F
!gdown 1hnIlTPW3AMeB69EbeaXCRIrpMVT1Vwmc
!gdown 1nZtB_RtxMg_MgoRczb8UWQX-AEK_l3qE
!gdown 1zLDUErwKdmF-RacOyHEuI_z_46LssQtP


# with open('X_train.pickle', 'rb') as handle:
#     X_train = pickle.load(handle)

# with open('X_test.pickle', 'rb') as handle:
#     X_test = pickle.load(handle)

# with open('Y_train.pickle', 'rb') as handle:
#     y_train = pickle.load(handle)

# with open('Y_test.pickle', 'rb') as handle:
#     y_test = pickle.load(handle)

X_train = pd.read_pickle('X_train.pickle')
X_test = pd.read_pickle('X_test.pickle')
y_train = pd.read_pickle('Y_train.pickle')
y_test = pd.read_pickle('Y_test.pickle')

from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.model_selection import StratifiedKFold


import datetime as dt

params = {
        "n_estimators": [50,100,150,200],
        "max_depth" : [3, 4, 5, 7],
        "learning_rate": [0.1, 0.2, 0.3],
        'subsample': [0.6, 0.8, 1.0], # sampling no of rows
        'colsample_bytree': [0.6, 0.8, 1.0],# sampling no of columns
        }
xgb = XGBClassifier(objective='multi:softmax', num_class=20)
# softmax is what you use when you've multi-class, will be covered in detail in NN module.

random_search = RandomizedSearchCV(xgb,
                                   param_distributions=params,
                                   n_iter=10,
                                   scoring='accuracy',
                                   n_jobs=-1,
                                   cv=3,
                                   verbose=2)


start = dt.datetime.now()
random_search.fit(X_train, y_train)
end = dt.datetime.now()

res = random_search.cv_results_

for i in range(len(res["params"])):
  print(f"Parameters:{res['params'][i]} Mean_score: {res['mean_test_score'][i]} Rank: {res['rank_test_score'][i]}")

print(f"Time taken for fits : {end - start}")
print(random_search.best_estimator_)

xgb = random_search.best_estimator_

xgb.fit(X_train, y_train)

print("Model acc",xgb.score(X_test, y_test))

"""# LightGBM"""

import lightgbm as lgb
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
#Refer: https://lightgbm.readthedocs.io/en/latest/Parameters.html
import datetime as dt
gridParams = {
    'learning_rate': [0.1, 0.3, 0.5],
    'boosting_type' : ['gbdt'],
    'objective' : ['multiclass'],
    'max_depth' : [5,6,7,8],
    'colsample_bytree' : [0.5,0.7],
    'subsample' : [0.5,0.7]
    }

clf = lgb.LGBMClassifier(num_classes=20)
random_cv = RandomizedSearchCV(clf,gridParams,verbose=3,cv=3,n_jobs = -1,n_iter=10)

start = dt.datetime.now()
random_cv.fit(X_train,y_train)
end = dt.datetime.now()

res = random_cv.cv_results_

for i in range(len(res["params"])):
  print(f"Parameters:{res['params'][i]} Mean_score: {res['mean_test_score'][i]} Rank: {res['rank_test_score'][i]}")

print(f"Time taken for fits : {end - start}")

lgb = random_cv.best_estimator_

lgb.fit(X_train, y_train)

print("Model acc",lgb.score(X_test, y_test))

from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import StackingClassifier
X, y = load_iris(return_X_y=True)
estimators = [
    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),
    ('svr', make_pipeline(StandardScaler(),
                          LinearSVC(random_state=42)))
]
clf = StackingClassifier(
    estimators=estimators, final_estimator=LogisticRegression()
)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, random_state=42
)
clf.fit(X_train, y_train).score(X_test, y_test)