# -*- coding: utf-8 -*-
"""1-Notes-KNN-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oHm8PVajZmQPj31HK4D9Wy2fbd7O_E9i

## Content

### kNN

- Blinkit Problem Statement
 - Issue with Logistic Regression

- Geometric Intuition



- The KNN Algorithm


- kNN Scratch Code


- Assumption of KNN

# **Business Case**

**Blinkit** is trying to find an **optimal number of delivery partners per store** for faster delivery

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/471/original/Screenshot_2023-07-23_at_7.43.57_PM.png?1690130758 width=700>

- Hence it has classified its store into 3 classes based on
 - how much products are delivered from a store

1. Class1 : High Traffic (Needs a high number of delivery partners)
2. Class2 : Moderate Traffic (Needs a decent number of delivery partners)
3. Class3 : Low Traffic (Needs a very small number of delivery partners)
"""

import pandas as pd
import numpy as np

!gdown 1ZdhRqYv-JizWV6DxO6C4R_k1kxPhmlF2

df=pd.read_csv('multiclass.csv')
df.head()

"""**Data Description**

|feature|Description|
|-|-|
|**Region**|where the store is located (1: Tier-1 city, 2: Tier-2 city, 3:Tier-3 city)|
|**Fresh**|Fresh food products delivery count |
|**Milk**|milk products delivery count|
|**Grocery**|Grocery products delivery count|
|**Frozen**|Frozen food products delivery count|
|**Detergents_Paper**|washing products delivery count|
|**Delicassen**|imported products, cooked meat delivery count|
|**class**|Store catgeory (Class1, Class2, Class3)|

Now lets see the count of sample each of the three classes
"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.histplot(data=df, x="class")
plt.title('Data Distribution')
plt.show()

"""Lets now define our X and y for the model and visualize the data"""

X=df[['Region',	'Fresh',	'Milk',	'Grocery',	'Frozen',	'Detergents_Paper',	'Delicassen'	]].copy()
y=df[['class']].copy()

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt


def scatter_visualize(visualize1,visualize2,y_label):
  scatter = plt.scatter(visualize[:,0] ,visualize[:,1],c = y.values[:,0] )
  plt.legend(handles=scatter.legend_elements()[0], labels=['Class1','Class2','Class3'])
  plt.show()

# reducing features to just 2 features for visualization purpose
pca = PCA(n_components=2)
visualize = pca.fit_transform(X)


scatter_visualize(visualize[:,0] ,visualize[:,1],y.values[:,0])

"""**observe**

Data is not only **multi classed imbalanced** data
- It is also **Non-Linear in nature**

#### Imagine if data ($n \approx 1 million $), will Polynomial Logistic Regression model be the right model ?

Ans: No, Clearly it will require **a lot of hyperparameter tuning**:
- for finding the **correct polynomial features for Logistic Regression**

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/473/original/Screenshot_2023-07-23_at_7.44.08_PM.png?1690130923 width=700>

This process will be time consuming, hence not the ideal approach

<br>

#### If store categories changes from 3 to 50, will OVR be the ideal approach ?

Ans: No, since for a  **50 class data, 50 different Logistic Regression model** will be required:
-  This process will be both **Expensive and time consuming**

<br>

Hence **Polynomial OVR Logistic Regression model not the ideal approach**

# **Geometric Intuition of kNN**

#### What model can be used for this Non-linear Multi-Class data ?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/472/original/Screenshot_2023-07-23_at_7.44.16_PM.png?1690130898 width=700>

Imagine if our training data looks like:

1. Class 1 data $→ (+) $
2. Class 2 data $→ (-) $
3. Class 3 data $→ (o) $

#### If we have a test data sample,($x^{q1}$), what will be class label for it ?

Ans: Clearly, $x^{q1}$ belongs to $(+)$ Class


<br>

#### What will be the class label for ($x^{q2}$) ?
Ans: $x^{q2}$ belongs to $(-)$ Class

#### How are we sure that $(x^{q1})$ belongs to $(+)$ and $(x^{q2})$ belongs to $(-)$ class ?

Ans: Simply by:
- **Observing the neighboring points** of $(x^{q1})$ and noticing as most points belong to $(+)$ Class,
 - hence $(x^{q1})$ → $(+)$

- Similarly for $(x^{q2})$,  **most of the neighboring points → (-) class**
 - hence $(x^{q2})$ → $(-)$


<br>


kNN (k Nearest Neighbour) algorithm works on the same intuition:
- Defining **class of the test datapoint based** on the **classes of the neighbourhood** of that datapoint

# **Understanding kNN**

#### How does kNN work with neighborhood of datapoint ?

Ans: Lets understand kNN with an example, Suppose **we have a test datapoint ($x^q = [2,5]$) and we have 6 training data such that**:

||f1|f2|y|
|:--|:--|:--|:--|
|$x^{(1)}$|3|6|1
|$x^{(2)}$|6|4|1
|$x^{(3)}$|8|2|3
|$x^{(4)}$|7|5|3
|$x^{(5)}$|1|4|2
|$x^{(6)}$|2|2|2

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/474/original/Screenshot_2023-07-23_at_7.44.29_PM.png?1690130963 width=700>

1. **Step1:** We find the  **Euclidean distance between $x_q = [2,5]$ and all the training data.**


||f1|f2|y|$d^{(i)} = \sqrt{(x^q_{f1}-x^{(i)}_{f1})^2 + (x^q_{f2}-x^{(i)}_{f2})^2}$|
|:--|:--|:--|:--|:--|
|$x^{(1)}$|3|6|1|$\sqrt{(1^2 + 1^2)} = 1.41$|
|$x^{(2)}$|6|4|1|$\sqrt{(4^2 + 1^2)} = 3.00$|
|$x^{(3)}$|8|2|3|$\sqrt{(6^2 + 3^2)} = 6.48$|
|$x^{(4)}$|7|5|3|$\sqrt{(5^2 + 0^2)} = 5.00$|
|$x^{(5)}$|1|4|2|$\sqrt{(1^2 + 1^2)} = 1.41$|
|$x^{(6)}$|2|2|2|$\sqrt{(0^2 + 2^2)} = 2.00$|

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/475/original/Screenshot_2023-07-23_at_7.44.37_PM.png?1690130998 width=700>

2. **Step2:** We sort the data based on the distances:

||f1|f2|y|$d^{(i)}$|
|:--|:--|:--|:--|:--|
|$x^{(1)}$|3|6|1|$1.41$|
|$x^{(5)}$|1|4|2|$1.41$|
|$x^{(6)}$|2|2|2|$2.00$|
|$x^{(2)}$|6|4|1|$3.00$|
|$x^{(4)}$|7|5|3|$5.00$|
|$x^{(3)}$|8|2|3|$6.48$|

<br>

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/476/original/Screenshot_2023-07-23_at_7.44.47_PM.png?1690131021 width=700>

3. **Step 3:** Assume we pick the 3 datapoints such that:
-  The distance from $x_q$ is minimum

||f1|f2|y|$d^{(i)}$|
|:--|:--|:--|:--|:--|
|$x^{(1)}$|3|6|1|$1.41$|
|$x^{(5)}$|1|4|2|$1.41$|
|$x^{(6)}$|2|2|2|$2.00$|

Hence we pick $x^{(1)},x^{(5)},x^{(6)}$


<br>

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/477/original/Screenshot_2023-07-23_at_7.44.57_PM.png?1690131095 width=700>

4. **Step4:** Find the majority vote on the class $(y)$ for these selected datapoints:
- And the **class which is in majority** becomes the **class label for $x_q$**

hence here  Class of $x_q = 2$

<br>

**Note:** This selection of datapoints  is done using a hyperparameter $k$,
- hence the algorithm is called $k$ nearest neighbors

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/479/original/Screenshot_2023-07-23_at_7.45.06_PM.png?1690131159 width=700>

#### What happens if $k = 4$ for kNN ?

Ans:  Lets look into the sorted data:

||f1|f2|y|$d^{(i)}$|
|:--|:--|:--|:--|:--|
|$x^{(1)}$|3|6|1|$1.41$|
|$x^{(5)}$|1|4|2|$1.41$|
|$x^{(6)}$|2|2|2|$2.00$|
|$x^{(2)}$|6|4|1|$3.00$|
|$x^{(4)}$|7|5|3|$5.00$|
|$x^{(3)}$|8|2|3|$6.48$|

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/478/original/Screenshot_2023-07-23_at_7.45.15_PM.png?1690131134 width=700>

Now we will select the nearest 4 datapoints:

||f1|f2|y|$d^{(i)}$|
|:--|:--|:--|:--|:--|
|$x^{(1)}$|3|6|1|$1.41$|
|$x^{(5)}$|1|4|2|$1.41$|
|$x^{(6)}$|2|2|2|$2.00$|
|$x^{(2)}$|6|4|1|$3.00$|


**observe**

$x^{(1)}$ and $x^{(2)}$ → Class1 while  $x^{(5)}$ and $x^{(6)}$ → Class2

**kNN cannot make a prediction**
- As there is a tie in majority vote

<br>


**note:** It is advisable to have $k \in odd$
- So  to avoid this issue of tie in majority vote.

Suppose we took $k =5 $ and the nearest neighbor looks like:

||y|
|:--|:--|
|$x^{(1)}$| 1 |
|$x^{(5)}$| 2 |
|$x^{(2)}$| 1 |
|$x^{(7)}$| 2 |
|$x^{(4)}$| 3 |

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/480/original/Screenshot_2023-07-23_at_7.45.24_PM.png?1690131190 width=700>

#### How to make prediction for when $k \in odd$ and have tie in majority vote ?
Ans: In such case, the hack is to:
- **Randomly pick the class label for any of the tied Classes**




For this example:

||y|
|:--|:--|
|$x^{(1)}$| 1 |
|$x^{(5)}$| 2 |
|$x^{(2)}$| 1 |
|$x^{(7)}$| 2 |
|$x^{(4)}$| 3 |


- kNN randomly picks Class1 or Class 2

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/482/original/Screenshot_2023-07-23_at_7.45.40_PM.png?1690131330 width=700>

# **kNN Scratch Code**

lets split the data into train, validation and test set
"""

from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

X_train_val, X_test, y_train_val, y_test = train_test_split(X,y,test_size=0.25, random_state=42)

X_train, X_val, y_train, y_val = train_test_split(X_train_val,y_train_val,test_size=0.25, random_state=42)

print(X_train.shape, y_train.shape)
print(X_val.shape, y_val.shape)
print(X_test.shape, y_test.shape)

"""#### SMOTE"""

from imblearn.over_sampling import SMOTE

# Create an instance of SMOTE
smt = SMOTE()

X_sm, y_sm = smt.fit_resample(X_train, y_train)

y_sm.value_counts()

"""Standardization"""

from sklearn.preprocessing import StandardScaler

st =  StandardScaler()

X_sm = st.fit_transform(X_sm.values)
X_val_scaled = st.transform(X_val.values)
X_test_scaled = st.transform(X_test.values)

y_sm = y_sm.values[:,0]
y_val = y_val.values[:,0]
y_test = y_test.values[:,0]

"""kNN Implementation from Scratch
 with k = 5

taking the 50th Val data as our $x_q$
"""

X_val_scaled[50]

"""finding distance"""

dist = np.sqrt(np.sum((X_val_scaled[50]-X_sm)**2,axis=1) )

print(dist.shape)

# Storing distance and Class labels together
distances = [(dist[i],y_sm[i]) for i in range(len(dist)) ]

# sort the distances
distances = sorted(distances)

# Nearest/First K points
distances = distances[:5]

distances = np.array(distances)

classes_counts = np.unique(distances[:,1],return_counts=True)

index = classes_counts[1].argmax()
pred = classes_counts[0][index]

print('kNN prediction:',pred)

print('Neighbors[(distance),(Label)]:',distances)

"""Formulating into one function"""

def knn(X,Y,queryPoint,k):
    """Predict the class label for the query point"""
    # Euclidean Distance
    dist = np.sqrt(np.sum((queryPoint-X)**2,axis=1) )

    # Storing distance and Class labels together
    distances = [(dist[i],Y[i]) for i in range(len(dist)) ]
    # sort the distances
    distances = sorted(distances)
    # Nearest/First K points
    distances = distances[:k]

    distances = np.array(distances)

    classes_counts = np.unique(distances[:,1],return_counts=True)

    index = classes_counts[1].argmax()
    pred = classes_counts[0][index]

    return int(pred),distances

"""Predicted class label for $x_q$"""

pred,neighbors = knn(X_sm, y_sm, X_val_scaled[50],5)

print(f'k nearest neighbors with the distance and class label :{neighbors}')

print(f'The predicted class label: {pred}')

"""**Observe**

All the 5 datapoints which are closest to the $x^q$ have:
- The same class labels = 3

Hence due to majority vote:
- $x^q$ → Class3

<br>

Lets see what is the actual class label for $x^q$
"""

y_val[50]

"""This means that kNN correctly predicted $x^q$


"""

from sklearn.metrics import confusion_matrix

y_pred = []

for i in range(X_test_scaled.shape[0]):
    output,_ = knn(X_sm, y_sm, X_test_scaled[i],5)
    y_pred.append(output)




cm = confusion_matrix(y_test,y_pred)

from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay(cm).plot()

from sklearn.metrics import classification_report

print(classification_report(y_pred,y_test))

"""**observe**

Clearly the kNN model performed quite well on the data, showing :
- kNN can handle Multi-Class problem
- kNN can handle Non-linearity in the data

# **Assumption of KNN:**

if data contains:
1. 33 $(+)$ Class samples
2. 33 $(-)$ Class samples
3. 33 $(o)$ Class samples

<br>

#### What assumption does kNN makes during prediction ?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/481/original/Screenshot_2023-07-23_at_7.45.57_PM.png?1690131249 width=700>

Ans: As kNN works on the principle that:
- All the **characteristic the majority nearest neighbors have, same** should be for the **query datapoint**

- hence **kNN assumes that the neighborhood** for a datapoint must be **homogeneous in nature (having same properties)**

Therefore if there is alot of noise/outliers in the data:

- kNN model fails in most cases
"""