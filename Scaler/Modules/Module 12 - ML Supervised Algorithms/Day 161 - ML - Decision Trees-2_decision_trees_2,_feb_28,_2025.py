# -*- coding: utf-8 -*-
"""Decision Trees - 2, Feb 28, 2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H5TOvH50tCYWzatttiLv50s_FNUGZ7Jb

# Interview Questions

### 1. **Question:**
When analyzing **bias vs. variance**, you discover your model consistently underpredicts for most data points and has stable predictions across multiple training sets. Which subtle conclusion might best describe this situation?

- A) The model exhibits high variance because it is sensitive to outliers in the dataset.
- B) The model has high bias, as it fails to capture the true relationship for most instances.
- C) The model’s variance must be high because it underpredicts consistently.
- D) The model necessarily has a perfect balance of bias and variance since errors are uniform.

<br>
<br>
<br>
<br>
<br>
<br>

**Correct Answer:**  B) The model has high bias, as it fails to capture the true relationship for most instances  
**Explanation:**  
Underprediction across the board with little change between different training sets suggests a systematic error—typical of high bias—rather than high variance, which would show inconsistent predictions.

---

### 2. **Question:**
You tune **K-Nearest Neighbors (KNN)** for a classification task. As you increase $k$, training accuracy decreases, but validation accuracy initially rises then declines. What is a nuanced reason for this behavior?

- A) Adding more neighbors never changes model bias or variance.
- B) Increasing $k$ drastically increases variance but always reduces bias.
- C) A moderate $k$ smooths decision boundaries, reducing variance without introducing too much bias; excessively large $k$ can over-smooth and increase bias.
- D) Once $k$ becomes large, the model automatically transitions to linear regression.

<br>
<br>
<br>
<br>
<br>
<br>

**Correct Answer:**  C) A moderate $k$ smooths decision boundaries, reducing variance without introducing too much bias; excessively large $k$ can over-smooth and increase bias  
**Explanation:**  
With small $k$, the model can be overly sensitive (high variance). Increasing $k$ helps reduce variance, but too large a $k$ can lead to over-simplified boundaries and higher bias.

---

### 3. **Question:**
When evaluating **logistic regression** on an imbalanced dataset, you notice high accuracy but poor recall. Which subtle factor might be causing this discrepancy?

- A) The model predicts nearly all instances as the minority class, lowering accuracy but increasing recall.
- B) Accuracy is unaffected by class imbalance, so an alternative explanation is needed.
- C) The model may be predicting mostly the majority class (negative), thus achieving high overall accuracy while rarely detecting true positives.
- D) High recall means the model is capturing all positives, contradicting the observed outcome.

<br>
<br>
<br>
<br>
<br>
<br>

**Correct Answer:**  C) The model may be predicting mostly the majority class (negative), thus achieving high overall accuracy while rarely detecting true positives  
**Explanation:**  
In imbalanced datasets, predicting the majority class can yield high accuracy but result in very low recall for the minority class. This is why accuracy alone can be misleading in skewed class distributions.

---

### 4. **Question:**
You apply **gradient descent** to a loss function you suspect is convex, yet you observe non-monotonic decreases in cost. What subtle aspect might explain this phenomenon?

- A) A standard convex loss cannot fluctuate; the cost must strictly decrease at each step.
- B) Even in convex losses, an excessively large learning rate can cause momentary cost increases (overshooting).
- C) Gradient descent automatically lowers the cost function in each iteration, eliminating fluctuations.
- D) The dataset size is too small, preventing true convexity from emerging.

<br>
<br>
<br>
<br>
<br>
<br>

**Correct Answer:**  B) Even in convex losses, an excessively large learning rate can cause momentary cost increases (overshooting)  
**Explanation:**  
Convexity ensures a single global minimum, but if the learning rate is too high, gradient descent steps can overshoot the minimum locally, causing temporary fluctuations in the loss.

---

### 5. **Question:**
A **KNN** model is used for regression, but predictions show unexpectedly large oscillations between neighboring points. Which subtle factor could best explain this?

- A) KNN for regression always averages the entire dataset, ensuring smooth predictions.
- B) A large $k$ value is causing these extreme swings in predictions.
- C) Maybe data is not scaled, so distance calculations overemphasize certain features, causing inconsistent neighbor sets.
- D) KNN ignores variance altogether, focusing only on bias.

<br>
<br>
<br>
<br>
<br>
<br>

**Correct Answer:**  C) The data is not scaled, so distance calculations overemphasize certain features, causing inconsistent neighbor sets  
**Explanation:**  
Unscaled features can distort distance metrics, making certain features dominate neighbor selection. This can produce volatile local predictions that vary drastically with small changes in input.

---

### 6. **Question:**
You suspect your **logistic regression** model suffers from high variance. Which subtle symptom below might confirm this suspicion?

- A) The model underfits consistently on both training and validation sets with large residual errors.
- B) Training performance is excellent, but the model generalizes poorly on new data (sharp drop in test accuracy).
- C) Logistic regression models can never overfit due to the sigmoid function bounding outputs.
- D) The decision boundary remains fixed, regardless of changing the dataset or regularization.

<br>
<br>
<br>
<br>
<br>
<br>

**Correct Answer:**  B) Training performance is excellent, but the model generalizes poorly on new data (sharp drop in test accuracy)  
**Explanation:**  
High variance is characterized by overfitting. When a model fits the training data extremely well but fails on unseen data, it’s a classic indicator of high variance.

---

### 7. **Question:**
In **gradient descent** with momentum, you notice your updates overshoot local minima more often than plain gradient descent. What nuanced factor might be responsible?

- A) Momentum accumulates past gradients, potentially boosting step sizes in certain directions, leading to overshooting if the momentum parameter is large.
- B) Momentum strictly reduces step sizes in convex regions, so overshooting is impossible.
- C) Momentum re-initializes the gradient for each batch, causing random fluctuations.
- D) Momentum always guarantees faster convergence to a global minimum without overshoot.

<br>
<br>
<br>
<br>
<br>
<br>

**Correct Answer:**  A) Momentum accumulates past gradients, potentially boosting step sizes in certain directions, leading to overshooting if the momentum parameter is large  
**Explanation:**  
While momentum typically helps escape small local minima and speeds convergence, if the momentum coefficient is too high, it can push updates beyond the optimal region, overshooting repeatedly.

---

### 8. **Question:**
In a **KNN** classifier, you notice the bias-variance trade-off changes differently than anticipated when you tweak $k$. Which subtle phenomenon might account for unexpected results?

- A) Increasing $k$ always results in a monotonic decrease in variance and no change in bias.
- B) Local data density anomalies or unscaled feature spaces can distort neighbor relationships, leading to unpredictable bias-variance shifts.
- C) The presence of nominal features automatically fixes the bias-variance dilemma in KNN.
- D) KNN does not have a bias-variance trade-off since it’s a purely instance-based method.

<br>
<br>
<br>
<br>
<br>
<br>

**Correct Answer:**  B) Local data density anomalies or unscaled feature spaces can distort neighbor relationships, leading to unpredictable bias-variance shifts  
**Explanation:**  
Although theoretical expectations suggest a clear pattern in bias-variance for KNN as $k$ changes, real-world data issues like unscaled features or irregular local densities can produce irregular or unexpected outcomes.

---

### 9. **Question:**
You’re analyzing a model’s performance using **bias/variance decomposition**. Which subtle misunderstanding can arise when concluding a model has “low bias, high variance”?

- A) It implies the model is guaranteed to underfit on training data.
- B) It suggests the model consistently predicts values higher than the true mean.
- C) One might incorrectly assume adding more features always reduces variance.
- D) One might overlook that high variance primarily manifests as large fluctuations across different training sets, not necessarily poor training error.

<br>
<br>
<br>
<br>
<br>
<br>

**Correct Answer:**  D) One might overlook that high variance primarily manifests as large fluctuations across different training sets, not necessarily poor training error  
**Explanation:**  
High variance means the model’s predictions change significantly when trained on different subsets. The model can still fit the training data well (low training error) but fail to generalize consistently.

---

### 10. **Question:**
A **logistic regression** model is optimized with batch gradient descent but converges very slowly. Which subtle reason might best explain this?

- A) Logistic regression’s cost function is non-convex, ensuring slow convergence by definition.
- B) A poorly chosen (too small) learning rate can make updates tiny, dragging out the training process.
- C) Batch gradient descent updates the weights after every single training example, preventing larger, more stable steps.
- D) Logistic regression always converges quickly due to the closed-form solution for weights.

<br>
<br>
<br>
<br>
<br>
<br>

**Correct Answer:**  B) A poorly chosen (too small) learning rate can make updates tiny, dragging out the training process  
**Explanation:**  
Logistic regression has a convex cost function, so it can converge efficiently with an appropriate learning rate. However, if the learning rate is too small, each update is minuscule, prolonging convergence.

### 11. **Question:**
In the context of **KNN**, if you use a complex distance metric that heavily weights certain feature interactions, which subtle outcome might you observe?

- A) The distance metric becomes feature-agnostic, making it easier to interpret predictions.
- B) KNN performance improves drastically for all datasets, suggesting no bias or variance issues.
- C) Overemphasis on specific feature interactions can cause erratic neighbor selections, increasing variance in predictions.
- D) KNN automatically switches to a linear decision boundary under complex distance metrics.

<br>
<br>
<br>
<br>
<br>
<br>

**Correct Answer:**  C) Overemphasis on specific feature interactions can cause erratic neighbor selections, increasing variance in predictions  
**Explanation:**  
Complex or uncalibrated distance metrics may disproportionately favor certain features or interactions, leading to inconsistent local neighborhoods. This can make the model’s behavior more sensitive to fluctuations in the data, thereby increasing variance.

---

### 12. **Question:**
A **logistic regression** model is trained on highly skewed data with a strong class imbalance. After carefully tuning the decision threshold, you find improved recall but a drop in precision. Which subtle trade-off might be at play?

- A) You’ve eliminated bias by adjusting the threshold, so any drop in precision must be a data artifact.
- B) The threshold shift is enforcing a more complex decision boundary, increasing bias and decreasing variance.
- C) By lowering the threshold to capture more positives, you likely introduced more false positives, hence lowering precision.
- D) Logistic regression with skewed data only influences the intercept, not the slope of the decision boundary.

<br>
<br>
<br>
<br>
<br>
<br>

**Correct Answer:**  C) By lowering the threshold to capture more positives, you likely introduced more false positives, hence lowering precision  
**Explanation:**  
Adjusting the decision threshold to identify more true positives (improving recall) often brings more false positives, reducing precision. This trade-off is a hallmark in imbalanced classification scenarios.

---

### 13. **Question:**
You suspect **high variance** in your linear model’s predictions. Which subtle diagnostic can confirm this suspicion?

- A) A near-perfect fit on the training set but unstable performance across multiple cross-validation folds.
- B) Minimal errors on both training and validation sets, indicating possible underfitting.
- C) Identical predictions for different subsets of training data, suggesting stable generalization.
- D) The model fails to converge on training data, showing no pattern of overfitting or underfitting.

<br>
<br>
<br>
<br>
<br>
<br>

**Correct Answer:**  A) A near-perfect fit on the training set but unstable performance across multiple cross-validation folds  
**Explanation:**  
When variance is high, the model fits the training data closely but exhibits wide fluctuations in performance across different validation splits. This is because the model is overly sensitive to the particular training examples provided.

---

### 14. **Question:**
During **batch gradient descent**, you notice that including a momentum term sometimes slows down the early stages of convergence. Which nuanced explanation might justify this behavior?

- A) Momentum only accelerates convergence and never slows it, so this must be an implementation bug.
- B) The initial updates with momentum can accumulate “inertia” in directions not aligned with the final descent path, causing a lag before reorienting.
- C) Momentum cancels out all gradient effects, forcing the model to rely on random fluctuations.
- D) A high learning rate combined with momentum ensures immediate convergence in the first few epochs.

<br>
<br>
<br>
<br>
<br>
<br>

**Correct Answer:**  B) The initial updates with momentum can accumulate “inertia” in directions not aligned with the final descent path, causing a lag before reorienting  
**Explanation:**  
Momentum sums past gradient updates, which might initially point in suboptimal directions. The optimizer can appear slow at first as it works to redirect that accumulated momentum toward the true gradient descent path.

---

### 15. **Question:**
A **KNN** classifier with $k = 1$ achieves perfect accuracy on the training set but generalizes poorly. From a bias-variance perspective, which subtle factor might explain this?

- A) $k = 1$ inherently reduces variance by looking at fewer neighbors.
- B) Using a single neighbor leads to high variance, as the decision boundary is shaped by each individual training point.
- C) The model’s bias is too high, causing it to underfit severely.
- D) The model is in perfect balance since it has zero training error.

<br>
<br>
<br>
<br>
<br>
<br>

**Correct Answer:**  B) Using a single neighbor leads to high variance, as the decision boundary is shaped by each individual training point  
**Explanation:**  
A 1-NN classifier often overfits because each training example becomes its own region of influence. This makes the model highly sensitive (high variance) to small perturbations in the training data and does not generalize well.

# Different Hyper-parameters in Decision Trees

### **1. `criterion`**
- **Meaning**: This defines the function to measure the quality of a split at each node in the tree. The two main options are:
  - `'gini'`: Gini impurity (default).
  - `'entropy'`: Information gain or entropy.

- **Example**:
  ```python
  DecisionTreeClassifier(criterion='entropy')
  ```
  - **Gini** works slightly faster, but **entropy** may provide better results for certain datasets, though it’s computationally heavier.

### **2. `splitter`**
- **Meaning**: This defines the strategy used to split at each node:
  - `'best'`: Chooses the best split.
  - `'random'`: Chooses the best random split.

- **Example**:
  ```python
  DecisionTreeClassifier(splitter='random')
  ```
  - **Random** splitting adds stochasticity to the tree and can be helpful for large datasets to speed up computation, though it can lead to suboptimal trees.

  When splitter='random' is used, the algorithm doesn’t evaluate all features at each node. Instead, it:
Randomly selects a subset of features at each node.
Evaluates possible splits only within the selected subset of features.
Chooses the best split among those random splits.

### **3. `max_depth`**
- **Meaning**: The maximum depth of the tree. Controls overfitting and underfitting:
  - `None`: No limit, the tree grows until all leaves are pure or contain fewer than `min_samples_split` samples.
  
- **Example**:
  ```python
  DecisionTreeClassifier(max_depth=5)
  ```
  - **Deeper trees** can overfit (high variance), while **shallow trees** (low `max_depth`) may underfit (high bias).

### **4. `min_samples_split`**
- **Meaning**: The minimum number of samples required to split an internal node. This prevents the tree from creating splits that are too specific.
  - Can be an integer or a float representing a fraction of the number of samples.

- **Example**:
  ```python
  DecisionTreeClassifier(min_samples_split=10)
  ```
  - Higher values (e.g., 10) restrict tree growth, **reducing overfitting** by preventing very small splits.

### **5. `min_samples_leaf`**
- **Meaning**: The minimum number of samples that must be present in a leaf node. Controls the size of leaf nodes.
  - Can be an integer or a float representing a fraction of the total number of samples.

- **Example**:
  ```python
  DecisionTreeClassifier(min_samples_leaf=5)
  ```
  - Larger values prevent leaves from being too small, **smoothing the model** and reducing overfitting.

### **6. `min_weight_fraction_leaf`**
- **Meaning**: Similar to `min_samples_leaf`, but defined as the fraction of the total sample weight needed to be in a leaf.
  
- **Example**:
  ```python
  DecisionTreeClassifier(min_weight_fraction_leaf=0.05)
  ```
  - Helps in dealing with sample weights and datasets with unequal sample distribution.

  - What is "Total Sample Weight"?
    In the context of decision trees, "total sample weight" refers to the sum of the weights of the samples (data points) that are used to train the model.

    By default, in most machine learning tasks, all samples (data points) have equal weight, meaning each sample contributes equally to the training process. In such cases, the total sample weight is simply the total number of samples in the dataset.

    However, if you are working with weighted samples, where certain samples are given more importance than others (through a weighting mechanism), the total sample weight becomes the sum of the weights of all samples, rather than just the count of the samples.

    How is "Total Sample Weight" Used in Decision Trees?
    In decision trees, the min_weight_fraction_leaf parameter controls how the splits are made by ensuring that each leaf node contains at least a certain fraction of the total sample weight.

    By default (if all samples have equal weight), the total sample weight is simply the number of samples in the dataset. For example, if you have 100 samples, the total sample weight is 100.

    When weights are applied to some samples (e.g., because of class imbalance or when using the sample_weight argument), the total sample weight becomes the sum of all the sample weights. For example, if some samples are given more weight (say, 1.5 for some and 1 for others), the total weight could be higher than just the number of samples.

    Example of Total Sample Weight:
    Equal Weights (No Sample Weighting):

    You have 100 samples in your dataset. All samples have a default weight of 1.
    Total sample weight = 100 (since every sample has equal weight of 1).
    Custom Sample Weights:

    If certain samples are more important and are assigned a weight, for example, 1.5, and others retain the default weight of 1, the total sample weight becomes the sum of these weights.

    If 50 samples have weight 1.5 and 50 have weight 1:

    Total sample weight =
    (
    50
    ×
    1.5
    )
    +
    (
    50
    ×
    1
    )
    =
    75
    +
    50
    =
    125
    (50×1.5)+(50×1)=75+50=125.

### **7. `max_features`**
- **Meaning**: The number of features to consider when looking for the best split.
  - `None`: All features are considered.
  - Integer: Consider a fixed number of features.
  - Float: Consider a percentage of features.
  - `'sqrt'` or `'log2'`: Consider the square root or logarithm of the total number of features.

- **Example**:
  ```python
  DecisionTreeClassifier(max_features='sqrt')
  ```
  - Reducing `max_features` can **increase bias** but **reduce variance**, often improving generalization, especially for large datasets.

### **8. `random_state`**
- **Meaning**: Controls the randomness of the estimator, especially for splitting (`splitter='random'`) and for randomizing `max_features`.
  - `None`: No specific seed, meaning results can vary each time.
  - Integer: A fixed seed for reproducibility.

- **Example**:
  ```python
  DecisionTreeClassifier(random_state=42)
  ```
  - Setting `random_state` makes results reproducible.

### **9. `max_leaf_nodes`**
- **Meaning**: Limits the number of leaf nodes in the tree.
  - `None`: No limit on the number of leaf nodes.

- **Example**:
  ```python
  DecisionTreeClassifier(max_leaf_nodes=10)
  ```
  - Reducing the number of leaf nodes simplifies the model and prevents overfitting by forcing the tree to stop splitting once the leaf node limit is reached.

### **10. `min_impurity_decrease`**
- **Meaning**: A node will split only if the impurity decrease (based on `criterion`) is greater than this threshold.
  
- **Example**:
  ```python
  DecisionTreeClassifier(min_impurity_decrease=0.01)
  ```
  - Prevents unnecessary splits by requiring a minimum improvement in impurity, further **regularizing the tree**.

### **11. `class_weight`**
- **Meaning**: Weights associated with classes to handle class imbalance. Can be:
  - `None`: All classes are equally weighted.
  - `'balanced'`: Adjusts weights inversely proportional to class frequencies.

- **Example**:
  ```python
  DecisionTreeClassifier(class_weight='balanced')
  ```
  - Useful for **handling imbalanced datasets** where certain classes have much fewer samples than others.

### **12. `ccp_alpha` (Cost Complexity Pruning)**
- **Meaning**: Controls **pruning** of the tree. A positive value for `ccp_alpha` will prune the tree by removing nodes with low impurity improvement.
  
- **Example**:
  ```python
  DecisionTreeClassifier(ccp_alpha=0.01)
  ```
  - Pruning can **simplify the tree** and prevent overfitting by removing unnecessary splits. Higher values of `ccp_alpha` lead to more aggressive pruning.


---

### **Examples of Tuning and the Impact**

Let’s demonstrate how tuning these parameters can impact a model’s performance.

#### **Example 1: Overfitting with No Constraints**
```python
# Unconstrained decision tree
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
y_pred = dt.predict(X_test)
```
- **Effect**: With no constraints (default settings), the tree can grow deeply, fitting very closely to the training data, possibly **overfitting**.

#### **Example 2: Controlling Tree Depth**
```python
# Limiting max depth to prevent overfitting
dt = DecisionTreeClassifier(max_depth=5)
dt.fit(X_train, y_train)
y_pred = dt.predict(X_test)
```
- **Effect**: Limiting `max_depth` to 5 prevents the tree from growing too deep, reducing **overfitting**. However, if set too low, the tree may **underfit**.

#### **Example 3: Handling Class Imbalance**
```python
# Using class weights for imbalanced data
dt = DecisionTreeClassifier(class_weight='balanced')
dt.fit(X_train, y_train)
y_pred = dt.predict(X_test)
```
- **Effect**: Assigning **balanced class weights** helps the tree give more importance to the minority class, improving performance on imbalanced datasets.

#### **Example 4: Regularizing with Minimum Samples per Leaf**
```python
# Prevent small leaves with min_samples_leaf
dt = DecisionTreeClassifier(min_samples_leaf=10)
dt.fit(X_train, y_train)
y_pred = dt.predict(X_test)
```
- **Effect**: Setting `min_samples_leaf=10` prevents very small and highly specific leaf nodes from being created, **regularizing** the tree.

---

# Rudimentary/Simple way of Hyper-Param Tuning
"""

import pandas as pd
import numpy as np

!gdown 19L3rYatfhbBL1r5MHrv-p_oM2wlvrhqk
!gdown 1OHLKJwA3qZopKPvlKoRldM6BvA1A4dYF
!gdown 1N7O_fWCTJLu8SIa_paKcDEzllgpMk8sK
!gdown 12Bh2AN8LcZAlg20ehpQrEWccUDaSdsOG

X_train = pd.read_pickle('preprocessed_X_sm.pickle')
X_test = pd.read_pickle('X_test.pickle')
y_train = pd.read_pickle('y_sm.pickle')
y_test = pd.read_pickle('y_test.pickle')

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import KFold, cross_validate, cross_val_score

kfold = KFold(n_splits=10)

depths = [3,4,5,6, 7,9,11,13,15]

for depth in depths:
    tree_clf = DecisionTreeClassifier(random_state=7, max_depth=depth)

    cv_acc_results = cross_validate(tree_clf, X_train, y_train, cv = kfold, scoring = 'accuracy', return_train_score = True)

    print(f"K-Fold for depth:{depth} Accuracy Mean: Train: {cv_acc_results['train_score'].mean()*100} Validation: {cv_acc_results['test_score'].mean()*100}")
    print(f"K-Fold for depth: {depth} Accuracy Std: Train: {cv_acc_results['train_score'].std()*100} Validation: {cv_acc_results['test_score'].std()*100}")
    print('***************')

tree_clf = DecisionTreeClassifier(random_state=7, max_depth=4)
tree_clf=tree_clf.fit(X_train, y_train)
pred = tree_clf.predict(X_test)

from IPython.display import Image
from six import StringIO
from sklearn.tree import export_graphviz
import pydot

features = list(X_train.columns)

# Ensure the target variable 'Attrition' is not there in the feature list

dot_data = StringIO()
export_graphviz(tree_clf, out_file=dot_data, feature_names=features, filled=True)
graph = pydot.graph_from_dot_data(dot_data.getvalue())
Image(graph[0].create_png())

import matplotlib.pyplot as plt

importances = tree_clf.feature_importances_
indices = np.argsort(importances)[::-1] # Sort feature importances in descending order
names = [X_train.columns[i] for i in indices] # Rearrange feature names so they match the sorted feature importances

plt.figure(figsize=(15, 7)) # Create plot
plt.title("Feature Importance") # Create plot title
plt.bar(range(X_train.shape[1]), importances[indices]) # Add bars
plt.xticks(range(X_train.shape[1]), names, rotation=90) # Add feature names as x-axis labels
plt.show() # Show plot





"""# Hyper-parameter tuning for Sklearn Models (or any model in general with fit/predict api)

We'll use the **Iris dataset** from `sklearn` for this demonstration. The Iris dataset is a popular dataset used for classification tasks and is included in the `sklearn.datasets` module.

We'll perform the following steps:
1. Load the Iris dataset.
2. Train a **Decision Tree** and **Logistic Regression** model.
3. Implement **Grid Search** and **Random Search** for hyperparameter tuning using `GridSearchCV` and `RandomizedSearchCV` from `sklearn.model_selection`.

### Step-by-Step Implementation

#### **1. Import the Required Libraries**
"""

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from scipy.stats import randint

"""
#### **2. Load the Dataset**
"""

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Target (class labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""#### **3. Grid Search for Decision Tree**
Grid search exhaustively searches over a grid of hyperparameters to find the best combination.
"""

# Define the hyperparameters grid for Decision Tree
param_grid_dt = {
    'criterion': ['gini', 'entropy'],  # Criteria for splitting
    'max_depth': [None, 5, 10, 15],    # Maximum depth of the tree
    'min_samples_split': [2, 10, 20],  # Minimum number of samples required to split a node
    'min_samples_leaf': [1, 5, 10],    # Minimum number of samples required to be at a leaf node
}

# Initialize the Decision Tree classifier
dt_classifier = DecisionTreeClassifier(random_state=42)

# Set up GridSearchCV
grid_search_dt = GridSearchCV(estimator=dt_classifier, param_grid=param_grid_dt, cv=5, n_jobs=-1, verbose=1)

# Fit GridSearchCV to the data
grid_search_dt.fit(X_train, y_train)

# Best parameters from Grid Search
print(f"Best parameters for Decision Tree: {grid_search_dt.best_params_}")

# Predict using the best estimator from GridSearchCV
y_pred_dt = grid_search_dt.best_estimator_.predict(X_test)

# Accuracy score for Decision Tree
accuracy_dt = accuracy_score(y_test, y_pred_dt)
print(f"Decision Tree Accuracy with Grid Search: {accuracy_dt:.4f}")

"""
#### **4. Random Search for Decision Tree**
Random search samples hyperparameters from the distribution you specify for a fixed number of iterations.
"""

# Define the hyperparameter distributions for Random Search
param_dist_dt = {
    'criterion': ['gini', 'entropy'],
    'max_depth': randint(1, 20),         # Randomly search for max_depth between 1 and 20
    'min_samples_split': randint(2, 20), # Randomly search for min_samples_split between 2 and 20
    'min_samples_leaf': randint(1, 10),  # Randomly search for min_samples_leaf between 1 and 10
}

# Set up RandomizedSearchCV
random_search_dt = RandomizedSearchCV(estimator=dt_classifier, param_distributions=param_dist_dt,
                                      n_iter=20, cv=5, n_jobs=-1, random_state=42, verbose=1)

# Fit RandomizedSearchCV to the data
random_search_dt.fit(X_train, y_train)

# Best parameters from Random Search
print(f"Best parameters for Decision Tree (Random Search): {random_search_dt.best_params_}")

# Predict using the best estimator from RandomizedSearchCV
y_pred_dt_random = random_search_dt.best_estimator_.predict(X_test)

# Accuracy score for Decision Tree (Random Search)
accuracy_dt_random = accuracy_score(y_test, y_pred_dt_random)
print(f"Decision Tree Accuracy with Random Search: {accuracy_dt_random:.4f}")

"""#### **5. Grid Search for Logistic Regression**"""

# Define the hyperparameters grid for Logistic Regression
param_grid_lr = {
    'penalty': ['l1', 'l2'], # Regularization type
    'C': [0.01, 0.1, 1, 10, 100],                # Inverse of regularization strength            # Solver for optimization
    'max_iter': [100, 200, 500]                   # Maximum number of iterations
}

# Initialize the Logistic Regression classifier
lr_classifier = LogisticRegression(random_state=42, solver='liblinear')

# Set up GridSearchCV
grid_search_lr = GridSearchCV(estimator=lr_classifier, param_grid=param_grid_lr, cv=5, n_jobs=-1, verbose=1)

# Fit GridSearchCV to the data
grid_search_lr.fit(X_train, y_train)

# Best parameters from Grid Search
print(f"Best parameters for Logistic Regression: {grid_search_lr.best_params_}")

# Predict using the best estimator from GridSearchCV
y_pred_lr = grid_search_lr.best_estimator_.predict(X_test)

# Accuracy score for Logistic Regression
accuracy_lr = accuracy_score(y_test, y_pred_lr)
print(f"Logistic Regression Accuracy with Grid Search: {accuracy_lr:.4f}")

"""#### **6. Random Search for Logistic Regression**

"""

# Define the hyperparameter distributions for Random Search
param_dist_lr = {
    'penalty': ['l1', 'l2'], # Regularization type
    'C': [0.01, 0.1, 1, 10, 100],                # Inverse of regularization strength            # Solver for optimization
    'max_iter': [100, 200, 500]    # Random number of iterations between 100 and 500
}

# Set up RandomizedSearchCV
random_search_lr = RandomizedSearchCV(estimator=lr_classifier, param_distributions=param_dist_lr,
                                      n_iter=20, cv=5, n_jobs=-1, random_state=42, verbose=1)

# Fit RandomizedSearchCV to the data
random_search_lr.fit(X_train, y_train)

# Best parameters from Random Search
print(f"Best parameters for Logistic Regression (Random Search): {random_search_lr.best_params_}")

# Predict using the best estimator from RandomizedSearchCV
y_pred_lr_random = random_search_lr.best_estimator_.predict(X_test)

# Accuracy score for Logistic Regression (Random Search)
accuracy_lr_random = accuracy_score(y_test, y_pred_lr_random)
print(f"Logistic Regression Accuracy with Random Search: {accuracy_lr_random:.4f}")

"""### **Explanation:**

1. **Grid Search**: Tries all possible combinations of the hyperparameters provided in `param_grid`. It's exhaustive but can be time-consuming for large grids.
   
2. **Random Search**: Randomly samples combinations of hyperparameters from the distributions provided in `param_dist`. It's faster than grid search, especially when the search space is large.

3. **Evaluation**: After training using `GridSearchCV` and `RandomizedSearchCV`, we print the best hyperparameters found and evaluate the model's performance on the test set using accuracy.

4. **`GridSearchCV` and `RandomizedSearchCV`**: Both these methods take a cross-validation (CV) approach to assess the performance of each hyperparameter configuration, ensuring the model generalizes well.

### **Results**:
The output of this code will include the best hyperparameters for each model and the corresponding accuracy scores. You can experiment with different datasets and hyperparameter ranges to see how the models perform in various scenarios.

## Decision Trees Implementation
"""

