# -*- coding: utf-8 -*-
"""3-Notes-DecisionTree-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12QL5x_yIBFqdvxr7hy8CU3H9CCNHDHbE

## Content

- **DT intuition**


- **How to split the nodes?**


- **Entropy**


- **Building a DT intuition**


- **Visualizing the process of building DT**

- **Scratch impl of DT (optional) - Post read**

- **Sklearn implementation**

## UseCase Intro: Employee Attrition
### You are a Data Scientist working at a Jio

- The company is facing a huge problem of employee attrition
- Your task is to help the company find a solution to this problem.

#### Why is attrition a problem?

  - A new employee asks for more compensation
  - Training of new employees
  - Lots of time and resources required for searching a new candidate

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/781/original/Screenshot_2023-07-28_at_11.12.39_AM.png?1690523151 width=700>

#### What can be done to solve the problem ?

1. Identify the employees who may leave in future.
  - Targeted approaches can be undertaken to retain such employees.
  - These might include addressing their problems with the company and so on ...

2. Help identify the key indicators/factors leading to an employee leaving.
  - #### What all reasons can you think of contributing to attrition ?
    - Forcing employees to come to office daily
    - Unhealthy culture etc
  - Identifying these key factors helps in taking better measures to improve employee retention

#### Dataset
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import io

!gdown 16KtxSt_QEGQvfluEaMls5cCHPwhRXgCk

df = pd.read_csv("HR-Employee-Attrition.csv")
df.info()

df.head()

"""#### Summary of EDA and Preprocessing

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/782/original/Screenshot_2023-07-28_at_11.12.47_AM.png?1690523216 width=700>

We perform EDA followed by preprocessing on the data which is covered in the post read

#### Post Read - Employee Attrition Usecase

Employee EDA: https://colab.research.google.com/drive/1OdxmAv5q-92ll5Jf8XrDYQp-4-mitgC8?usp=sharing

#### Final dataset after preprocessing
"""

!gdown 19L3rYatfhbBL1r5MHrv-p_oM2wlvrhqk
!gdown 1OHLKJwA3qZopKPvlKoRldM6BvA1A4dYF
!gdown 1N7O_fWCTJLu8SIa_paKcDEzllgpMk8sK
!gdown 12Bh2AN8LcZAlg20ehpQrEWccUDaSdsOG

import pickle
# Load data (deserialize)
with open('preprocessed_X_sm.pickle', 'rb') as handle:
    X_train = pickle.load(handle)

with open('X_test.pickle', 'rb') as handle:
    X_test = pickle.load(handle)

with open('y_sm.pickle', 'rb') as handle:
    y_train = pickle.load(handle)

with open('y_test.pickle', 'rb') as handle:
    y_test = pickle.load(handle)

# train data shape
X_train.shape

# test data shape
X_test.shape

X_train.head()

"""## DT intuition

Say, we have attrition data with 2 features
- Age
- Overtime

When we plot the data, it looks something like this :

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/783/original/Screenshot_2023-07-28_at_11.12.53_AM.png?1690523246 width=700>

#### Can we use logistic regression to classify this data ?

**No** as it is a **linear model** and we have **non linear data** with us.

- Logistic regression will not perform well in this case.

#### Can we use KNN to solve this problem ?

**Yes** as it works on assumptions that
- neighbourhood should be homogenous
- i.e. datapoints belonging to same class are close to each other.

This assumption is valid here.

#### But, KNN has a big disadvantage

- We **can't productionize** KNN for large datasets

**Why?**

Because entire training data is used every time we make an inference.

So, it will be difficult to use KNN in this case.

Let's try something else.


Let's try to **split** the data into regions using **axis parallel hyperplanes**

The splitted region  along with decision boundaries will look like :

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/784/original/Screenshot_2023-07-28_at_11.13.00_AM.png?1690523302 width=700>

With the help of these hyperplanes,
- we can represent these **homogenous splitted region** using **if else conditions**

Let's see how it looks like :

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/785/original/Screenshot_2023-07-28_at_11.13.06_AM.png?1690523371 width=700>

Now, that we have the conditions with us

Let's try to represent these conditions digramatically:

Notice that
- This is resembling a **tree structure**

where
- **top most node** is called **root node**
- **Bottom most node** is called **leaf node**
- and **the nodes in between** them are called **Internal node**


This is called a **Decision Tree**

#### Visualization

Here's an visualization to help you understand how we use these axis parallel hyperplane to split the data.

https://mlu-explain.github.io/decision-tree/

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/084/257/original/download.gif?1722512151' width=800>

Notice that
- how easily it is able to classify multiclass data

- which might have been an overhead if we decided to use logistic regression

#### (Optional) But, Will DT only work when decision boundaries are axis parallel ?

Say, we have the following dataset :

Rememeber that, in DT
- **each individual hyperplane is axis parallel.**
- the **final decision boundary** which is **made out of combining these hyperplane need not be axis parallel**

Let's see how DT will handle this case:

It will make multiple axis parallel hyperplane to split the region in homogenous subspace.


But, if you were see its effective decision boundary , it'll come out to be :

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/787/original/Screenshot_2023-07-28_at_11.13.19_AM.png?1690524717 width=700>

Whenever we get a datapoint

- Based on the set of rules/ condition
    - we classify it into one of the classes.

**Logically**, it is combination of **nested if else conditions**

**Geomtrically**, it is combination of **axis parallel hyperplanes**.
- and this combination of axis parallel hyperplane helps us in classifying non linear data

#### One of the main advantage of Decision Trees are
  
DTs are **easily interpretable**

**How so?**


  - Let's take an example:
    - One conclusion that can be derived from above decision tree is
      - Employees younger than 29 yrs and work overtime >= 2.5hrs are more likely to churn
    - This can help the company take specific decisions in this direction to reduce attrition such as:
      - Incentivise overtime
      - Reducing workload

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/786/original/Screenshot_2023-07-28_at_11.13.12_AM.png?1690524563 width=700>

If you notice that,
- while building the hyperplanes here
- we were **trying to split** data **into** **homogenous region/ subspace** containing only single class datapoints

#### What do we need to create these homogenous subspace ?

We need 2 things
- Feature
- Condition to split the data.


We have the features with us.
- We just need to figure out how to find these split conditions.


This is what **learning/ training** means in **decision tree**

Let's learn how to split the data into homogenous regions

# Points to Remember

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/788/original/Screenshot_2023-07-28_at_11.13.28_AM.png?1690524797 width=700>

## How to split the nodes ?

#### But, how do we find these rules ?

- Obviously, we cannot manually create rules for the entire data
- So, we will have to **learn these rules from the data**

<br>

#### What is the end goal of classification ?

Our purpose is to achieve max possible predicition confidence and accuracy

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/789/original/Screenshot_2023-07-28_at_11.13.34_AM.png?1690524836 width=700>

<br>

#### How do decision trees improve predicition confidence ?
  - By dividing heterogenous subspaces into smaller subspaces

We want
  - A homogenous subspace i.e. which contains data belonging to only one class
  
   **Why ?**
    - Confidence of predicition will be maximum in this case

Say, we have a node with following distribution

And we are given with two options to split this node.

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/790/original/Screenshot_2023-07-28_at_11.13.41_AM.png?1690524942 width=700>

#### But, why are we looking for pure nodes ?

In order to understand that,

Let's see **how DT assign class label at prediction time**.


During prediction time,
- it predicts the label as the one of majority class at that node.

Let's see with an example:

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/084/258/original/i.png?1722512193' width=800>

As we are taking majority vote
- **pure node** will have **more confidence** in the prediction

If we were to pick **option 1**
- We have **purer node** and more homogenous data
- It'll give us **more confidence** in our prediction


<br>

If we were to pick option 2
- There is **less condifence** in the prediction
- i.e. we are 60% confident in our prediction (left child node) that our datapoint belong to positive class
- Hence, node is **impure** and more hetrogenous data


**So, we'll pick option 1 for split.**

#### But, what does confidence here means ?

**Think of condifence as class probability.**

For example:
- In left node,
    - we have datapoints as follows:
        
            +ve class : 10
            -ve class : 90


So, we can say that
- if a datapoint belong to left node
- there is a 90% probability it belongs to -ve class

or class probability is 0.9.

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/084/259/original/i2.png?1722512221' width=800>

**Conclusion**:
- **Purer** the nodes, more **homogenous** the data is.
- **More homogenous** the data in the node is, **more confident** we are about our prediction belonging to specific class.
- **purer** the node, **more the probability** that datapoints belongs to particular class.

So, our **objective** is:
- To **split a node** in such a way
    - that we **get purer nodes**


#### But, how do we quantify the purity of the nodes ?

#### How do we calculate purity mathematically ?

We do so using **Entropy**

Let's understand what entropy is

## Entropy

Entropy is used to **measure the impurity** not purity
- i.e. it measures the hetrogenity of the node.


Fun fact:
- The concept of entropy comes from basic probability/ information theory
- where it is used to measure the randomness.

So,

More the hetrogenity in the node,
- larger the value of entropy will be and vice versa.

Since we want the **nodes** to be **pure**,
- we want **entropy** as **low** as possible.

Let's look into the fomulation of Entropy

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/791/original/Screenshot_2023-07-28_at_11.13.48_AM.png?1690524980 width=700>

### Entropy formulation

Say, Y be a **discrete random variable**.

- it can take **k discrete values** i.e y âˆˆ {$y_1, y_2, y_3, ..., y_k$}


#### How many discrete values of y do we have in employee attrition problem ?

Since, there are **two classes** (churn or stay),
- we have two discrete values for y in this case.

#### What will be the entropy for our binary case classification problem ?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/792/original/Screenshot_2023-07-28_at_11.13.54_AM.png?1690525078 width=700>

Let's try to understand entropy for binary class case using an example

Say, we have 4 jars.
- Each jar is filled with balls

Balls are of two colors
- Red ball
- Blue ball

You have been asked to pick a ball from these 4 jars


#### Which jars will give you highest confidence on whether the ball you have picked is red or not ?

Jar 3 and 4 will give us highest confidence.
- As there jars are pure.
- So, we are pretty sure
    - We won't get a red ball in Jar 3
    - And we'll definately get red ball in Jar 4.

- In Jar 1,
    - which is impure (as it has both red and blue balls)
    - we won't have much confidence on whether the ball being picked is red or not.

Let's represent these confidence in numerical terms. i.e. **calculate entropy of each jar**.

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/793/original/Screenshot_2023-07-28_at_11.14.01_AM.png?1690525135 width=700>

Notice that
- Jar with equal number of red and blue balls have highest entropy i.e. 1
- Jar with only red balls or blue balls has lowest entropy i.e. 0.

#### Plotting entropy

Let's plot entropy for binary system and see how it looks like:

Desmos plot: https://www.desmos.com/calculator/avaplvktso
"""

from IPython.display import IFrame

IFrame(src="https://www.desmos.com/calculator/avaplvktso", width=700, height=375)

"""<img src= https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/794/original/Screenshot_2023-07-28_at_11.14.07_AM.png?1690525169 width=700>

Notice that
- Value of entropy is maximum
    - when the probability is 0.5 i.e. equal number of datapoints for each class
- Entropy value is minimum (Entropy = 0 )
    - when the probability P(Y = 1) is either 0 or 1 (pure node)

#### Visualization for entropy

In case you want to see how entropy changes as number of datapoint changes, here's a visualization for that.

https://mlu-explain.github.io/decision-tree/


<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/084/260/original/i3.png?1722512255' width=800>

**Conclusion:**
- Entropy is **maximum** when the node is **impure** (P(Y =1) = 0.5) i.e. 1
- Entropy is  **minimum** when the node is **pure** i.e. 0
- Entropy lies between 0 and 1.

Now, that we have learnt about purity and how to use entropy to calculate that.

Let's see how Decision Trees uses it for internal working

# Points to Remember

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/795/original/Screenshot_2023-07-28_at_11.14.13_AM.png?1690525211 width=700>

## How Decision Tree works? Building a DT intuition

Let's understand it using a dummy example.


Consider a dataset with 100 datapoints
- and 2 features (Gender , Age < 35 )

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/796/original/Screenshot_2023-07-28_at_11.14.20_AM.png?1690525257 width=700>

#### Can we use this root node for predicition ?

No. We can't. It is highly hetrogenous

Hence, it'll have a high entropy.

Let's calculate its entropy

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/797/original/Screenshot_2023-07-28_at_11.14.27_AM.png?1690525296 width=700>

So, we should split the node in order to reduce the entropy.
- and make it homogenous.

We have two features with us. i.e. Gender and Age < 35.

#### Which feature shall we use for splitting ?

We don't know until we calculate the entropy of split for the feature.

#### Splitting using Gender feature and age<35

Let's first split using Gender feature


When we split using gender
- we get two child nodes
- one for Male
- other for Female.

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/798/original/Screenshot_2023-07-28_at_11.14.34_AM.png?1690525348 width =700>

Let's calculate the entropy of child to check whether the entropy has reduced or not

Now that we have calculated entropy of parent as well as both child
- we need to check if there is a reduction in entropy.

But, each child node has an entropy value.
- In order to see the reduction in entropy,
    - we first need to combine the child entropy to get a single value out of it.
    - then we can compare it with parent entropy.

#### How do we combine child entropy ?

**Simple average?**

- When we take simple average,
    - we are ignoring the proportions of datapoints belonging to each node

There can be a case where
- Child 1 contains 98 datapoints
- Child 2 contains 2 datapoints

We should include the number of datapoints it is impacting while calculating combined entropy.


So, we should take **weighted average** in this case.

#### How do we calculate weighted entropy of child nodes ?

We simply do so by multiplying the datapoint proportion with its entropy value

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/799/original/Screenshot_2023-07-28_at_11.14.40_AM.png?1690525396 width=700>

There is a slight reduction in weighted child entropy (0.88) compared to parent entropy (0.97)

- So, we are moving towards purer nodes.

This **reduction in entropy** i.e. Parent - weight entropy of child is termed as **Information gain**

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/800/original/Screenshot_2023-07-28_at_11.14.46_AM.png?1690525429 width=700>

We can say that we want to
- **maximize information gain**
- or **minimize entropy**

We want to maximize information gain.

So, there is chance that there is some other feature
- which is providing more information gain than Gender feature

So, we should use that feature instead.

This means we should calculate Information gain for Age < 35 feature as well

The information gain for each feature is as follows:

- $I_G(Parent, Age < 35) = 0.257$
- $I_G(Parent, Gender) = 0.091$

#### Which feature shall we pick to split the root node ?

We pick feature s.t it gives us maximum information gain
- In this case, splitting using Age < 35 is giving us maximum information gain.
    - So, we'll pick it to split our root node.

Let's split our node using Age<35 feature

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/801/original/Screenshot_2023-07-28_at_11.14.53_AM.png?1690525534 width=700>

Let's talk about left child node

#### Has the entropy of left child node reduced to 0 or close to 0 ?

No.
- Although the entropy is lower than before
- but it is still an impure node.

In order to achieve more confidence in prediction,
- we should further split this node.

Again, in order to split this node
- we'll calculate information gain using features
- say, these features are gender, salary, years of experience.


Whichever feature gives us the maximum information gain,
- we'll split the node using that feature.

Assume that gender gave us highest info. gain,
- we'll split this node using gender

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/802/original/Screenshot_2023-07-28_at_11.56.08_AM.png?1690525583 width=700>

We continue doing so
- until we get purer nodes
- i.e. confidence in prediction is high.

#### What happens if we have more than 2 categories for a feature ? How do we split in that case ?

In case where there are more than 2 categories in a feature,
- we simply make a child node for each category

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/803/original/Screenshot_2023-07-28_at_11.14.59_AM.png?1690525611 width=700>

## Visualizing the process of building DT

link: https://drive.google.com/file/d/1CBFhQ2bYf81kTN-x_T6B1hcbIVqbXjAz/view?usp=sharing

paper: https://opus.bibliothek.uni-augsburg.de/opus4/frontdoor/deliver/index/docId/79711/file/ECML_PKDD_Decision_Tree_Learning.pdf

Steps to follow:
1. Unzip the file
2. Launch index.html to launch the project
3. Go to Data (top right) -> Import training data -> data.csv

4. Dataset are present in folder named "Files". You can move your custom dataset into that folder. However, there are few limits on number of columns and format of data (csv)

It contains data for attrition use case (3 features)
- Gender
- Age < 35
- Marital Status


4. Select Mode as "Stepwise". It'll build DT node by node.

5. Click on build.

6. Click build again to move to next stage of DT

For each stage, you can view the information gain on left pane of window.

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/084/261/original/i3.png?1722512314' width = 700>

<center><img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/084/262/original/i4.gif?1722512330' width=800>

Now that we have learnt how decision tree works.

Let's try to implement it from scratch

## (Optional) Post Read - Scratch impl of DT

link : https://colab.research.google.com/drive/1QpgOv1W8x_l81GPN6ebQTb58eqA0RaNm?usp=sharing

## Sklearn implemenation
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""#### Fetching data"""

!gdown 19L3rYatfhbBL1r5MHrv-p_oM2wlvrhqk
!gdown 1OHLKJwA3qZopKPvlKoRldM6BvA1A4dYF
!gdown 1N7O_fWCTJLu8SIa_paKcDEzllgpMk8sK
!gdown 12Bh2AN8LcZAlg20ehpQrEWccUDaSdsOG

import pickle
# Load data (deserialize)
with open('preprocessed_X_sm.pickle', 'rb') as handle:
    X_train = pickle.load(handle)

with open('X_test.pickle', 'rb') as handle:
    X_test = pickle.load(handle)

with open('y_sm.pickle', 'rb') as handle:
    y_train = pickle.load(handle)

with open('y_test.pickle', 'rb') as handle:
    y_test = pickle.load(handle)

"""#### Reading data

#### Importing sklearn DT


sklearn DT doc - https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html
"""

from sklearn.tree import DecisionTreeClassifier

"""#### Initiating instance"""

model = DecisionTreeClassifier(criterion='entropy')

"""#### Fitting the model"""

model.fit(X_train, y_train)

"""#### Get the train / test score"""

model.score(X_test, y_test)

model.score(X_train, y_train)

"""There is a huge difference in train and test score
- Model is currently overfitting.

#### Visualizing DT
"""

from sklearn.tree import plot_tree
plt.figure(figsize=(14,14))
plot_tree(model, filled = True);

"""Notice that
- how deep is this DT is


#### How do we stop it from overfitting ?

We'll learn about how to handle bias variance tradeoff in DT in next lecture

#Points to Remember

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/804/original/Screenshot_2023-07-28_at_11.15.09_AM.png?1690525651 width=700>

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/040/805/original/Screenshot_2023-07-28_at_11.15.18_AM.png?1690525674 width=700>
"""