# -*- coding: utf-8 -*-
"""9-Notes-Logistic-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rzrDiwA8N_2J-QL8d4VkZ7D_vV2mahm8

## Content

- Sklearn implementation of logistic regression
- Accuracy Metric

- Hyperparameter Tuning

- LogOdds and Logit

- Impact of outliers

- Multiclass classification

## Sklearn Implementation of Logistic regression

Let's load the data of our business case now
"""

#Churn prediction in telecom.
import numpy as np
import matplotlib.pyplot as plt

!gdown 1uUt7uL-VuF_5cpodYRiriEwhsldeEp3m

import pandas as pd
churn = pd.read_csv("churn_logistic.csv")
churn.head()

"""We will choose 5 features for our logistic regression which we selected using simple EDA

You can go through the EDA of this to understand how we selected these features:

https://colab.research.google.com/drive/1nkbiGCMrevDzdSG9yN5bXaxeC8CPJSQg?usp=sharing
"""

cols = ['Day Mins', 'Eve Mins', 'Night Mins', 'CustServ Calls', 'Account Length']
y = churn["Churn"]
X = churn[cols]
X.shape

"""Let's split the data into training, validation and testing



"""

from sklearn.model_selection import train_test_split

X_tr_cv, X_test, y_tr_cv, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
X_train, X_val, y_train, y_val = train_test_split(X_tr_cv, y_tr_cv, test_size=0.25,random_state=1)
X_train.shape

"""We will scale our data before fitting the model"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

X_train

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)

model.coef_

model.intercept_

model.predict(X_train)

"""## Accuracy Metric

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/061/866/original/z.png?1705231614' width=700>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/061/867/original/z.png?1705231642' width=700>

Let's implement our accuracy metric now
"""

def accuracy(y_true, y_pred):
  return np.sum(y_true==y_pred)/y_true.shape[0]

accuracy(y_train, model.predict(X_train))

accuracy(y_val, model.predict(X_val))

"""So our model has a validation accuracy of 0.71, or 71.49%

Let's see how we can perform hyperparameter tuning on our logistic regression model

##**Hyperparameter tuning**

We will tune the regularization rate of our model.

You can refer to the documentation for the various list of parameters in logistic regression.

Link: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

Hence let's start doing hyper parameter tuning on parameter $C = \frac{1}{\lambda}$  to increase the performance of the model
"""

from sklearn.pipeline import make_pipeline
train_scores = []
val_scores = []
scaler = StandardScaler()
for la in np.arange(0.01, 5000.0, 100): # range of values of Lambda
  scaled_lr = make_pipeline(scaler, LogisticRegression(C=1/la))
  scaled_lr.fit(X_train, y_train)
  train_score = accuracy(y_train, scaled_lr.predict(X_train))
  val_score = accuracy(y_val, scaled_lr.predict(X_val))
  train_scores.append(train_score)
  val_scores.append(val_score)

"""Now, let's plot the graph and pick the Regularization Parameter $λ$ which gives the best validation score"""

plt.figure(figsize=(10,5))
plt.plot(list(np.arange(0.01, 5000.0, 100)), train_scores, label="train")
plt.plot(list(np.arange(0.01, 5000.0, 100)), val_scores, label="val")
plt.legend(loc='lower right')

plt.xlabel("Regularization Parameter(λ)")
plt.ylabel("Accuracy")
plt.grid()
plt.show()

"""- We see how Validation increases to a peak and then decreases

- Notice as Regularization is increasing, the Accuracy decreasing since model is moving towards Underfit

Let's take lambda value as 1000 for this data and check the
results
"""

model = LogisticRegression(C=1/1000)
model.fit(X_train, y_train)

accuracy(y_train, model.predict(X_train))

accuracy(y_val, model.predict(X_val))

"""We can observe an increase of 0.01, or 1%, in both training and validation data

Let's check our model for test data too
"""

accuracy(y_test, model.predict(X_test))

"""## Logit/ Log odds

### Log odds interpretation of logistic regression

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/061/869/original/z.png?1705232422' width=800>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/061/870/original/z.png?1705232448' width=800>

#### Which concept of earlier is this similar to?

Remember, $σ(p)$ also defined probability.

So if we simplify our winning/losing as belonging to class 1/0, then $σ(p)$ here defines the probability of belonging to class 1 (winning class)

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/061/871/original/z.png?1705232478' width=800>

..

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/061/872/original/z.png?1705232505' width=800>

#### What does this mean geometrically?

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/061/873/original/z.png?1705232529' width=800>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/061/874/original/z.png?1705232572' width=800>

To find the probability of the point lying, we simply apply exponential to both sides and solve for p, which would give:

$p=\frac{1}{1+e^{-z}}$

Note: Sigmoid and Logit and just inverse of each other, and both can be used to build a logistic regression model

## Impact of outliers

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/061/875/original/z.png?1705232605' width=700>

### Case I: When the outlier lies on the correct side

Now, $\hat{y}=σ(z^i)$

<img src="https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/036/753/original/image_2023-06-14_052158593.png?1686700322" height=500 width=600>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/061/876/original/z.png?1705232633' width=700>

Since the Loss is very less in this case:

=> The impact of outlier is **very less**

### Case II: When the outlier is on the opposite/wrong side

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/061/877/original/z.png?1705232658' width=700>

Let's say $z^i=-4.3$

So $\hat{y}$ becomes 0.01

Therefore, L = $-log_e(0.01)$

This comes out almost equal to 4.6, which is a very large value

=> The impact of outlier will be **very high**

Thus the best thing is to find the outlier and remove them, so that we get accurate results

## Multi-class classification

Till now we have seen how to use logistic regression to classify between two classes

But in real world there will be cases with many more classes

#### How can we use logistic regression in cases with more than two output classes?

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/061/878/original/z.png?1705232699' width=700>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/061/879/original/z.png?1705232726' width=700>

To train these models, we can't use the same dataset, since our data will have three classes.

So we will modify our data for the three models.

Say for model 1, to check whether the input is orange or not,
- Our output column will be modified by replacing the values with orange as 1, and rest values with 0

We will do the same for the other two models

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/061/880/original/z.png?1705232752' width=700>

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/061/881/original/z.png?1705232778' width=700>

#### Now given an input point, how to predict which class it belongs to?

<img src='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/061/882/original/z.png?1705232804' width=700>

Let's see an implementation of the same using sklearn

### Sklearn Code implementation for MultiClass Classification

Importing libraries
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from sklearn.inspection import DecisionBoundaryDisplay

"""Creating some data with multiple classes"""

# dataset creation with 3 classes
from sklearn.datasets import make_classification

X, y = make_classification(n_samples= 498,
                           n_features= 2,
                           n_classes = 3,
                           n_redundant=0,
                           n_clusters_per_class=1,
                           random_state=5)
y=y.reshape(len(y), 1)

print(X.shape, y.shape)

"""Plotting the data"""

plt.scatter(X[:, 0], X[:, 1], c = y)
plt.show()

"""Splitting the data into train validation and test set"""

from sklearn.model_selection import train_test_split

X_tr_cv, X_test, y_tr_cv, y_test = train_test_split(X, y, test_size=0.2, random_state=4)
X_train, X_val, y_train, y_val = train_test_split(X_tr_cv, y_tr_cv, test_size=0.25,random_state=4)
X_train.shape

"""training the OneVsRest Logistic Regression model"""

model = LogisticRegression(multi_class='ovr')
# fit model
model.fit(X_train, y_train)

"""Checking the Accuracy of Training, validation and Test dataset"""

print(f'Training Accuracy:{model.score(X_train,y_train)}')
print(f'Validation Accuracy :{model.score(X_val,y_val)}')
print(f'Test Accuracy:{model.score(X_test,y_test)}')

"""Creating Hyperplane of OVR LogisticRegression for the entire data"""

X

n_labels = len(np.unique(y_train))

for i in range(n_labels):
    sub_model = LogisticRegression(penalty=model.penalty, C=model.C)
    sub_model.coef_ = model.coef_[i].reshape(1, -1)
    sub_model.intercept_ = model.intercept_[i].reshape(-1, 1)
    sub_model.classes_ = np.array([0, 1])

    y_train_ovr = np.where(y_train == i, 1, 0)
    score = sub_model.score(X_train, y_train_ovr)

    print(f"OVR for label={i}, score={score:.4f}")

_, ax = plt.subplots()
DecisionBoundaryDisplay.from_estimator(model, X, response_method="predict", cmap=plt.cm.Paired, ax=ax)
plt.title("Decision surface of LogisticRegression")
plt.axis("tight")

# Plot also the training points
colors = "bry"
for i, color in zip(model.classes_, colors):
        idx = np.where(y == i)
        plt.scatter(
            X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired, edgecolor="black", s=20
        )


# Plot the three one-against-all classifiers
xmin, xmax = plt.xlim()
ymin, ymax = plt.ylim()
coef = model.coef_
intercept = model.intercept_

def plot_hyperplane(c, color):
        def line(x0):
            return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]

        plt.plot([xmin, xmax], [line(xmin), line(xmax)], ls="--", color=color)

for i, color in zip(model.classes_, colors):
        plot_hyperplane(i, color)

plt.show()

"""**Observe**

We can see how One-vs-Rest Logistic Regression is able to classify Multi-class Classification data
"""