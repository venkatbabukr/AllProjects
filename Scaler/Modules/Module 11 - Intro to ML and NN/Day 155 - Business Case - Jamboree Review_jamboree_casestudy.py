# -*- coding: utf-8 -*-
"""Jamboree-casestudy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lNopMa9QDz0q9C9iFL8z1z1G3X6fkC40

## Problem Statement:

Jamboree has helped thousands of students like you make it to top colleges abroad. Be it GMAT, GRE or SAT, their unique problem-solving methods ensure maximum scores with minimum effort.

They recently launched a feature where students/ learners can come to their website and check their probability of getting into the IVY league college. This feature estimates the chances of graduate admission from an Indian perspective.

### How can you help here?
- Your analysis will help Jamboree in understanding what factors are important in graduate admissions and how these factors are interrelated among themselves. It will also help predict one's chances of admission given the rest of the variables.

## 1. Importing the libraries and loading the data
"""

from logging import warning
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


import warnings
warnings.filterwarnings("ignore")


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import Lasso,Ridge,LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score

df = pd.read_csv("Jamboree.csv")

"""## 2. Basic Exploration"""

df.head()

df.shape

df.info()

"""## 3. EDA"""

df.head()

df["Serial No."].nunique() == df.shape[0]

"""#### Univariate Analysis"""

fig = sns.distplot(df['GRE Score'], kde=True)
plt.title("Distribution of GRE Scores")
plt.show()

fig = sns.distplot(df['TOEFL Score'], kde=True)
plt.title("Distribution of TOEFL Scores")
plt.show()

fig = sns.distplot(df['University Rating'], kde=True)
plt.title("Distribution of University Rating")
plt.show()

fig = sns.distplot(df['SOP'], kde=True)
plt.title("Distribution of SOP Ratings")
plt.show()

fig = sns.distplot(df['CGPA'], kde=True)
plt.title("Distribution of CGPA")
plt.show()

plt.show()

"""Q) How do we check if my distribution is normal?
- qq plots
- Shapiro-Wilk Test
- KS Test
- Boxplot

#### Bivariate Analysis
"""

fig = sns.regplot(x="GRE Score", y="TOEFL Score", data=df)
plt.title("GRE Score vs TOEFL Score")
plt.show()

## Insights: People with higher GRE Scores also have higher TOEFL Scores which is justified because both TOEFL and GRE have a verbal section which although not similar are relatable

fig = sns.regplot(x="GRE Score", y="CGPA", data=df)
plt.title("GRE Score vs CGPA")
plt.show()

## Insights: Although there are exceptions, people with higher CGPA usually have higher GRE scores maybe because they are smart or hard working

fig = sns.scatterplot(x="CGPA", y="LOR ", data=df, hue="Research")
plt.title("LOR vs CGPA")
plt.show()

"""Insights: LORs are not that related with CGPA so it is clear that a persons LOR is not dependent on that persons academic excellence. Having research experience is usually related with a good LOR which might be justified by the fact that supervisors have personal interaction with the students performing research which usually results in good LOR."""

## Perform a Hypothesis Test to confirm this statistical evidence.

fig = sns.scatterplot(x="GRE Score", y="LOR ", data=df, hue="Research")
plt.title("GRE Score vs LOR")
plt.show()

"""Insights: GRE scores and LORs are also not that related. People with different kinds of LORs have all kinds of GRE scores"""

fig = sns.scatterplot(x="CGPA", y="SOP", data=df)
plt.title("GRE Score vs CGPA")
plt.show()

"""Insights: CGPA and SOP are not that related because Statement of Purpose is related to academic performance, but since people with good CGPA tend to be more hard working so they have good things to say in their SOP which might explain the slight  move towards higher CGPA as along with good SOPs"""

fig = sns.scatterplot(x="GRE Score", y="SOP", data=df)
plt.title("GRE Score vs SOP")
plt.show()

fig = sns.scatterplot(x="TOEFL Score", y="SOP", data=df)
plt.title("GRE Score vs SOP")
plt.show()

df.head()

## Pair plot
## Create a function that takes a list of columns as input and return scatterplots of all unique combinations.

"""### Correation Analysis"""

corr = df.corr()
sns.heatmap(corr, linewidths=.5, annot=True)
plt.show()

df = df.drop(['Serial No.'], axis=1)

corr = df.corr()
sns.heatmap(corr, linewidths=.5, annot=True)
plt.show()

df.corr()

"""## 4. ML Design: Preprocessing"""

## Split our data into X and y.

X = df.drop(['Chance of Admit '], axis=1)
y = df['Chance of Admit ']

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.20, shuffle=True)

X_train, y_train

## Scaling my data

X_train_columns=X_train.columns

std=StandardScaler()

X_train_std=std.fit_transform(X_train)

X_train_std

X_train=pd.DataFrame(X_train_std, columns=X_train_columns)
X_train

X_test

"""## 5. Model Building"""

models = [
           ['Linear Regression :', LinearRegression()],

          ['Lasso Regression :', Lasso(alpha=0.5)], #try with different alpha values
          ['Ridge Regression :', Ridge(alpha=1.0)] #try with different alpha values
          ]

for name,model in models:
    model.fit(X_train, y_train.values)
    predictions = model.predict(std.transform(X_test))
    print(name, (np.sqrt(mean_squared_error(y_test, predictions))), r2_score(y_test, predictions))
    print("coefficient: ", model.coef_)
    print("intercept: ", model.intercept_)
    print("------"* 10)

"""#### OLS(Ordinary Least Square Model)"""

# sklearn ---> Gradient Descent
# StatsModel ---> OLS (Cumputation of finiding the loss using Matrix Mutiplication)

import statsmodels.api as sm

X_train = sm.add_constant(X_train)

model = sm.OLS(y_train.values, X_train).fit()

print(model.summary())

X_train_new=X_train.drop(columns='SOP')

model1 = sm.OLS(y_train.values, X_train_new).fit()
print(model1.summary())

"""## Assumptions of LR model"""

# 1) No Multicollinearity (VIF)
# 2) Normality of Residuals
# 3) No Hetroscadascity
# 4) No Autocorrelation
# 5) Assumption of Linearity



"""### VIF(Variance Inflation Factor)"""

from statsmodels.stats.outliers_influence import variance_inflation_factor

def calculate_vif(dataset,col):
  dataset=dataset.drop(columns=col,axis=1)
  vif=pd.DataFrame()
  vif['features']=dataset.columns
  vif['VIF_Value']=[variance_inflation_factor(dataset.values,i) for i in range(dataset.shape[1])]
  return vif

calculate_vif(X_train_new,[])

X_test_std= std.transform(X_test)

X_test=pd.DataFrame(X_test_std, columns=X_train_columns) # col name same as train datasets

X_test

X_test = sm.add_constant(X_test)

X_test_del=list(set(X_test.columns).difference(set(X_train_new.columns)))

X_test_del

print(f'Dropping {X_test_del} from test set')

X_test_new=X_test.drop(columns=X_test_del)

X_test_new

#Prediction from the clean model
pred = model1.predict(X_test_new)

from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error

print('Mean Absolute Error ', mean_absolute_error(y_test.values,pred) )
print('Root Mean Square Error ', np.sqrt(mean_squared_error(y_test.values,pred) ))

"""### Mean of Residuals"""

residuals = y_test.values-pred
mean_residuals = np.mean(residuals)
print("Mean of Residuals {}".format(mean_residuals))

"""### Test for Homoscedasticity"""

p = sns.scatterplot(x=pred,y=residuals)
plt.xlabel('predicted values')
plt.ylabel('Residuals')
plt.ylim(-0.4,0.4)
plt.xlim(0,1)
p = sns.lineplot(x=[0,26], y=[0,0], color='blue')
p = plt.title('Residuals vs fitted values plot for homoscedasticity check')

import statsmodels.stats.api as sms
from statsmodels.compat import lzip
name = ['F statistic', 'p-value']
test = sms.het_goldfeldquandt(residuals, X_test)
lzip(name, test)

"""### Normality of residuals"""

p = sns.distplot(residuals,kde=True)
p = plt.title('Normality of error terms/residuals')

"""Create a function that will take a trained model and give you the output."""

1) Train the model as it is with outliers (Model 1) [Baseline]
2) Train the model without outlier (i.e loss of data): (Model2)



"""## Insights and Recommendation"""

## Insights will come from EDA
## Recommendation will come from Model Performance/ Feature Importance.

