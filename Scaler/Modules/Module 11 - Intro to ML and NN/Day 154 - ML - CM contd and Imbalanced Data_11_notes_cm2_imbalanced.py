# -*- coding: utf-8 -*-
"""11-Notes-CM2-Imbalanced.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ccnNQe_q4KNwVM-OEor12zhCv98BSpoH

## Content

- **Recap**

- **Sensitivity and Specificity**

- **ROC**


- **AUC**

- **Precision Recall Curve**

- **Imbalanced data**
    - Model confidence

- **Handling imbalance**
    - **Class weights**
    - **Undersampling**
    - **Oversampling**
    - **SMOTE**

## **Recap**

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/222/original/Screenshot_2023-08-08_at_11.36.16_AM.png?1691475401 width=700>

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/224/original/Screenshot_2023-08-08_at_11.36.24_AM.png?1691475519 width=700>

## Spam vs Non-Spam: Business Case



You are working in Google and have a task to create an Email spam detection model

Here,
- **not spam** → Class 0
- **spam** → Class 1

<br>




**Note:** For simplicity, lets call:
-  Class 0 **Not Spam** as Negative Class
- and Class 1 **Spam** as Positive Class

Lets Load the data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

!gdown 1dw56R8SzKgTgiKurfBLUTxmiewJacMkt

dt = pd.read_csv('Spam_finalData.csv')




X_train,X_test,y_train,y_test = train_test_split(dt.drop(['label_num'],axis=1),dt['label_num'])

y_test.value_counts().plot(kind='bar')
plt.xlabel('Class')
plt.ylabel('Number of Samples')
plt.title('Test Data Distribution')
plt.show()


print(f'Training Data:{X_train.shape},{y_train.shape}, Testing Data: {X_test.shape},{y_test.shape}')





model = LogisticRegression()
model.fit(X_train,y_train)

"""## **Sensitivity**

Suppose we have **Fortis Hospital data** where:
- 100 patients are Cancerous (Class1)
- 900 patients are not-Cancerous (Class0)

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/225/original/Screenshot_2023-08-08_at_11.36.32_AM.png?1691475554 width=700>

<br>

In medical firms, **screening test are conducted**
- in order to **identify whether a patient carries the disease or not**

<br>

#### How would we want the screening test to perform like for cancerious patients?

Ans: We would want the screening test to:
- Correctly predict as many Cancer patients (TP ⇑)
- While keeping the misclassification of Cancer patients low (FN ⇓)

<br>

**note:** This want of having **TP high** and **FN low** from a screeing test, is called **sensitivity**

#### Why is high Sensitivity important ?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/226/original/Screenshot_2023-08-08_at_11.36.38_AM.png?1691475592 width=700>

Ans: Imagine if the screening test fails to identify 95 cancer patients:

- Due to **lack of treatments**, the cancer disease will spread
- and make the **conditions for the patient more severe**

<br>

Hence **high Sensitivity** of the screening test  becomes **crucial**:
- As the **consequences of failing to treat the disease worsens the patient's condition**

#### How to calculate Sensitivity using Confusion Matrix?

Ans: For a model to have high Sensitivity,
- TP ⇑
- FN ⇓

hence sensitivity can be interpreted as:
- $Sensitivity = \frac{TP}{TP+FN}$


**observe:**

The equation to define **Sensitivity is same as Recall**

<br>

**note:** Since **Sensitivity tracks the rate of TP**, it is also called as **True Positive Rate (TPR)**

#### What will happen if a model/Screeing test is "in-sensitive"?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/227/original/Screenshot_2023-08-08_at_11.36.45_AM.png?1691475648 width=700>

Ans: It will cause  **FN to increase**:
- And as discussed worsen Cancer patient condition due to lack of treatment


<br>

#### How to measure the rate of change in FN ?

ans: It is calculated using **False Negative Rate (FNR)**:

 - $FNR  = 1 - Specificity  = 1  - \frac{TP}{TP+FN} = \frac{FN}{TP+FN}$


<br>

**note:** As these patients are missed by the model/screening test,  **FNR** is called **Miss Rate**

## **Specificity**

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/228/original/Screenshot_2023-08-08_at_11.36.52_AM.png?1691475687 width=700>

#### We talked how model have high TP but what about TN and FP ?
Ans: We would want the model to:
- Correctly predict as many Non-Cancer patients (TN ⇑)
- While keeping the misclassification of Non-Cancer patients low (FP ⇓)

**note:** This want of having **TN high** and **FP low** from a screeing test, is called **specificity**

<br>

#### Why is high Specificity important ?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/229/original/Screenshot_2023-08-08_at_11.37.29_AM.png?1691475739 width=700>

ans: Imagine if the screening test identifies 600  Non-cancer patients as cancerous:

- This will cause fruitless treatments on these patients which are quite expensive
- Also will create a social Stigma, anxiousness, Stress to these patients

Hence, **Specificity becomes crucial**:
- In order to avoid unnecessary expensive treatments, medications , social stigma and anxiety for the patient


<br>

**observe**

**Specificity is basically Sensitivity** defined **for Negative Class (Class0)**

#### How to calculate Specificity using Confusion Matrix ?

Ans: As to have high Specificity:
- TN ⇑
- FP ⇓

Hence we can define Specificity as:

$Specificity = \frac{TN}{TN + FP}$

**Note:** As Sensitivity measures the rate of TP ,
- Specificity measures the rate of TN, hence also called as **True Negative Rate (TNR)**

#### What will happen if a model/Screeing test is "not-specific"?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/230/original/Screenshot_2023-08-08_at_11.37.41_AM.png?1691475799 width=700>

Ans: It will cause  **FP to increase**:
- And as discussed, resulting in more Non-Cancer patients(class0) to undego
 - irrelevant expensive treatments, anxiety and social stigma


<br>

#### How to measure the rate of change in FP ?

ans: AS **FNR** is used to measure change in FN , rate of change in FP  is calculated using **False Positive Rate (FPR)**:

 - $FPR  = 1 - Specificity  = 1  - \frac{FP}{TN+FP} = \frac{FP}{TN+FP}$

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/044/106/original/Screenshot_2023-08-24_at_10.22.37_AM.png?1692852989 width=700>

## **ROC (Reciever Operating Characterstic Curve)**

#### Recall our model got a 88% F1score, what can be done to increase performance ?

Ans: Doing some hyperparameter tuning might help:
- Adding regularization

#### What if, adding regularization does not do major improvement in model, what can be the issue ?
Ans: Recall our Cancer patient example, where:
- We wanted to correctly classify cancer patients
- And for this, we used a threshold ($\tau$) of 0.4 instead of 0.5

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/231/original/Screenshot_2023-08-08_at_11.37.49_AM.png?1691475864 width=700>

#### How to ensure we select the correct threshold ?

Ans: For this, we will be doing 3 steps.

**Step1:** Remember, Logistic Regression **outputs probabilities [$ p = P(y=1|x)$ or $p =  P(y=0|x)$] before thresholding**

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/232/original/Screenshot_2023-08-08_at_11.37.57_AM.png?1691475926 width=700>

-  Find the probablities for $p = P(y=1|x)$  and **sort  probabilities in descending order**

**Step2:** Set each probabilities as threshold ($\tau$) and find $\hat{y}$:

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/234/original/Screenshot_2023-08-08_at_11.38.06_AM.png?1691475976 width=700>

- After creating $\hat{y}$: find **TPR and FPR**
  - $TPR = \frac{TP}{TP+FN}$

  - $FPR = \frac{FP}{TN+FP}$


**Note:** $\hat{y} = 1 $ if $p \geq \tau$ , else $\hat{y} = 0$

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/235/original/Screenshot_2023-08-08_at_11.38.16_AM.png?1691476007 width=700>

<br>

**Step3:** After Step2, we will have **pairs of TPR and FPR values for each probabilities**:

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/237/original/Screenshot_2023-08-08_at_11.38.23_AM.png?1691476087 width=700>

- Now, we plot TPR (Y-axis) vs FPR (X-axis) for each threshold
- We **select the one threshold based on our problem**

<br>


**Note:** Since in our Spam classifier problem, we want FPR $\approx 0$
- We **pick that threshold** which has **high TPR and low FPR**

<br>


**observe**

The **plot TPR (Y-axis) vs FPR (X-axis)** for each threshold will **look like a curve**. This is called as **ROC (Reciever Operating Characteristic) curve**

- ROC term comes from electronics and radio engineering
- usually used in signal procressing

## **Understanding steps for ROC**

Suppose we have 6 sample data, such that:

- X =[$x^{(1)},x^{(2)},x^{(3)},x^{(4)},x^{(5)}, x^{(6)}$]
- Y = [1, 1, 0, 1, 0 , 0]
- p = [0.65, 0.94, 0.3, 0.92, 0.7 , 0.2]



**Observe**

data is balanced
- Equal number of class 1 and class 0 samples

<br>

**Step1**

We sort the entire data based on values of $p$ in decreasing order:

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/238/original/Screenshot_2023-08-08_at_11.38.33_AM.png?1691476121 width=700>

- p = [0.94, 0.92, 0.7, 0.65, 0.3, 0.2]
- X = [$x^{(2)},x^{(4)},x^{(5)},x^{(1)},x^{(3)}, x^{(6)}$]
- Y = [1, 1, 0, 1, 0, 0]

**Step2**

Taking each probabilites as threshold and finding $\hat{y}$

- Showing when:  $\tau^{1}  = p^{1} =  0.94$,


| X | Y | P | $\hat{y}$ |
| :-- | :--| :--| :--|
|$x^{2}$| 1  | 0.94| 1 |
|$x^{4}$| 1  | 0.92| 0 |
|$x^{5}$| 0  | 0.70| 0 |
|$x^{1}$| 1  | 0.65| 0 |
|$x^{3}$| 0  | 0.30| 0 |
|$x^{6}$| 0  | 0.20| 0 |

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/239/original/Screenshot_2023-08-08_at_11.39.11_AM.png?1691476159 width=700>

<br>

We now calculate TPR and FPR:
#### As TP = 1 , FP = 0 , FN  = 2 , TN = 3, what will be TPR and FPR value ?
Ans: TPR and FPR value are:

- $TPR^{(1)} = \frac{TP}{TP + FN} = \frac{1}{1 + 2} = 0.33 $

- $FPR^{(1)} = \frac{FP}{FP + TN} = \frac{0}{0 + 3} = 0 $

We repeat **Step2** for every other probabilities

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/240/original/Screenshot_2023-08-08_at_11.39.20_AM.png?1691476199 width=700>

- This will give us pair of **TPR and FPR** for each of the 6 probabilities


| P | TPR | FPR |
| :--| :--| :--|
| 0.94| 0.33 | 0.00 |
| 0.92| 0.50 | 0.00 |
| 0.70| 0.67 | 0.33 |
| 0.65| 1.00 | 0.33 |
| 0.30| 1.00 | 0.67 |
| 0.20| 1.00 | 1.00 |

**Step3**, after finding all the pair TPR and FPR for each probabilites, We **plot each of TPR and FPR values**

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/241/original/Screenshot_2023-08-08_at_11.39.28_AM.png?1691476231 width=700>

#### What will be the ideal threshold to pick ?

Ans: In Spam classifier, we want minimum FP value while maximizing TP:
- The best tradeoff is achieved when $P = 0.65$

<br>

**Observe**

The ROC curve is very irregular in shape

#### Lets consider a Random Model, what will be TPR and FPR ?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/242/original/Screenshot_2023-08-08_at_11.39.36_AM.png?1691476266 width=700>

Ans: the $\hat{y}$  for Random model, will be same as jumbling $y$
- As the example data was balanced:
 - TP+FN = FP + TN = $\frac{6}{2}  = 3 $

- And if **there are k TP points, due to randomness**
  - There will be **k FP points as well**
  - Showing TPR = FPR

<br>

Hence on plotting TPR vs FPR for Random model,
- It becomes a line with equation: $(y=x)$

```
Which of the following statements is true regarding the ROC curve?
a) The closer the curve is to the diagonal line, the better the model's performance.
b) The further the curve is from the diagonal line, the better the model's performance.
c) The ROC curve is only applicable to binary classification problems.
d) We get ROC by plotting TPR and FPR
```
**Correct Answer**
b) The further the curve is from the diagonal line, the better the model's performance.

## **AU-ROC (Area Under ROC Curve)/ AUC**

Imagine we have two models: Model A , Model B and their plot look like:

#### According to the plot which model is better Model A or Model B ?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/243/original/Screenshot_2023-08-08_at_11.40.10_AM.png?1691476298 width=700>

Ans: Since TPR value of Model B is greater than Model A at the same FPR value

- Hence Model B better than Model A

We see intitutively, Model B better than Model A, but

<br>

#### How to mathematically show Model B better than Model A ?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/246/original/Screenshot_2023-08-08_at_12.05.07_PM.png?1691476520 height=600 width=700>

Ans: Lets consider area under the ROC curve (AU-ROC/ AUC):

- Then AUC of Model B > AUC of Model A,

Hence Model B is better than Model A

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/245/original/Screenshot_2023-08-08_at_12.03.07_PM.png?1691476401 height=400 width=400>

```
What will be the AUC for a random model ?

a) 0
b) 0.5
c) 1
d) 0.25

```
**Correct Answer**
b) 0.5

#### What will be the value of TPR and FPR for an ideal model for each threshold ?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/247/original/Screenshot_2023-08-08_at_11.40.28_AM.png?1691476592 width=700>

Ans: When threshold is close to 1 ( most of the $\hat{y} = 0$)
- The $TPR \approx 1$ and FPR $\approx 0$

When threshold is close to 0, (most of the $\hat{y} = 1$ )
- Then both $TPR, FPR \approx 1$


<br>


#### What will be the AU-ROC for an ideal model ?
Ans: The curve for Ideal model will be
1. Line starting from $(0,0)$ to $(0,1)$
2. and another line starting from $(0,1)$ to $(1,1)$

Hence the **AU-ROC will be the area of the whole square = $1 \times 1 = 1$**

<br>

#### What will be the AU-ROC for a Bad model ?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/248/original/Screenshot_2023-08-08_at_11.40.36_AM.png?1691476624 width=700>

Ans: Clearly we can say, the curve for the Bad model will have
1. Line starting from $(0,0)$ to $(1,0)$
2. and another line starting from $(1,0)$ to $(1,1)$

Hence AU-ROC $ = 0 $

#### What does AU-ROC $= 0$ really mean ?

ans: It means that
- **the bad model probabilities ($p$) misclassifies every $y$ of the data.**

#### Would doing $(1-p)$ on every model probability fix the Bad model ?

Ans: **Yes**, as now the model AUC becomes:
- $AUC_{new} = 1 - AUC_{old} = 1 - 0  = 1 $

<br>

**Note:** Therefore, **any bad model** which has a curve below the random model $= 0.5$ , **can be fixed**
- By doing a simple hack of reversing the probabilities

#### How is AU-ROC curve  different from Precision, Recall or F1-score?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/249/original/Screenshot_2023-08-08_at_11.40.47_AM.png?1691476803 width=700>

Ans: In Precision, Recall or F1 score:

- We used a certrain threshold on  which we found $\hat{y^{(i)}}$


While in AU-ROC curve:
- we find **AUC metric on every possible threshold**

Suppose we have a label $y = [1,1,0,1,1]$ and there are 2 models, such that:

1. $P_{M1} $: $[0.95, 0.92, 0.80, 0.76, 0.71]$

2. $P_{M2} $: $[0.2, 0.1, 0.08, 0.06, 0.02]$

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/250/original/Screenshot_2023-08-08_at_11.40.56_AM.png?1691476842 width=700>

#### What will be $\hat{y^{(i)}}$ for M1 when threshold = $0.95$ ?

Ans: $\hat{y^{(i)}}_{M1} = [1, 0 , 0 , 0, 0 ]$

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/251/original/Screenshot_2023-08-08_at_11.41.06_AM.png?1691476888 width=700>

#### What will be $\hat{y^{(i)}}$ for M2 when threshold = $0.2$ ?

Ans: $\hat{y^{(i)}}_{M2} = [1, 0 , 0 , 0, 0 ]$

<br>

Similarly, when threshold = $0.92$, for M1:
- $\hat{y^{(i)}}_{M1} = [1, 1 , 0 , 0, 0 ]$

And threshold = $0.1$, for M2:
- $\hat{y^{(i)}}_{M2} = [1, 1 , 0 , 0, 0 ]$

**observe**

As the **ordering of the threshold of 2 models are same**:
- Hence $[[TPR^{(1)}_{M1}, FPR^{(1)}_{M1}],...,[TPR^{(n)}_{M1}, FPR^{(n)}_{M1}] ] =  [[TPR^{(1)}_{M2}, FPR^{(1)}_{M2}],...,[TPR^{(n)}_{M2}, FPR^{(n)}_{M2}] ] $

- Meaning the $AUC_{M1} = AUC_{M2}$

## **AU-ROC curve Code**
"""

from sklearn.metrics import roc_curve, roc_auc_score

"""stores model probabilities"""

probability = model.predict_proba(X_test)

probability

"""**Observe**

```Probability``` variable contains 2 probability $P(Y=1 |X)$ and $P(Y=0 |X )$

#### But for thresholding we need only one probability, what can be done ?

Ans: lets consider only $ p = P(Y=1 |X) $



"""

probabilites = probability[:,1]

fpr, tpr, thr = roc_curve(y_test,probabilites)

plt.plot(fpr,tpr)

#random model
plt.plot(fpr,fpr,'--',color='red' )
plt.title('ROC curve')
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.show()

# AUC
roc_auc_score(y_test,probabilites)

"""## **issue with AU-ROC**

**observe**

We get an AU-ROC value of 97.8%
- showing that the model is performing very good

But recall our F1 Score was just 88%
- which made us believe our model was a decent one

<br>

#### Why a huge difference in model performance when using AU-ROC and F1 score ?
ans: Recall our data is imbalance:(70% $\rightarrow$ class 0 and 30% $\rightarrow$ class1):

- ROC curves provide an aggregate measure of model performance across all possible classification thresholds.

- Hence it can make a **poor model on the minority class appear better**
 - by focusing more on the performance on the majority class.

<br>

**Note:** When data is highly imbalanced,
- AU-ROC is not prefered

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/044/107/original/Screenshot_2023-08-24_at_10.24.54_AM.png?1692853066 width=700>

```
If data contains 50 spam and 300 non-spam samples, then which is true?
a) ROC may overestimate the model's performance.
b) ROC may underestimate the model's performance.
c) ROC does provide useful information.
d) ROC cannot be created
```
**Correct Answer**

a) ROC may overestimate the model's performance.

## **Precision - Recall curve**

#### What can be used instead of AU-ROC curve, when data is imbalanced ?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/252/original/Screenshot_2023-08-08_at_11.41.18_AM.png?1691476974 width=700>

Ans: Since F1 score works well for imbalance data:
- We take **Precision and Recall values for every probability** instead of TPR and FPR

<br>

Rest of the approach remains same as AU-ROC

**Note:** Finding area under the curve of Precision and recall is called **AU-PRC**

## **Precision Recall curve**
"""

from sklearn.metrics import precision_recall_curve
from sklearn.metrics import auc

precision, recall, thr = precision_recall_curve(y_test, probabilites)

plt.plot(recall, precision)

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('PR curve')
plt.show()

auc(recall, precision)

"""**observe**

Now the **AU-PRC** comes close to F1 score
- Showing that **PRC** worked just fine in imbalanced data

#### Extra read

Some extra information for classification Metrics in PostRead:

- [Lift and Chart, Cheatsheet for Metrics, metric vs loss](https://colab.research.google.com/drive/1WadYQgslAmgSqSu9ZMcGoWSNp7HkEGln?usp=drive_link)

## **Imbalance Data**

Even though ,AU-PRC = 90%, still the Logistic Regression model haven't increased performance
- and we know its because of the imbalance in data

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/253/original/Screenshot_2023-08-08_at_11.41.27_AM.png?1691477017 width=700>

#### When do we say that data is imbalanced ?
Ans: when the **number of samples of a class exceeds** the number of samples of the other class:
1. if there is a 70-30% Class sample ratio → **slightly imbalanced**

2. if there is a 80-20%/90-10% Class sample ratio → **imbalanced**

3. if there is a 95-5% Class sample ratio → **extremely imbalanced**


<br>

**Note:**
- The class which has **more number of samples → Majority Class**
- The class which has **less number of samples → Minority Class**

## **Imbalance Data with Logistic Regression**

##(Optional)

#### What happens to Logistic Regression when data is balanced ?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/254/original/Screenshot_2023-08-08_at_11.41.35_AM.png?1691477060 width=700>

Ans: Recall that **Logistic Regression creates a hyperplane**($\pi: y = w^Tx+w_0$):
- That classify the data into +ve (spam) and -ve (non-spam) classes

This hyperplane is achieved:
- When **Gradient Descent finds optimized weights**($w,w_0$) for the hyperplane

#### What happens to Logistic Regression when data is imbalanced ?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/255/original/Screenshot_2023-08-08_at_11.41.43_AM.png?1691477111 width=700>

Ans: recall that in the logloss function, we use probability of model($p = P(y=1|X)$):

- Which is the probability of the features $X$  to have a class label = 1

This could also mean:
- how confident, Logistic regression is, in predicting that the class label for this sample is Class 1

<br>

Suppose we have two negative class points such that:

1. for Point1, model probability: $p^{(1)} = 0.5$

2. for Point2, model probability: $p^{(2)} = 0.1$

Then logloss for each point becomes:
1. $logloss = -(1-0) \times log(1-0.5) = 0.301 $

2. $logloss = -(1-0) \times log(1-0.1) = 0.045 $


**observe**

Clearly we can see:
1. When model probability $p^{(2)} = 0.1$:
- **the model is confident** that the point does not belong to Class1
- then **the Logloss value is very small**

- As opposed to **when model is uncertain( $p^{(1)} =0.5$), logloss is comparatively higher**


<br>

## **Model Confidence**

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/257/original/Screenshot_2023-08-08_at_11.41.59_AM.png?1691477143 width=700>

Model is more confident, when point lies away from its hyperplane:
- Hence **both the +ve and -ve samples pushes the hyperplane** away from them

Therefore **the optimal hyperplane is in between the +ve and -ve samples**
- when data is balanced

<br>

#### Where does hyperplane go when data is imbalanced ?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/259/original/Screenshot_2023-08-08_at_12.17.10_PM.png?1691477255 width=700>

Ans: Clearly, the majority class samples dominates:
- Hence it pushes the hyperplane away from them
- making the hyperplane be close to the minority class



**Thus making the new hyperplane uncertain for predicting minority class**

## **Handling Imbalance Data**

### **Class Weight**

We know data imbalance can be the cause for hindering the performance of our model,

#### How to make imbalance data balanced ?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/261/original/Screenshot_2023-08-08_at_12.17.27_PM.png?1691477378 width=700>

Ans: Suppose we have 1000 data sample such that:

1. Spam email $\rightarrow$ 150
2. Non-Spam email $\rightarrow$ 850

**observe**

Non-spam Emails ($  \frac{850}{150} = 5.67 $) times more than Spam emails

<br>

#### How to make Spam email samples be equivalent to Non-Spam email samples ?

Ans: By giving a weightage to Spam datapoints such that:
1. A single Spam datapoint $ =  5.67$  Non-spam datapoint
  - Meaning if there are **3 spam datapoints**, it is equivalent of having **$3\times 5.67 \approx 18$ non-spam datapoint**

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/262/original/Screenshot_2023-08-08_at_11.42.42_AM.png?1691477430 width=700>

<br>


#### Where should this weightage parameter be added to in the model ?
Ans: Since Imbalance data effects the logloss the most,
- We add this weightage parameter ($W_i$) to the loss function such that:

  - $Loss = \frac{1}{n}[\sum_{i=1}^{n} logloss_i \times W_i + \lambda \sum_{j=1}^{d} w_j^2 ]$

<br>

Here:
-  $W_i = 5.67$ for +ve class samples (Minority class)
-  $W_i = 1$ for -ve class samples (Majority class)

**note** This **weightage parameter($W_i$) is called Class weight**

### **Class weight Code**

Lets now see how its implemented in Sklearn for Logisitic Regression:
"""

y_train.value_counts().plot(kind='bar')
plt.xlabel('Class')
plt.ylabel('Number of Samples')
plt.title('Train Data Distribution')
plt.show()

"""**observe**

The training data:
- Non-spam data = 2727
- Spam data = 1151

Hence weightage parameter becomes:
- $W_i = \frac{2727}{1151} = 2.37$
"""

# Model creation, prediction

def training(model,X_train,y_train,X_test,y_test):

  model.fit(X_train, y_train)

  train_y_pred = model.predict(X_train)
  test_y_pred = model.predict(X_test)

  train_score = f1_score(y_train, train_y_pred)
  test_score = f1_score(y_test, test_y_pred)

  return train_score,test_score

# minority class needs more re-weighting


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score

model = LogisticRegression(class_weight={0:1,1:2.37})

f1_train,f1_test = training(model,X_train,y_train,X_test,y_test)
print(f'Training F1 score:{f1_train}, Testing F1 score:{f1_test}')

"""**Observe**

how introducing Weighted-loss,
- did not do much change in F1-score

<br>


#### What can be the reason ?
Ans: lets check the confusion matrix
"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

y_pred = model.predict(X_test)

conf_matrix = confusion_matrix(y_test, y_pred)

ConfusionMatrixDisplay(conf_matrix).plot()

"""**Observe**

Clearly, by introducing Class weights,
- Model has predicted many Non-Spam emails as Spam ($FP ⇑$)
- Hence the F1 score is low

## **Undersampling**

As we saw, introducing Class weights did not help much!

#### What can be done to make imbalance data balanced ?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/263/original/Screenshot_2023-08-08_at_11.42.53_AM.png?1691477492 width=700>

Ans: Since Non-Spam samples are more in number,
- lets randomly drop these samples such that:
  - the number of Non-Spam data = number of Spam data

<br>

**Note**

This method of randomly dropping majority class sample is called **undersampling**

Clearly, dropping samples of data causes:
- Reduces the model reliability because now model of a **smaller training dataset**


#### When to use Undersampling ?

Ans: Typically used when the number of sample ($n$) is very large $\approx$ 1 Billion

## **Oversampling**

#### Since our data do not have $n \approx 1$Billion, we cannot use Undersampling, what else can we do ?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/265/original/Screenshot_2023-08-08_at_11.43.00_AM.png?1691477522 width=700>

Ans: A data is balanced when there is equal number of samples for Class 0 and Class 1
- Instead of removing datapoints, lets **randomly select minority class samples**
- and **create dupilicate entry to increase the minority class**

<br>

**note:** This method of adding randomly selected minority class sample is called **Oversampling**

## **Understanding Oversampling**

Suppose there are 1000 samples such that:

- 850 samples are Non-Spam
- 150 samples are Spam

Clearly, we randomly select Spam class samples and create duplicate entries such that:

- Number of samples of Spam = 850

<br>

##**Oversampling code**
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score
from imblearn.over_sampling import RandomOverSampler

# Create an instance of RandomOverSampler
oversampler = RandomOverSampler()

# Perform oversampling on the training data
print('Before Oversampling')
print(y_train.value_counts())
X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train, y_train)

print('After Oversampling')
print(y_train_oversampled.value_counts())

model = LogisticRegression()

f1_train,f1_test = training(model,X_train_oversampled, y_train_oversampled,X_test,y_test)

print(f'Training F1 score:{f1_train}, Testing F1 score:{f1_test}')

"""**Observe**

Training F1 Score is much higher than testing F1 Score

<br>

#### What can be said when training performance > testing performance ?

Ans: Model Overfits
- This means if we add same repitive samples of minority class, **it can lead to overfitting**

#### Why does model overfits in oversampling technique ?

Ans: because oversampling just **repeats samples**
- This makes the model to over learn patterns

<br>

#### What can be a smarter approach for oversampling ?
Ans: Instead of repeating the samples:
- Lets create **synthetically new samples** for our minority class label

- This approach will provide new samples to the model so it does not over learns any patterns

<br>

## **SMOTE (Synthetically Minority Oversampling Technique)**

#### How to create synthetic samples ?

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/267/original/Screenshot_2023-08-08_at_11.43.08_AM.png?1691477611 width=700>

Suppose from our Spam data:
- We randomly select a class1 label sample $x^{(1)}$

Now we find **Euclidean distance between this sample$x^{(1)}$ and all the remaining samples**:
- Pick the nearest 3 Class 1 label samples $x^{(5)},x^{(20)},x^{(10)} $

We then randomly choose a number between $[0,1]$, lets call it $ϕ$:
1. and multiply it with each of the Euclidean distances and add it to $x^{(1)}$

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/042/268/original/Screenshot_2023-08-08_at_11.43.16_AM.png?1691477645 width=700>

- $x^{(new1)} = x^{(1)} +ϕ \times dist(x^{(1)},x^{(5)})$
 - $x^{(new2)} = x^{(1)} +ϕ \times dist(x^{(1)},x^{(20)})$
 - $x^{(new3)} = x^{(1)} +ϕ \times dist(x^{(1)},x^{(10)})$

This creates three new synthetic datapoints for our minority class

<br>

**Note:** This approach of creating new synthetic datapoint is called **SMOTE
(Synthetically Minority Oversampling Technique)**

<img src="https://drive.google.com/uc?id=16IjdHQqcddwUOtbOzVYrQufofftbOReZ" width="700" height="200">

Image shows briefly how  **SMOTE** creates data

##**[SMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html) Code**
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score
from imblearn.over_sampling import SMOTE

# Create an instance of SMOTE
smt = SMOTE()


# Perform SMOTE on the training data
print('Before SMOTE')
print(y_train.value_counts())

X_sm, y_sm = smt.fit_resample(X_train, y_train)
print('After Oversampling')
print(y_train_oversampled.value_counts())

model = LogisticRegression(C= 5, penalty= 'l1', solver = 'liblinear')

f1_train,f1_test = training(model,X_sm, y_sm,X_test,y_test)

print(f'Training F1 score:{f1_train}, Testing F1 score:{f1_test}')

"""**Observe**

With SMOTE:
- The model performed the best reaching a test F1score of 91.47%

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/044/107/original/Screenshot_2023-08-24_at_10.24.54_AM.png?1692853066 width=700>

<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/044/108/original/Screenshot_2023-08-24_at_10.25.16_AM.png?1692853151 width=700>
"""