{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "$\\newcommand{\\trinom}[3]{\\begin{pmatrix} #1 \\\\ #2 \\\\ #3 \\end{pmatrix}}$"
      ],
      "metadata": {
        "id": "fYg6zZW9T5vs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vector concepts**\n",
        "### Vector\n",
        "  * Norm of vector ||$\\vec{v}$||: Magnitude of the vector (typically it's length).\n",
        "    * L1 norm $||\\vec{v}||_1$: Is the manhattan distance $x_1$ + $x_2$ + ... + $x_n$\n",
        "    * L2 norm $||\\vec{v}||_2$: Is the euclidian distance $\\sqrt{x_1^2 + x_2^2 + ... x_n^2}$\n",
        "### Angle between vectors\n",
        "  * Gives us idea of similarity between the two vector embeddings\n",
        "    * Lesser angle => Two vectors are more similar\n",
        "  * Typically determined by doing dot product. See [dot product formula](#dot_product_formula).\n",
        "  * Two vectors $\\vec{v_1}$ and $\\vec{v_2}$ are said to be orthogonal/perpendicular to each other when their dot product = 0 i.e. $\\vec{v_1} . \\vec{v_2}$ = 0\n",
        "### Distance\n",
        "  * Distance between point and decision hyperplane\n",
        "    * Gives us a measure of confidence of classification of the point by the hyperplane.\n",
        "      * Greater distance ~= More confidence\n",
        "      * Lesser distance ~= Less confidence\n",
        "    * Always use the [distance formula](#point_distance_formula) to measure distance of point from decision plane.\n",
        "  * Distance between two parallel hyperplanes\n",
        "### Hyperplanes\n",
        "  * The bounding planes that separate given space into two half-spaces: Positive halfspace and negative halfspace.\n",
        "#### Weight vector\n",
        "  * The vector that is orthogonal to the given decision hyperplane.\n",
        "#### Bias\n",
        "  * Weight vector determines direction, while bias geometrically identifies the distance of the hyperplane from origin.\n",
        "  * Three different interpretations of bias:\n",
        "    * **Geometric:** ***Weights rotate*** the hyperplane; ***bias translates*** it. If you remove bias, you’re saying: “The origin itself has semantic meaning.” - which is almost never true in real data.\n",
        "    * **Mathematical/Algebraic:** Bias represents one more degree of freedom/dimension.  \n",
        "    Rewrite the classifier like this:  \n",
        "    $w^T$x + b = $\\binom{w}{b}^T$ $\\binom{x}{1}$ = [w b]$\\binom{x}{1}$.  \n",
        "\n",
        "      Now bias is just: a weight on a constant feature\n",
        "    * **Logical:** Prior or default decision.  \n",
        "    “If there were no signal at all, what would I predict?”  \n",
        "    In the No-signal case, x = 0, so $w^T$x + b = b.  \n",
        "    So:  \n",
        "    b > 0 => Default towards +ve classification  \n",
        "    b < 0 => Default towards -ve classification  \n",
        "    Bias encodes the classifier’s baseline belief before seeing any evidence."
      ],
      "metadata": {
        "id": "us4pGHJ789Ew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Important formulae**\n",
        "\n",
        "* <a id=\"dot_product_formula\">Dot product:</a> $\\vec{v_1}.\\vec{v_2}$ = $|v_1||v_2| cos\\theta$\n",
        "  * Use this to determine many things:\n",
        "    * Angle between two vectors\n",
        "    * Projection of one vector on other\n",
        "* <a id=\"point_distance_formula\">Distance of point</a> $x_1$ from hyperplane $\\vec{w}$.x + $w_0$ = 0 is: ($\\vec{w}$.$x_1$ + $w_0$) / ||$\\vec{w}$||\n",
        "  * If distance is +ve, the point $x_1$ is in positive halfspace of the decision hyperplane\n",
        "  * If distance is -ve, the point is in negative halfspace of the decision hyperplane\n",
        "* Distance of origin from hyperplane $\\vec{w}$.x + $w_0$ = 0 is: $w_0$ / ||$\\vec{w}$||\n",
        "* Distance between hyperplanes\n",
        "  * Two hyperplanes $\\vec{w}x + w_0 = 0$ and $\\vec{w'}x + w'_0 = 0$ are said to be parallel only if one can be expressed as a multiple of another. i.e. both can be expressed as $ax_1 + bx_2 + ... c_1 = 0$ and $ax_1 + bx_2 + ... c_2 = 0$\n",
        "  * For such hyperplanes, distance between them = $|c_2 - c_1|$/$\\sqrt{a^2 + b^2...}$"
      ],
      "metadata": {
        "id": "LJhAJe4PlTv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Classification concepts**\n",
        "\n",
        "### Gain function\n",
        "G(X, $\\vec{w}$, $w_0$) = $\\sum_{i=1}^{n}$ ($\\vec{w}^Tx_i$ + $w_0$) * $y_i$ / $||\\vec{w}||$\n",
        "\n",
        "$\\vec{w^*}$, $w_0^*$ = $argmax_{\\vec{w},w_0}$ [G(X, $\\vec{w}$, $w_0$)] = $argmax_{\\vec{w},w_0}$ $\\sum_{i=1}^{n}$ ($\\vec{w}^Tx_i$ + $w_0$) * $y_i$ / $||\\vec{w}||$\n",
        "\n",
        "## Loss function\n",
        "L(X, $\\vec{w}$, $w_0$) = -G(X, $\\vec{w}$, $w_0$)\n",
        "\n",
        "$\\vec{w^*}$, $w_0^*$ = $argmin_{\\vec{w},w_0}$ [L(X, $\\vec{w}$, $w_0$)] = $argmin_{\\vec{w},w_0}$ -( $\\sum_{i=1}^{n}$ ($\\vec{w}^Tx_i$ + $w_0$) * $y_i$ / $||\\vec{w}||$)"
      ],
      "metadata": {
        "id": "mBHMDzmukzkG"
      }
    }
  ]
}