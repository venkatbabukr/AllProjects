{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "$\\newcommand{\\trinom}[3]{\\begin{pmatrix} #1 \\\\ #2 \\\\ #3 \\end{pmatrix}}$"
      ],
      "metadata": {
        "id": "7kTvDjuadqn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1. Partial Derivative - 1**\n",
        "\n",
        "Let f(x,y) = 8$x^3$ + 3$x^2y^2$ + 2x - 2y - 1.\n",
        "\n",
        "What is the partial derivative of f(x, y) with respect to x?"
      ],
      "metadata": {
        "id": "ANuD_mYtti85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Ans:**\n",
        "\n",
        "24$x^2$ + 6x$y^2$ + 2\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "$\\partial{f(x, y)}$/$\\partial{x}$ = 24$x^2$ + 6x$y^2$ + 2"
      ],
      "metadata": {
        "id": "6zGn4l9ktyMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q2. Derivative - 1**\n",
        "\n",
        "Let f($x_1$,$x_2$) = $x_1^2$ - $3x_1x_2$.\n",
        "\n",
        "What is the derivative of f($x_1$,$x_2$) with respect to $x_1$?"
      ],
      "metadata": {
        "id": "zpPTjWZOud3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Ans:**\n",
        "$\\partial{f}$/$\\partial{x_1}$ = 2$x_1$ - 3$x_2$"
      ],
      "metadata": {
        "id": "JoG1iIWU2jeQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q3. Gradient - 1**\n",
        "\n",
        "Let f($x_1$, $x_2$, $x_3$) = $a^T$(x + b), where\n",
        "\n",
        "$\\vec{x}$ = $\\trinom{x_1}{x_2}{x_3}$, a = $\\trinom{2}{1}{1}$ and b = $\\trinom{1}{0}{0}$\n",
        "\n",
        "What is the gradient of f w.r.t $\\vec{x}$?"
      ],
      "metadata": {
        "id": "CK8d0jhCvKPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Ans:**\n",
        "\n",
        "$\\trinom{2}{1}{1}$\n",
        "\n",
        "**Explanation:**\n",
        "f($\\vec{x}$) = 2$x_1$ + $x_2$ + $x_3$ + 2 * 1 + 1 * 0 + 1 * 0\n",
        "\n",
        "= 2$x_1$ + $x_2$ + $x_3$ + 2\n",
        "\n",
        "$\\nabla_{\\vec{x}}{f}$ = $\\trinom{\\partial{f}/\\partial{x_1}}{\\partial{f}/\\partial{x_2}}{\\partial{f}/\\partial{x_3}}$ = $\\trinom{2}{1}{1}$"
      ],
      "metadata": {
        "id": "q3vQEAtIvZvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q4. Gradient - 2**\n",
        "\n",
        "Let f($\\vec{x}$) = $x^T$ (x + b) where\n",
        "\n",
        "$\\vec{x}$ = $\\trinom{x_1}{x_2}{x_3}$, b = $\\trinom{1}{3}{5}$\n",
        "\n",
        "What is $\\nabla_{\\vec{x}}{f}$?"
      ],
      "metadata": {
        "id": "U0RbXV2pvoJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Ans:**\n",
        "$\\nabla_{\\vec{x}}{f}$ = $\\trinom{2x_1 + 1}{2x_2 + 3}{2x_3 + 5}$\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "f($\\vec{x}$) = $x_1^2$ + $x_2^2$ + $x_3^2$ + $x_1$ + 3$x_2$ + 5$x_3$\n",
        "\n",
        "Therefore, $\\nabla_{\\vec{x}}{f}$ = $\\trinom{2x_1 + 1}{2x_2 + 3}{2x_3 + 5}$"
      ],
      "metadata": {
        "id": "-IR_kF9v4Zi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q5. Gradient descent - 1**\n",
        "\n",
        "Which of the following statement(s) is true?\n",
        "\n",
        "a. The update equation for gradient descent with respect to parameter vector $\\vec{w}$, for loss function f($\\vec{w}$) and learning rate $\\eta$ is $\\vec{w}^{t+1}$ = $\\vec{w}^{t}$ - $\\eta$$\\nabla_{\\vec{w}}{f(\\vec{w})}$\n",
        "\n",
        "b. The update equation for gradient ascent with respect to parameter vector $\\vec{w}$, for loss function f($\\vec{w}$) and learning rate $\\eta$ is $\\vec{w}^{t+1}$ = $\\vec{w}^{t}$ + $\\eta$$\\nabla_{\\vec{w}}{f(\\vec{w})}$\n",
        "\n",
        "c. For f($\\vec{w}$) = $w^2$ + 7, if the training loss fluctuates for a long time while using gradient descent, we must decrease the learning rate $\\eta$\n",
        "\n",
        "c. For f($\\vec{w}$) = $w^2$ + 7, if the training loss fluctuates for a long time while using gradient descent, we must increase the learning rate $\\eta$"
      ],
      "metadata": {
        "id": "-jjmvUFJwUWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Ans:**\n",
        "\n",
        "Statements a. b. and c. are true\n",
        "\n",
        "d. is wrong/false"
      ],
      "metadata": {
        "id": "FiOkvspxwwQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q6. Gradient descent - 2**\n",
        "\n",
        "Let f($\\vec{w}$,$w_0$) = y($\\vec{w}^t$x + $w_0$).\n",
        "\n",
        "For a single datapoint x = (1,2,1) and y = -1, which of the following are the correct update equations for $\\vec{w}$ and $w_0$ if we are using gradient descent?"
      ],
      "metadata": {
        "id": "tT4a19OFw2dU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Ans:**\n",
        "\n",
        "$\\vec{w}^{t+1}$ = $\\vec{w}^t$ + $\\eta$x; $w_0^{t+1}$ = $w_0^t$ + $\\eta$\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "Given function: f($\\vec{w}$,$w_0$) = y($\\vec{w}^t$x + $w_0$)\n",
        "\n",
        "i. The equation of Gradient Descent for $\\vec{w}$ is:\n",
        "\n",
        "$\\vec{w}^{t+1}$ = $\\vec{w}^t$ - $\\eta$$\\partial{f}$/$\\partial{\\vec{w}}$\n",
        "\n",
        "Since\n",
        "$\\partial{f}$/$\\partial{\\vec{w}}$ = y.x,\n",
        "\n",
        "On substituting y=-1, this becomes:\n",
        "\n",
        "> $\\vec{w}^{t+1}$ = $\\vec{w}^t$ + $\\eta$x\n",
        "\n",
        "ii. The equation of Gradient Descent for $w_0$ is:\n",
        "\n",
        "$w_0^{t+1}$ = $w_0^t$ - $\\eta$$\\partial{f}$/$\\partial{w_0}$\n",
        "\n",
        "Since\n",
        "$\\partial{f}$/$\\partial{w_0}$ = y,\n",
        "\n",
        "On substituting y=-1, this becomes: $w_0^{t+1}$ = $w_0^t$ + $\\eta$.1\n",
        "\n",
        "> $w_0^{t+1}$ = $w_0^t$ + $\\eta$\n",
        "\n"
      ],
      "metadata": {
        "id": "PnEplLMKxdre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AQ1. Partial Derivative - 2**\n",
        "\n",
        "Let f($x_1$, $x_2$, $x_3$, $x_4$) = $x_2$.ln($x_1$) + $x_3$.$e^{x_4}$ - 2$x_1^2$ - cos(5$x_3$). What is the partial derivative of f($x_1$, $x_2$, $x_3$, $x_4$) with respect to $x_2$?"
      ],
      "metadata": {
        "id": "Gf0D5GyRx2by"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Ans:**\n",
        "$\\partial{f(x_1, x_2, x_3, x_4)}$/$\\partial{x_2}$ = ln($x_1$)"
      ],
      "metadata": {
        "id": "F9Cs3ojJyAn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q2. Derivative - 2**\n",
        "\n",
        "Let f($x_1$, $x_2$) = $x_2e^{x_1}$ - 4$x_2^2$.\n",
        "\n",
        "What is the derivative of f($x_1$, $x_2$) with respect to $x_1$?"
      ],
      "metadata": {
        "id": "AqFKy4NZyIvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Ans:**\n",
        "\n",
        "$\\partial{f(x_1, x_2)}$/$\\partial{x_1}$ = $x_2e^{x_1}$"
      ],
      "metadata": {
        "id": "ynhCLsIlyX2U"
      }
    }
  ]
}