# -*- coding: utf-8 -*-
"""web_scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/182UOWhRIIw1tvMdCs_ZuK2q9RCriedle

# Big heading

## smaller heading

### even smaller
"""

import requests

base_url = "http://books.toscrape.com/index.html"

home_page = requests.get(base_url)


# checking whether the request made was successful or not
if home_page.status_code == 200:
  print("SUCCESS")
else:
  print(f"FAILED, status code: {home_page.status_code}")

type(home_page)

home_page

home_page.

"""1. 200 OK: The request has been successfully processed, and the server returns the requested content.
2. 404 Not Found: The requested resource or page could not be found on the server.
3. 403 Forbidden: Access to the requested resource is forbidden or not allowed for the client.
4. 500 Internal Server Error: The server encountered an unexpected error while processing the request.
5. 302 Found (or 301 Moved Permanently): The requested resource has been temporarily (or permanently) moved to a different URL, and the client should follow the redirection.
"""

from bs4 import BeautifulSoup

home_page.content

soup = BeautifulSoup(markup = home_page.content, parser="html.parser")

books = soup.find_all(name="li", class_="col-xs-6 col-sm-4 col-md-3 col-lg-3")
len(books)

soup.find(name = "script")

book1 = books[0]

book1

book1.findChild(name="a")

book_url = book1.findChild(name="a").get("href")

book_url

book1

from urllib.parse import urljoin

book_url = urljoin(base_url, book_url)
book_url

book1_page

book1_page = requests.get(book_url)
book_info = book1_page.content

soup = BeautifulSoup(markup = book_info, parser="html.parser")

book_name = soup.find(name="h1")
book_name.text

book_table = soup.find_all(name="tr")
book_table

book_table[0].text

book_data = {}
for row in book_table:
  key = row.find(name="th").text
  value = row.find(name="td").text
  book_data[key] = value

book_data

page_url = "https://books.toscrape.com/catalogue/page-1.html"

page_content = requests.get(page_url).content
page_soup = BeautifulSoup(markup=page_content, parser="html.parser")
page_books = page_soup.find_all(name="li", class_="col-xs-6 col-sm-4 col-md-3 col-lg-3")

print(len(page_books))

page_books

def scrape_book(book_url):
  book_info = requests.get(book_url).content
  book_soup = BeautifulSoup(markup=book_info, parser="html.parser")

  book_data = {}

  # getting name
  name = book_soup.find(name="h1").getText()
  book_data['name'] = name

  # getting other data
  book_table_data = book_soup.find_all(name="tr")
  for row in book_table_data:
    key = row.find(name="th").getText()
    value = row.find(name="td").text
    book_data[key] = value

  # let's also keep the url of book in final result
  book_data['url'] = book_url
  return book_data

books_data = []

for book in page_books:
  book_url = book.findChild(name="a").get("href")
  #print(book_url)
  # converting relative URL to absolute
  book_url = urljoin(base_url, '/catalogue/'+book_url)
  #print(book_url)

  book_data = scrape_book(book_url)
  books_data.append(book_data)

books_data[:3]

page_books[3]

base_url+'/catalogue'

#Quiz

from bs4 import BeautifulSoup

# HTML
html_snippet = """
<div>
  <p>Hello, <b>World!</b></p>
  <p>Welcome to <a href="https://example.com">Example</a>.</p>
</div>
"""

soup = BeautifulSoup(html_snippet, 'html.parser')

extracted_text = soup.find(name="div").getText()

print(extracted_text)

books_data[0]

s = books_data[0]['Price (excl. tax)']
books_data[0]['Price (excl. tax)'] = float(s[1:])

books_data[0]

