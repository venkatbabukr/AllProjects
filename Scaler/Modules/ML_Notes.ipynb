{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkatbabukr/AllProjects/blob/main/Scaler/Modules/ML_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\newcommand{\\trinom}[3]{\\begin{pmatrix} #1 \\\\ #2 \\\\ #3 \\end{pmatrix}}$"
      ],
      "metadata": {
        "id": "fYg6zZW9T5vs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Statistics concepts**"
      ],
      "metadata": {
        "id": "LRamWhSJF_oz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical measures\n",
        "\n",
        "Mean\n",
        "Median\n",
        "Mode\n",
        "Variance\n",
        "Standard deviation"
      ],
      "metadata": {
        "id": "PobMnQ8XGFjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ML Model Components**\n",
        "\n",
        "1. **Variables ($X, y$):** The Data Representation. This is your raw material.\n",
        "    * **Features** ($X$): The **independent variables**. In math terms, these are often represented as vectors or matrices.\n",
        "    * **Targets** ($y$): The **dependent variable** you are trying to predict. They are also called as **labels**/**categories** in ML classification problems.\n",
        "2. **Parameters ($\\theta$ or w, b):** The Model State. These are the internal settings that define exactly how the mapping function behaves.\n",
        "    * Weights ($w$): Determine the strength of the signal for each feature.\n",
        "    * Bias ($b$): An additive constant that allows the model to shift the activation function.\n",
        "3. **Mapping Function ($f_{\\theta}(x)$):** The Hypothesis Space. This is the \"shape\" of the machine (e.g., a straight line, a complex neural network).   \n",
        "   \n",
        "   > To describe in simple mathematical terms: $y = f_{\\theta}(x)$\n",
        "4. **Loss Function ($L$):** The Objective Function (or Cost Function). This is the \"North Star\" that tells the model whether it's doing a good job or a bad one."
      ],
      "metadata": {
        "id": "JRiZx9NHMDPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ML Paradigms**\n",
        "\n",
        "| Paradigm | The Core Idea | The Mathematical Goal |\n",
        "| :--- | :--- | :--- |\n",
        "| **Supervised Learning** | Learning with a teacher. | Mapping $x$ to a known $y$ (Label/Target). |\n",
        "| **Unsupervised Learning** | Learning by finding patterns. | Modeling the underlying structure or distribution of $x$. |\n",
        "| **Reinforcement Learning** | Learning by trial and error. | Maximizing a numerical reward signal through actions. |"
      ],
      "metadata": {
        "id": "9bMkGcbOWGP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ML variants**\n",
        "\n",
        "## Supervised learning\n",
        "\n",
        "1. **Classification problems**: When target is a category or class label\n",
        "    * Binary classification\n",
        "    * Multiclass classification\n",
        "2. **Regression problems**: When target is a number\n",
        "3. **Forecasting problems**: Special type of regression problems where timeseries is involved.\n",
        "\n",
        "## Un-supervised learning\n",
        "\n",
        "1. **Clustering**\n",
        "\n",
        "## Hybrid learning\n",
        "\n",
        "1. **Recommendation problems**:   \n",
        "Examples:\n",
        "    * Video recommendation done by youtube or prime etc...\n",
        "    * Songs recommendation done by Spotify\n",
        "    * Shopping recommendations done by Amazon  \n",
        "    \n",
        "   Recommendation problems can be modelled as both Supervised or Un-supervised.\n",
        "     * **Supervised modeling/visualisation**: Extension of **classification problems**. Once we have classification, for a new data point, we not only find it's classification but also recommend to the new data point all other data that fall in the particular class/category!\n",
        "     * **Unsupervised modeling/visualisation**: Cluster the data points to find similar groups. Now assign the new data point to the most suitable cluster and do mutual recommendations...\n"
      ],
      "metadata": {
        "id": "GLpZniNLQpJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Handling Outliers**"
      ],
      "metadata": {
        "id": "Qa6EVHryDSkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vector concepts**\n",
        "### Vector\n",
        "  * Norm of vector ||$\\vec{v}$||: Magnitude of the vector (typically it's length).\n",
        "    * L1 norm $||\\vec{v}||_1$: Is the manhattan distance $x_1$ + $x_2$ + ... + $x_n$\n",
        "    * L2 norm $||\\vec{v}||_2$: Is the euclidian distance $\\sqrt{x_1^2 + x_2^2 + ... x_n^2}$\n",
        "### Angle between vectors\n",
        "  * Gives us idea of similarity between the two vector embeddings\n",
        "    * Lesser angle => Two vectors are more similar\n",
        "  * Typically determined by doing dot product. See [dot product formula](#dot_product_formula).\n",
        "  * Two vectors $\\vec{v_1}$ and $\\vec{v_2}$ are said to be orthogonal/perpendicular to each other when their dot product = 0 i.e. $\\vec{v_1} . \\vec{v_2}$ = 0\n",
        "### Distance\n",
        "  * Distance between point and decision hyperplane\n",
        "    * Gives us a measure of confidence of classification of the point by the hyperplane.\n",
        "      * Greater distance ~= More confidence\n",
        "      * Lesser distance ~= Less confidence\n",
        "    * Always use the [distance formula](#point_distance_formula) to measure distance of point from decision plane.\n",
        "  * Distance between two parallel hyperplanes\n",
        "### Hyperplanes\n",
        "  * The bounding planes that separate given space into two half-spaces: Positive halfspace and negative halfspace.\n",
        "#### Weight vector\n",
        "  * The vector that is orthogonal to the given decision hyperplane.\n",
        "#### Bias\n",
        "  * Weight vector determines direction, while bias geometrically identifies the distance of the hyperplane from origin.\n",
        "  * Three different interpretations of bias:\n",
        "    * **Geometric:** ***Weights rotate*** the hyperplane; ***bias translates*** it. If you remove bias, you’re saying: “The origin itself has semantic meaning.” - which is almost never true in real data.\n",
        "    * **Mathematical/Algebraic:** Bias represents one more degree of freedom/dimension.  \n",
        "    Rewrite the classifier like this:  \n",
        "    $w^T$x + b = $\\binom{w}{b}^T$ $\\binom{x}{1}$ = [w b]$\\binom{x}{1}$.  \n",
        "\n",
        "      Now bias is just: a weight on a constant feature\n",
        "    * **Logical:** Prior or default decision.  \n",
        "    “If there were no signal at all, what would I predict?”  \n",
        "    In the No-signal case, x = 0, so $w^T$x + b = b.  \n",
        "    So:  \n",
        "    b > 0 => Default towards +ve classification  \n",
        "    b < 0 => Default towards -ve classification  \n",
        "    Bias encodes the classifier’s baseline belief before seeing any evidence."
      ],
      "metadata": {
        "id": "us4pGHJ789Ew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Important formulae**\n",
        "\n",
        "* <a id=\"dot_product_formula\">Dot product:</a> $\\vec{v_1}.\\vec{v_2}$ = $|v_1||v_2| cos\\theta$\n",
        "  * Use this to determine many things:\n",
        "    * Angle between two vectors\n",
        "    * Projection of one vector on other\n",
        "* <a id=\"point_distance_formula\">Distance of point</a> $x_1$ from hyperplane $\\vec{w}$.x + $w_0$ = 0 is: ($\\vec{w}$.$x_1$ + $w_0$) / ||$\\vec{w}$||\n",
        "  * If distance is +ve, the point $x_1$ is in positive halfspace of the decision hyperplane\n",
        "  * If distance is -ve, the point is in negative halfspace of the decision hyperplane\n",
        "* Distance of origin from hyperplane $\\vec{w}$.x + $w_0$ = 0 is: $w_0$ / ||$\\vec{w}$||\n",
        "* Distance between hyperplanes\n",
        "  * Two hyperplanes $\\vec{w}x + w_0 = 0$ and $\\vec{w'}x + w'_0 = 0$ are said to be parallel only if one can be expressed as a multiple of another. i.e. both can be expressed as $w_1x_1 + w_2x_2 + ... w_0 = 0  (ax_1 + bx_2 + ... c_1 = 0)$ and $w_1x_1 + w_2x_2 + ... p.w'_0 = 0 (ax_1 + bx_2 + ... c_2 = 0)$\n",
        "  * For such hyperplanes, distance between them = $|p.w'_0 - w_0|$/$\\sqrt{w_1^2 + w_2^2...}$ i.e. ($|c_2 - c_1|$/$\\sqrt{a^2 + b^2...})$"
      ],
      "metadata": {
        "id": "LJhAJe4PlTv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target Variable Encoding"
      ],
      "metadata": {
        "id": "CII2YRzixJRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature scaling"
      ],
      "metadata": {
        "id": "787kJJXexMIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Minimisation/Optimization Functions**"
      ],
      "metadata": {
        "id": "mBHMDzmukzkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gain & Loss Functions\n",
        "\n",
        "### Gain function\n",
        "G(X, $\\vec{w}$, $w_0$) = $\\sum_{i=1}^{n}$ ($\\vec{w}^Tx_i$ + $w_0$) * $y_i$ / $||\\vec{w}||$\n",
        "\n",
        "$\\vec{w^*}$, $w_0^*$ = $argmax_{\\vec{w},w_0}$ [G(X, $\\vec{w}$, $w_0$)] = $argmax_{\\vec{w},w_0}$ $\\sum_{i=1}^{n}$ ($\\vec{w}^Tx_i$ + $w_0$) * $y_i$ / $||\\vec{w}||$\n",
        "\n",
        "### Loss function\n",
        "L(X, $\\vec{w}$, $w_0$) = -G(X, $\\vec{w}$, $w_0$)\n",
        "\n",
        "$\\vec{w^*}$, $w_0^*$ = $argmin_{\\vec{w},w_0}$ [L(X, $\\vec{w}$, $w_0$)] = $argmin_{\\vec{w},w_0}$ -( $\\sum_{i=1}^{n}$ ($\\vec{w}^Tx_i$ + $w_0$) * $y_i$ / $||\\vec{w}||$)\n"
      ],
      "metadata": {
        "id": "-0b-wl7J7ucT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear regression - Mean Square Error (MSE)\n",
        "\n",
        "Let $\\vec{W} = \\begin{bmatrix}\\vec{w} & w_0\\end{bmatrix}$\n",
        "\n",
        "$MSE(X, \\vec{W}) = 1/m * (Y - X^T.\\vec{W})^2$\n",
        "\n",
        "Gradient descent formula will be\n",
        "\n",
        "$\\vec{W}^{t+1} = \\vec{W}^t - \\eta \\nabla_{\\vec{W}}MSE(X, \\vec{W})$\n",
        "\n",
        "The partial derivative\n",
        "\n",
        "$\\nabla_{\\vec{W}}MSE(X, \\vec{W}) = \\nabla_{\\vec{W}} 1/m * (Y - X^T.\\vec{W})^2$\n",
        "\n",
        "= $2 * 1/m * (Y - X^T.\\vec{W}) . (-X^T)$ [--> By applying chain rule]\n",
        "\n",
        "= $-2/m * (Y - \\hat{Y}).X^T$\n",
        "\n",
        "Therefore, we get:\n",
        "\n",
        "$\\vec{w}^{t+1} = \\vec{w}^t + \\eta(2/m * (y - \\hat{y}).X^T)$\n",
        "\n",
        "$w_0^{t+1} = w_0^t + \\eta(2/m * (y - \\hat{y}))$"
      ],
      "metadata": {
        "id": "1ua53w3V8AGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $R^2$ Coefficient of Determination\n",
        "\n",
        "* Significance or meaning of $R^2$ in layman terms:\n",
        "    * This is a measure of **Goodness-of-fit**!\n",
        "    * Captures how much variation (variance) the best fit is able to explain.\n",
        "* Higher $R^2$ => Better fitting model with following caveats.\n",
        "\n",
        "    * Overfitting trap: If the model is trained with lot of noise, then it has incorporated so much noise that it'll fail to work in real world! Therefore, ensure there is maximum noise reduction in training data!\n",
        "\n",
        "    * Biased modeling! If we come up with a linear regression model for a pattern that is actually non-linear in real world, even very high $R^2$ doesn't ensure our model is correct! Solution: Always check your **Residual Plots** to see if the errors are random\n",
        "\n",
        "    * $R^2$ doesn't work well for non-linear models!\n",
        "\n",
        "**Formula, explanation, derivation:**\n",
        "\n",
        "$R^2$ = 1 - Variance(Errors)/Variance(Y)\n",
        "\n",
        "= $1 - MSE/\\sigma^2(Y)$\n",
        "\n",
        "= 1 - RSS/TSS\n",
        "\n",
        "Here,\n",
        "\n",
        "RSS (Residual Sum of Squares) = np.sum((y - y_pred) ** 2)\n",
        "\n",
        "TSS (Total Sum of Squares) = np.sum((y - y.mean()) ** 2)\n",
        "\n",
        "**How can we use $R^2$ effectively?**\n",
        "\n",
        "* Find out $R^2$ of your training model only to understand **how well the model has captured the variance in data**.\n",
        "    * A low $R^2$ means, the model has not yet captured variance in data.\n",
        "    * A high $R^2$ means the model has good fit with the data points.\n",
        "* This metric can help at times when we don't have enough training and testing data.\n",
        "* **Adjusted $R^2$** is more helpful when we have very less training & testing data and wish to find out whether adding a new feature is improving the model or worsening the model..."
      ],
      "metadata": {
        "id": "3A1GIDRnBZlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adjusted $R^2$\n",
        "\n",
        "Overcomes practical symptoms/drawbacks of $R^2$:\n",
        "1. Overfitting\n",
        "    * If you have 5 data points and 4 predictors, your $R^2$ will almost always be 1.\n",
        "2. False confidence\n",
        "    * When a model perfectly memorizes all the specific answers of training, it's $R^2$ will be close to 1, but the model might fail actual exam!\n",
        "\n",
        "**Formula, explanation, derivation**\n",
        "\n",
        "$R^2_{adj}$ = $1 - \\frac{RSS / df_{residual}}{TSS / df_{total}}$\n",
        "\n",
        "* $df_{total}$ = m - 1 (One less than the number of data items)\n",
        "\n",
        "* $df_{residual}$ = $df_{total}$ - k (The number of features)  \n",
        "&emsp;&emsp;&emsp;&emsp;&emsp;= m - 1 - k\n",
        "\n",
        "Substituting all these, we get:\n",
        "\n",
        "$R^2_{adj}$ = $1 - \\frac{RSS * (m - 1)}{TSS * (m - 1 - k)}$\n",
        "\n",
        "One more formula:\n",
        "\n",
        "Since RSS/TSS = 1 - $R^2$, we get\n",
        "\n",
        "$R^2_{adj}$ = 1 - $\\frac{(1 - R^2) * (m - 1)}{(m - 1 - k)}$\n",
        "\n",
        "**Logical understanding**\n",
        "\n",
        "* If your RSS increases, naturally both $R^2$ and $R^2_{adj}$ decrease.\n",
        "\n",
        "* If your RSS remains the same or decreases marginally with added features/detectors, your $R^2$ might remain same but $R^2_{adj}$ will decrease indicating you are adding dead weight detectors!\n",
        "\n",
        "* If your RSS decreases significantly with added features/detectors, both $R^2$ and $R^2_{adj}$ will start to increase indicating that you are indeed getting a better model!\n",
        "\n",
        "**Intuition**\n",
        "\n",
        "The formula is designed to penalize complexity - when we try to get a linear regression model by adding more features!\n",
        "\n",
        "As you can visualize a squiggly line in 2-D can be made to look a perfect straight line (or fitted within a hyperplane) in N-D! But that doesn't mean the model is correct, it means we made the model look correct by adding lot of dead weight predictors!\n",
        "\n",
        "Say you have 3 datapoints that look like a triangle when plotted in 2-D, you can't find a linear regression for this! But if you increase the dimensionality to 3-D even by adding any dead weight predictor, you can immediately find a plane that covers all the three points! Does this mean the problem is now a linear regression? $R^2$ will intuitively say ***Yes***, but $R^2_{adj}$ is a metric that will try it's best to apply brakes on this illusion/false confidence.\n",
        "\n",
        "\n",
        "As you keep adding dimensions (features), you are essentially giving the model enough \"room\" to twist and turn until it touches every data point.\n",
        "* Simple $R^2$ sees this and says: \"Wow, zero error! Perfect model!\n",
        "* \"$R^2_{adj}$ looks at the \"cost\" of those extra dimensions and says: \"You used 10 dimensions to explain 10 points. That's not a model; that's just a map of the noise.\"\n",
        "\n",
        "If $R^2$ is 0.99 but $R^2_{adj}$ is 0.50, your \"squiggly line in N-D\" has overfitted the data, and it will likely fail miserably the moment you give it a new $X$ value."
      ],
      "metadata": {
        "id": "RZo2Qk2NAVxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abbreviations\n",
        "\n",
        "| Abbreviation | Expansion | More notes |\n",
        "| :-- | :-- | :-- |\n",
        "| **Data Analysis** |\n",
        "| EDA | Exploratory Data Analysis | See Pandas notes on EDA |\n",
        "| **Machine Learning** |\n",
        "| ***Section 1*** |\n",
        "| GD<br>BGD | Gradient Descent<br>Batch Gradient Descent | Algorithm to optimally find the minima of loss function<br>Batch Gradient Descent uses entire data set to perform gradient descent |\n",
        "| MLRM | Multiple Linear Regression Model | * A Linear regression model built on more than one variable.<br>* Visualization can be achieved upto just 2 variables (max 3D graph plot)<br>* Hyperplane will be built in a d+1 dimensional graph where d is the number of variables |\n",
        "| SGD | Stochastic Gradient Descent | * Variant of GD<br>* It is a stochastic process/stochastic algorithm to find minima of loss function.<br>* Fast when compared to BGD |\n",
        "| SLRM | Simple Linear Regression Model | * A Linear regression model built on single variable.<br>* Can be visualized using 2D graph plot |\n",
        "| ***Section 2*** |\n",
        "| ME | Mean Error | * Just mean of the errors<br> *Avoid it completely as it is prone to miscalculation (esp. when errors cancel out and mean becomes 0 despite errors present!) |\n",
        "| MAE | Mean Absolute Error | |\n",
        "| MSE | Mean Square Error | MSE is equivalent to variance in errors! |\n",
        "| RMSE | Root Mean Square Error | * RMSE is the standard deviation in errors!<br>* Better for reporting purposes whereas MSE is most suited for optimisation over MAE |\n",
        "| $R^2$ | Coefficient of determination | Details here |\n",
        "| RSS | Residual Sum of Squares | Details here |\n",
        "| TSS | Total Sum of Squares | Details here |"
      ],
      "metadata": {
        "id": "NN-yn1Hv618w"
      }
    }
  ]
}
